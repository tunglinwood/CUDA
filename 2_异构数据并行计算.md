# 2_异构数据并行计算

* [2.1 数据并行性](https://github.com/tunglinwood/CUDA/blob/main/2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97.md#21-%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E6%80%A7)
* [2.2 CUDA C程序结构](https://github.com/tunglinwood/CUDA/blob/main/2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97.md#22-cuda-c%E7%A8%8B%E5%BA%8F%E7%BB%93%E6%9E%84)
* [2.3 向量加法内核]()
* [2.4 设备全局内存和数据传输]()
* [2.5 内核函数和线程]()
* [2.6 调用内核函数]()
* [2.7 编译]()
* [2.8 总结]()
* [练习]()

数据并行性是指对数据集不同部分的计算工作可以相互独立地完成，因此可以并行进行。许多应用程序表现出大量的数据并行性，使它们适合于可扩展的并行执行。因此，并行程序员必须熟悉数据并行性的概念以及用于编写利用数据并行性的代码的并行编程语言构造。在本章中，我们将使用CUDA C语言构造开发一个简单的数据并行程序。

## 2.1 数据并行性
当现代软件应用程序运行缓慢时，问题通常出在数据上——需要处理的数据太多。图像处理应用程序处理具有数百万到万亿像素的图像或视频。科学应用程序使用数十亿网格点建模流体动力学。分子动力学应用程序必须模拟成千上亿原子之间的相互作用。航空公司调度处理数千次航班、机组人员和机场登机口。这些像素、粒子、网格点、相互作用、航班等大多数通常可以独立处理。例如，在图像处理中，将彩色像素转换为灰度仅需要该像素的数据。模糊图像时，将每个像素的颜色与附近像素的颜色平均，只需该小区域内的像素数据。即使是看似全局的操作，例如找到图像中所有像素的平均亮度，也可以分解为许多可以独立执行的小计算。这种对不同数据片段的独立评估是数据并行性的基础。编写数据并行代码意味着围绕数据（重新）组织计算，以便我们可以并行执行结果的独立计算，从而更快地完成整体任务——通常要快得多。

![image](https://github.com/user-attachments/assets/d49864d9-1a50-4d0f-9d8a-e44e33c07d55)
> 图2.1

让我们通过一个彩色转灰度转换的例子来说明数据并行性的概念。图2.1显示了一张由许多像素组成的彩色图像（左侧），每个像素包含一个从0（黑色）到1（全强度）变化的红色、绿色和蓝色分数值（r, g, b）。
要将彩色图像（图2.1的左侧）转换为灰度图像（右侧），我们需要通过应用以下加权和公式来计算每个像素的亮度值L：
$$ L = 0.21r + 0.72g + 0.07b $$

> ### RGB颜色图像表示
> 在RGB表示中，图像中的每个像素被存储为一个(r, g, b)值的元组。图像行的格式为(r g b) (r g b) . . . (r g b)，如下面的概念图所示。每个元组指定了红色（R）、绿色（G）和蓝色（B）的混合比例。换句话说，对于每个像素，r、g和b值代表了在像素呈现时红色、绿色和蓝色光源的强度（0表示黑暗，1表示全强度）。
> ![image](https://github.com/user-attachments/assets/47c6ad49-4007-45a9-9083-1fe5a27f68e1)
> 这三种颜色的实际混合比例在行业指定的颜色空间中有所不同。在这里，AdobeRGB颜色空间中三种颜色的有效组合被显示为三角形的内部区域。
  每种混合的垂直坐标（y值）和水平坐标（x值）显示了像素强度的G和R分量的比例。剩余部分（1-y-x）的像素强度分配给B。为了呈现图像，每个像素的r、g、b值用于计算像素的总强度（亮度）以及混合系数（x、y、1-y-x）。

如果我们将输入视为一个由RGB值组成的数组I，而输出视为相应的亮度值数组O，我们将得到如图2.2所示的简单计算结构。例如，O[0]是通过根据上述公式计算I[0]中的RGB值的加权和生成的；O[1]是通过计算I[1]中的RGB值的加权和生成的；O[2]是通过计算I[2]中的RGB值的加权和生成的；依此类推。这些每像素的计算彼此之间没有依赖关系。所有这些计算都可以独立进行。显然，颜色到灰度的转换表现出丰富的数据并行性。当然，完整应用中的数据并行性可能更为复杂，本书的大部分内容都致力于教授找到和利用数据并行性所需的并行思维。

![image](https://github.com/user-attachments/assets/812f37f6-7c27-43fa-983b-91b0ea98e90b)
> 图2.2

> 任务并行与数据并行
> 数据并行不是并行编程中唯一使用的并行类型。任务并行在并行编程中也被广泛使用。任务并行通常通过应用程序的任务分解来实现。例如，一个简单的应用程序可能需要进行向量加法和矩阵向量乘法。每一个都是一个任务。如果这两个任务可以独立完成，就存在任务并行性。I/O和数据传输也是常见的任务来源。
> 在大型应用程序中，通常有大量的独立任务，因此有大量的任务并行性。例如，在一个分子动力学模拟器中，自然任务的列表包括振动力、旋转力、非键合力的邻居识别、非键合力、速度和位置、以及基于速度和位置的其他物理属性。
> 一般而言，数据并行是并行程序可扩展性的主要来源。对于大数据集，通常可以找到丰富的数据并行性，从而能够利用大规模并行处理器，并随着每一代拥有更多执行资源的硬件的发展提升应用性能。然而，任务并行在实现性能目标方面也可以发挥重要作用。当我们介绍流（streams）时，将会进一步讨论任务并行性。

## 2.2 CUDA C程序结构

现在我们准备学习如何编写CUDA C程序，以利用数据并行性实现更快的执行速度。CUDA C在流行的ANSI C编程语言基础上进行了最小化的新语法和库函数扩展，使程序员能够针对包含CPU核心和大规模并行GPU的异构计算系统进行编程。顾名思义，CUDA C是建立在NVIDIA的CUDA平台上的。

CUDA目前是大规模并行计算中最成熟的框架。它在高性能计算行业中得到了广泛应用，并且在最常见的操作系统上提供了编译器、调试器和性能分析工具等基本工具。

CUDA C程序的结构反映了计算机中主机（CPU）和一个或多个设备（GPU）的共存。每个CUDA C源文件可以包含主机代码和设备代码的混合。默认情况下，任何传统的C程序都是只包含主机代码的CUDA程序。可以在任何源文件中添加设备代码。设备代码通过特殊的CUDA C关键字清晰地标记出来。设备代码包括函数或内核，其代码以数据并行的方式执行。

![image](https://github.com/user-attachments/assets/74b9401b-0903-41b2-8b81-35c52b54d42b)
> 图2.3 CUDA程序执行

CUDA程序的执行如图2.3所示。执行从主机代码（CPU串行代码）开始。当调用内核函数时，会在设备上启动大量线程以执行内核。所有由内核调用启动的线程统称为一个网格。这些线程是CUDA平台上并行执行的主要载体。图2.3展示了两个线程网格的执行。当一个网格的所有线程完成执行时，该网格终止，执行继续在主机上进行，直到启动另一个网格。

请注意，图2.3显示了一个简化的模型，其中CPU执行和GPU执行不重叠。许多异构计算应用程序管理CPU和GPU执行的重叠，以充分利用CPU和GPU。

启动一个网格通常会生成许多线程以利用数据并行性。在颜色到灰度转换的示例中，每个线程可以用于计算输出数组O的一个像素。在这种情况下，网格启动生成的线程数等于图像中的像素数。对于大型图像，将生成大量线程。由于高效的硬件支持，CUDA程序员可以假设这些线程生成和调度所需的时钟周期非常少。这一假设与传统的CPU线程形成对比，后者通常需要数千个时钟周期来生成和调度。在下一章中，我们将展示如何实现颜色到灰度转换和图像模糊内核。在本章的其余部分，我们将以向量加法为示例进行讲解，以保持简单。

> ### 线程
> 线程是现代计算机中处理器执行顺序程序的一种简化视图。一个线程包括程序的代码、正在执行的代码位置以及其变量和数据结构的值。就用户而言，线程的执行是顺序的。用户可以使用源级调试器通过逐条执行语句、查看将要执行的下一条语句并检查变量和数据结构的值来监控线程的进展。
> 线程已经在编程中使用了很多年。如果程序员希望在应用程序中启动并行执行，他/她可以使用线程库或特殊语言创建和管理多个线程。在CUDA中，每个线程的执行同样是顺序的。CUDA程序通过调用内核函数来启动并行执行，这会导致底层运行机制启动一个线程网格，以并行处理数据的不同部分。

## 2.3 向量加法内核

我们使用向量加法来演示CUDA C程序的结构。向量加法可以说是最简单的数据并行计算，相当于顺序编程中的 "Hello World"。在展示向量加法的内核代码之前，先回顾一下传统向量加法（主机代码）函数的工作原理是有帮助的。图2.4显示了一个简单的传统C程序，它由一个主函数和一个向量加法函数组成。在我们的所有示例中，若需要区分主机和设备数据，我们将给主机使用的变量名后缀加上“_h”，给设备使用的变量名后缀加上“_d”，以提醒自己这些变量的预期用途。由于图2.4中只有主机代码，因此我们只看到以“_h”结尾的变量。

假设要相加的向量存储在主程序中分配和初始化的数组A和B中。输出向量存储在数组C中，该数组也在主程序中分配。为了简洁起见，我们不显示A、B和C在主函数中如何分配或初始化的细节。指向这些数组的指针与包含向量长度的变量N一起传递给`vecAdd`函数。请注意，`vecAdd`函数的参数加上了“_h”后缀，以强调它们由主机使用。当我们在接下来的步骤中引入设备代码时，这种命名约定将很有帮助。

```cuda
// Compute vector sum C_h = A_h + B_h
void vecAdd(float* A_h, float* B_h, float* C_h, int n) {
  for (int i = 0; i < n; ++i) {
    C_h[i] = A_h[i] + B_h[i];
  }
}
int main() {
    // Memory allocation for arrays A, B, and C
    // I/O to read A and B, N elements each
    ...
    vecAdd(A, B, C, N);
}

```
> 图2.4 传统的向量加法的简单 C 代码示例

图2.4中的`vecAdd`函数使用`for`循环迭代向量元素。在第i次迭代中，输出元素C_h[i]接收A_h[i]和B_h[i]的和。向量长度参数n用于控制循环，以使迭代次数与向量长度匹配。该函数通过指针A_h、B_h和C_h分别读取A和B的元素并写入C的元素。当`vecAdd`函数返回时，主函数中的后续语句可以访问C的新内容。

> C语言中的指针
> 图2.4中的函数参数A、B和C是指针。在C语言中，可以使用指针来访问变量和数据结构。
虽然浮点变量V可以这样声明：
```c
float V;
```
但指针变量P可以这样声明：
```c
float *P;
```
> 通过语句`P = &V`将V的地址赋值给P，我们让P“指向”V。`*P`成为V的同义词。例如，`U = *P`将V的值赋给U。再比如，`*P = 3`将V的值改为3。
> 在C程序中，可以通过指向其第0个元素的指针来访问数组。例如，语句`P = &(A[0])`使P指向数组A的第0个元素。`P[i]`成为`A[i]`的同义词。实际上，数组名A本身就是指向其第0个元素的指针。
> 在图2.4中，将数组名A作为第一个参数传递给函数调用`vecAdd`，使得函数的第一个参数A_h指向A的第0个元素。因此，函数体内的A_h[i]可以用来访问主函数中数组A的A[i]。
> 关于C语言中指针的详细用法，请参阅Patt和Patel（Patt & Patel，2020）的易于理解的解释。

![image](https://github.com/user-attachments/assets/d8a2e5dc-87f4-4fd0-9c9e-db0333b9c14d)

```cuda
void vecAdd(float* A, float* B, float* C, int n) {
    int size = n* sizeof(float);
    float *d_A *d_B, *d_C;
    // Part 1: Allocate device memory for A, B, and C
    // Copy A and B to device memory
    //...
    // Part 2: Call kernel – to launch a grid of threads
    // to perform the actual vector addition
    // ...
    // Part 3: Copy C from the device memory
    // Free device vectors
    //...
  
}
```  
> 图2.5 将工作移至设备的修改后的`vecAdd`函数概述


一种并行执行向量加法的直接方法是修改`vecAdd`函数并将其计算移至设备。图2.5显示了这种修改后的`vecAdd`函数的结构。函数的第1部分在设备（GPU）内存中分配空间以存储A、B和C向量的副本，并将A和B向量从主机内存复制到设备内存。第2部分调用实际的向量加法内核以在设备上启动一个线程网格。第3部分将和向量C从设备内存复制到主机内存，并释放设备内存中的三个数组。

请注意，修改后的`vecAdd`函数实际上是一个外包代理，它将输入数据传输到设备，激活设备上的计算，并从设备收集结果。代理以这样一种方式进行，使得主程序甚至不需要知道向量加法现在实际上是在设备上完成的。实际上，由于所有数据来回复制，这种“透明”的外包模型可能非常低效。人们通常会将大型和重要的数据结构保留在设备上，并简单地从主机代码调用设备函数。然而，现在我们将使用简化的透明模型来介绍基本的CUDA C程序结构。修订函数的详细信息以及内核函数的组成方式将是本章其余部分的主题。

## 2.4 设备全局内存和数据传输
在当前的 CUDA 系统中，设备通常是带有自己动态随机存取存储器（DRAM）的硬件卡，这种内存被称为设备全局内存，简称为全局内存。例如，NVIDIA Volta V100 带有 16GB 或 32GB 的全局内存。称其为“全局”内存是为了将其与其他类型的设备内存区分开来，其他类型的设备内存也对程序员可访问。关于 CUDA 内存模型和不同类型设备内存的详细信息将在第 5 章“内存架构和数据局部性”中讨论。

对于向量加法内核，在调用内核之前，程序员需要在设备全局内存中分配空间，并将数据从主机内存传输到设备全局内存中的分配空间。这对应于图 2.5 的第 1 部分。同样，在设备执行之后，程序员需要将结果数据从设备全局内存传输回主机内存，并释放设备全局内存中不再需要的分配空间。这对应于图 2.5 的第 3 部分。CUDA 运行时系统（通常在主机上运行）提供了应用程序编程接口（API）函数来代表程序员执行这些活动。从此以后，我们将简单地说数据从主机传输到设备，作为数据从主机内存复制到设备全局内存的简写。相同的适用于相反方向。

在图 2.5 中，`vecAdd` 函数的第 1 部分和第 3 部分需要使用 CUDA API 函数来分配设备全局内存用于 A、B 和 C；将 A 和 B 从主机传输到设备；在向量加法之后将 C 从设备传输到主机；并释放 A、B 和 C 的设备全局内存。我们将首先解释内存分配和释放函数。

`cudaMalloc()`：
- **用途**：在设备（GPU）全局内存中分配内存。
- **参数**：
  1. 指向已分配对象的指针的地址：这是一个指向指针的指针（`void**`），将被设置为指向设备上分配的内存。
  2. 以字节为单位的已分配对象的大小：指定要分配多少内存，以字节为单位。

`cudaFree()`：
- **用途**：释放在设备（GPU）全局内存中分配的内存。
- **参数**：
  - 被释放对象的指针：接受一个指针，该指针指向先前用`cudaMalloc()`分配的内存。它不会更改指针本身的值，但会释放它所指向的内存。
> 图 2.6 cudaMalloc, cudaFree是CUDA编程中管理GPU内存的关键，允许在并行计算期间高效地进行数据传输和资源利用。

图 2.6 显示了两个用于分配和释放设备全局内存的 API 函数。
`cudaMalloc` 函数可以从主机代码中调用，以为对象分配一块设备全局内存。读者应该注意到 `cudaMalloc` 和标准 C 运行时库 malloc 函数之间的惊人相似性。这是有意为之的；CUDA C 是 C 的最小扩展版。CUDA C 使用标准 C 运行时库 malloc 函数来管理主机内存，并将 `cudaMalloc` 作为对 C 运行时库的扩展。通过尽可能接近原始 C 运行时库的接口，CUDA C 最小化了 C 程序员重新学习这些扩展的时间。

`cudaMalloc` 函数的第一个参数是指针变量的地址，该变量将被设置为指向分配的对象。指针变量的地址应该被强制转换为 (void **) 类型，因为函数期望一个通用指针；内存分配函数是一个通用函数，不受限于任何特定类型的对象。这个参数允许 `cudaMalloc` 函数将分配的内存地址写入提供的指针变量中，而不管其类型如何。调用内核的主机代码将此指针值传递给需要访问分配内存对象的内核。`cudaMalloc` 函数的第二个参数给出了要分配的数据大小，以字节数表示。这个第二个参数的使用与 C 的 malloc 函数的 size 参数一致。

我们现在用以下简单代码示例来说明 `cudaMalloc` 和 `cudaFree` 的使用：

```cuda
float *A_d
int size=n*sizeof(float);
cudaMalloc((void**)&`A_d`, size);
. . .
cudaFree(A_d);
```

这是图 2.5 中示例的延续。为了清晰起见，我们给指针变量加上后缀“_d”，以表明它指向设备全局内存中的对象。传递给 `cudaMalloc` 的第一个参数是指针 `A_d` 的地址（即 `&A_d`）并强制转换为 void 指针。当 `cudaMalloc` 返回时，`A_d` 将指向为 A 向量分配的设备全局内存区域。传递给 `cudaMalloc` 的第二个参数是要分配的区域大小。
由于 size 是以字节数表示的，程序员在确定 size 的值时需要将数组中的元素数量转换为字节数。例如，在为包含 n 个单精度浮点元素的数组分配空间时，size 的值将是 n 乘以单精度浮点数的大小，在当前计算机中为 4 字节。因此 size 的值将是 n*4。在计算完成后，调用 `cudaFree` 并以指针 `A_d` 作为参数来释放 A 向量在设备全局内存中的存储空间。
注意 `cudaFree` 不需要改变 `A_d` 的值；它只需要使用 `A_d` 的值将分配的内存返回到可用池。因此，只有 `A_d` 的值而不是地址作为参数传递。`A_d`、`B_d` 和 `C_d` 中的地址指向设备全局内存中的位置。这些地址不应该在主机代码中取消引用。它们应该用于调用 API 函数和内核函数。在主机代码中取消引用设备全局内存指针可能会导致异常或其他类型的运行时错误。
读者应该用 `B_d` 和 `C_d` 指针变量的类似声明以及相应的 `cudaMalloc` 调用来完成图 2.5 中 `vecAdd` 示例的第 1 部分。此外，图 2.5 中的第 3 部分可以用 `cudaFree` 调用 `B_d` 和 `C_d` 来完成。

`cudaMemcpy()`：
- **用途**：用于在主机（CPU）和设备（GPU）之间传输内存数据。
- **参数**：
  1. 目标地址的指针：指向要将数据复制到的目标位置。
  2. 源地址的指针：指向要从中复制数据的源位置。
  3. 复制的字节数：指定要复制的数据量，以字节为单位。
  4. 传输类型/方向：指定数据传输的类型或方向，如从主机到设备（`cudaMemcpyHostToDevice`），从设备到主机（`cudaMemcpyDeviceToHost`），或在设备内存之间传输（`cudaMemcpyDeviceToDevice`）。
> 图 2.7 cudaMemcpy 宿主与设备之间的API 函数

一旦主机代码在设备全局内存中为数据对象分配了空间，它就可以请求将数据从主机传输到设备。这是通过调用 CUDA API 函数之一来实现的。图 2.7 显示了这样一个 API 函数 `cudaMemcpy`。`cudaMemcpy` 函数接受四个参数。第一个参数是指向要复制的数据对象的目标位置的指针。第二个参数指向源位置。第三个参数指定要复制的字节数。第四个参数指示涉及复制的内存类型：从主机到主机、从主机到设备、从设备到主机以及从设备到设备。例如，内存复制函数可以用来将数据从设备全局内存中的一个位置复制到设备全局内存中的另一个位置。`vecAdd` 函数在向量加法之前调用 `cudaMemcpy` 函数将 `A_h` 和 `B_h` 向量从主机内存复制到设备内存中的 `A_d` 和 `B_d`，并在加法完成后将 `C_d` 向量从设备内存复制到主机内存中的 `C_h`。假设 `A_h`、`B_h`、`A_d`、`B_d` 和 size 的值已经设置，如前所述，下面是三个 `cudaMemcpy` 调用。两个符号常量 `cudaMemcpyHostToDevice` 和 `cudaMemcpyDeviceToHost` 是 CUDA 编程环境中已识别的预定义常量。注意，通过适当排列源指针和目标指针并使用适当的传输类型常量，可以使用相同的函数在两个方向上传输数据。

```cuda
cudaMemcpy(A_d, A_h, size, cudaMemcpyHostToDevice);
cudaMemcpy(B_d, B_h, size, cudaMemcpyHostToDevice);
...
cudaMemcpy(C_h, C_d, size, cudaMemcpyDeviceToHost);
```

```cuda
01 void vecAdd(float* A_h, float* B_h, float* C_h, int n) {
02     int size = n * sizeof(float); // 计算需要分配的内存大小（字节数）
03     float *A_d, *B_d, *C_d; // 定义指向设备内存的指针
04
05     cudaMalloc((void **) &A_d, size); // 在设备内存中分配空间给向量 A
06     cudaMalloc((void **) &B_d, size); // 在设备内存中分配空间给向量 B
07     cudaMalloc((void **) &C_d, size); // 在设备内存中分配空间给向量 C
08
09     cudaMemcpy(A_d, A_h, size, cudaMemcpyHostToDevice); // 从主机内存复制向量 A 到设备内存
10     cudaMemcpy(B_d, B_h, size, cudaMemcpyHostToDevice); // 从主机内存复制向量 B 到设备内存
11
12     // 调用内核代码 – 将在后面展示
13     ...
14
15     cudaMemcpy(C_h, C_d, size, cudaMemcpyDeviceToHost); // 从设备内存复制向量 C 到主机内存
16
17     cudaFree(A_d); // 释放设备内存中分配给向量 A 的空间
18     cudaFree(B_d); // 释放设备内存中分配给向量 B 的空间
19     cudaFree(C_d); // 释放设备内存中分配给向量 C 的空间
20 }
```
> 图 2.8 完整`vecAdd()`的示例

总而言之，图 2.4 中的主程序调用 `vecAdd`，该函数也在主机上执行。图 2.5 中概述的 `vecAdd` 函数在设备全局内存中分配空间，请求数据传输，并调用执行实际向量加法的内核。我们将这种类型的主机代码称为调用内核的存根。我们在图 2.8 中展示了 `vecAdd` 函数的更完整版本。
与图 2.5 相比，图 2.8 中的 `vecAdd` 函数在第 1 部分和第 3 部分是完整的。第 1 部分为 `A_d`、`B_d` 和 `C_d` 分配设备全局内存，并将 `A_h` 传输到 `A_d`，将 `B_h` 传输到 `B_d`。这是通过调用 `cudaMalloc` 和 `cudaMemcpy` 函数来完成的。读者被鼓励编写自己的函数调用并填写适当的参数值，并将他们的代码与图 2.8 中显示的代码进行比较。
第 2 部分调用内核，将在以下小节中描述。第 3 部分将向量和数据从设备复制到主机，以便这些值在主函数中可用。这是通过调用 `cudaMemcpy` 函数来完成的。然后它通过调用 `cudaFree` 函数释放设备全局内存中的 `A_d`、`B_d` 和 `C_d` 的内存（图 2.9）。

> ### CUDA 中的错误检查和处理
>在程序中进行错误检查和处理非常重要。CUDA API 函数会返回标志，指示在处理请求时是否发生了错误。大多数错误是由于调用中使用了不适当的参数值造成的。
>为了简洁起见，我们在示例中不会显示错误检查代码。然而，在实际使用中，我们应该在函数调用周围添加代码来测试错误条件，并打印错误信息，以便用户能意识到错误的发生。以下是一个简单的错误检查代码示例：
>
>> ```shell
>> cudaError_t err = cudaMalloc((void**) &A_d, size);
>> if (err != cudaSuccess) {
>>    printf("%s in %s at line %d\n", cudaGetErrorString(err), __FILE__, __LINE__);
>>    exit(EXIT_FAILURE);
>> }
>> ```
> ### 代码解释
>1. **错误检查**：
>   `cudaError_t err = cudaMalloc((void**) &A_d, size);`：调用 `cudaMalloc` 函数，并将返回的错误码存储在 `err` 中。
> 2. **检查错误**：
>   `if (err != cudaSuccess)`：检查 `err` 是否表示成功。如果不是，说明发生了错误。
> 3. **打印错误信息**：
>   `printf("%s in %s at line %d\n", cudaGetErrorString(err), __FILE__, __LINE__);`：打印错误信息。`cudaGetErrorString(err)` 会返回错误的详细描述。`__FILE__` 和 `__LINE__` 分别表示出错的文件名和行号。
> 4. **退出程序**：
>   `exit(EXIT_FAILURE);`：如果发生错误，退出程序并返回失败状态。


