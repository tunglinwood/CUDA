# 2_异构数据并行计算

* [2.1 数据并行性](https://github.com/tunglinwood/CUDA/blob/main/2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97.md#21-%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E6%80%A7)
* [2.2 CUDA C程序结构](https://github.com/tunglinwood/CUDA/blob/main/2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97.md#22-cuda-c%E7%A8%8B%E5%BA%8F%E7%BB%93%E6%9E%84)
* [2.3 向量加法内核](https://github.com/tunglinwood/CUDA/blob/main/2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97.md#23-%E5%90%91%E9%87%8F%E5%8A%A0%E6%B3%95%E5%86%85%E6%A0%B8)
* [2.4 设备全局内存和数据传输](https://github.com/tunglinwood/CUDA/blob/main/2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97.md#24-%E8%AE%BE%E5%A4%87%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98%E5%92%8C%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93)
* [2.5 内核函数和线程](https://github.com/tunglinwood/CUDA/blob/main/2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97.md#25-%E5%86%85%E6%A0%B8%E5%87%BD%E6%95%B0%E5%92%8C%E7%BA%BF%E7%A8%8B)
* [2.6 调用内核函数](https://github.com/tunglinwood/CUDA/blob/main/2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97.md#26-%E8%B0%83%E7%94%A8%E5%86%85%E6%A0%B8%E5%87%BD%E6%95%B0)
* [2.7 编译](https://github.com/tunglinwood/CUDA/blob/main/2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97.md#27-%E7%BC%96%E8%AF%91)
* [2.8 总结](https://github.com/tunglinwood/CUDA/blob/main/2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97.md#28-%E6%80%BB%E7%BB%93)
* [练习](https://github.com/tunglinwood/CUDA/blob/main/2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97.md#%E7%BB%83%E4%B9%A0)

数据并行性是指对数据集不同部分的计算工作可以相互独立地完成，因此可以并行进行。许多应用程序表现出大量的数据并行性，使它们适合于可扩展的并行执行。因此，并行程序员必须熟悉数据并行性的概念以及用于编写利用数据并行性的代码的并行编程语言构造。在本章中，我们将使用CUDA C语言构造开发一个简单的数据并行程序。

|[top](https://github.com/tunglinwood/CUDA/blob/main/2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97.md#2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97)|
|-|

## 2.1 数据并行性
当现代软件应用程序运行缓慢时，问题通常出在数据上——需要处理的数据太多。图像处理应用程序处理具有数百万到万亿像素的图像或视频。科学应用程序使用数十亿网格点建模流体动力学。分子动力学应用程序必须模拟成千上亿原子之间的相互作用。航空公司调度处理数千次航班、机组人员和机场登机口。这些像素、粒子、网格点、相互作用、航班等大多数通常可以独立处理。例如，在图像处理中，将彩色像素转换为灰度仅需要该像素的数据。模糊图像时，将每个像素的颜色与附近像素的颜色平均，只需该小区域内的像素数据。即使是看似全局的操作，例如找到图像中所有像素的平均亮度，也可以分解为许多可以独立执行的小计算。这种对不同数据片段的独立评估是数据并行性的基础。编写数据并行代码意味着围绕数据（重新）组织计算，以便我们可以并行执行结果的独立计算，从而更快地完成整体任务——通常要快得多。

![image](https://github.com/user-attachments/assets/d49864d9-1a50-4d0f-9d8a-e44e33c07d55)
> 图2.1

让我们通过一个彩色转灰度转换的例子来说明数据并行性的概念。图2.1显示了一张由许多像素组成的彩色图像（左侧），每个像素包含一个从0（黑色）到1（全强度）变化的红色、绿色和蓝色分数值（r, g, b）。
要将彩色图像（图2.1的左侧）转换为灰度图像（右侧），我们需要通过应用以下加权和公式来计算每个像素的亮度值L：
$$ L = 0.21r + 0.72g + 0.07b $$

> ### RGB颜色图像表示
> 在RGB表示中，图像中的每个像素被存储为一个(r, g, b)值的元组。图像行的格式为(r g b) (r g b) . . . (r g b)，如下面的概念图所示。每个元组指定了红色（R）、绿色（G）和蓝色（B）的混合比例。换句话说，对于每个像素，r、g和b值代表了在像素呈现时红色、绿色和蓝色光源的强度（0表示黑暗，1表示全强度）。
> ![image](https://github.com/user-attachments/assets/47c6ad49-4007-45a9-9083-1fe5a27f68e1)
> 这三种颜色的实际混合比例在行业指定的颜色空间中有所不同。在这里，AdobeRGB颜色空间中三种颜色的有效组合被显示为三角形的内部区域。
  每种混合的垂直坐标（y值）和水平坐标（x值）显示了像素强度的G和R分量的比例。剩余部分（1-y-x）的像素强度分配给B。为了呈现图像，每个像素的r、g、b值用于计算像素的总强度（亮度）以及混合系数（x、y、1-y-x）。

如果我们将输入视为一个由RGB值组成的数组I，而输出视为相应的亮度值数组O，我们将得到如图2.2所示的简单计算结构。例如，O[0]是通过根据上述公式计算I[0]中的RGB值的加权和生成的；O[1]是通过计算I[1]中的RGB值的加权和生成的；O[2]是通过计算I[2]中的RGB值的加权和生成的；依此类推。这些每像素的计算彼此之间没有依赖关系。所有这些计算都可以独立进行。显然，颜色到灰度的转换表现出丰富的数据并行性。当然，完整应用中的数据并行性可能更为复杂，本书的大部分内容都致力于教授找到和利用数据并行性所需的并行思维。

![image](https://github.com/user-attachments/assets/812f37f6-7c27-43fa-983b-91b0ea98e90b)
> 图2.2

> ### 任务并行与数据并行
> 数据并行不是并行编程中唯一使用的并行类型。任务并行在并行编程中也被广泛使用。任务并行通常通过应用程序的任务分解来实现。例如，一个简单的应用程序可能需要进行向量加法和矩阵向量乘法。每一个都是一个任务。如果这两个任务可以独立完成，就存在任务并行性。I/O和数据传输也是常见的任务来源。
> 在大型应用程序中，通常有大量的独立任务，因此有大量的任务并行性。例如，在一个分子动力学模拟器中，自然任务的列表包括振动力、旋转力、非键合力的邻居识别、非键合力、速度和位置、以及基于速度和位置的其他物理属性。
> 一般而言，数据并行是并行程序可扩展性的主要来源。对于大数据集，通常可以找到丰富的数据并行性，从而能够利用大规模并行处理器，并随着每一代拥有更多执行资源的硬件的发展提升应用性能。然而，任务并行在实现性能目标方面也可以发挥重要作用。当我们介绍流（streams）时，将会进一步讨论任务并行性。

|[top](https://github.com/tunglinwood/CUDA/blob/main/2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97.md#2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97)|
|-|

## 2.2 CUDA C程序结构

现在我们准备学习如何编写CUDA C程序，以利用数据并行性实现更快的执行速度。CUDA C在流行的ANSI C编程语言基础上进行了最小化的新语法和库函数扩展，使程序员能够针对包含CPU核心和大规模并行GPU的异构计算系统进行编程。顾名思义，CUDA C是建立在NVIDIA的CUDA平台上的。

CUDA目前是大规模并行计算中最成熟的框架。它在高性能计算行业中得到了广泛应用，并且在最常见的操作系统上提供了编译器、调试器和性能分析工具等基本工具。

CUDA C程序的结构反映了计算机中主机（CPU）和一个或多个设备（GPU）的共存。每个CUDA C源文件可以包含主机代码和设备代码的混合。默认情况下，任何传统的C程序都是只包含主机代码的CUDA程序。可以在任何源文件中添加设备代码。设备代码通过特殊的CUDA C关键字清晰地标记出来。设备代码包括函数或内核，其代码以数据并行的方式执行。

![image](https://github.com/user-attachments/assets/74b9401b-0903-41b2-8b81-35c52b54d42b)
> 图2.3 CUDA程序执行

CUDA程序的执行如图2.3所示。执行从主机代码（CPU串行代码）开始。当调用内核函数时，会在设备上启动大量线程以执行内核。所有由内核调用启动的线程统称为一个网格。这些线程是CUDA平台上并行执行的主要载体。图2.3展示了两个线程网格的执行。当一个网格的所有线程完成执行时，该网格终止，执行继续在主机上进行，直到启动另一个网格。

请注意，图2.3显示了一个简化的模型，其中CPU执行和GPU执行不重叠。许多异构计算应用程序管理CPU和GPU执行的重叠，以充分利用CPU和GPU。

启动一个网格通常会生成许多线程以利用数据并行性。在颜色到灰度转换的示例中，每个线程可以用于计算输出数组O的一个像素。在这种情况下，网格启动生成的线程数等于图像中的像素数。对于大型图像，将生成大量线程。由于高效的硬件支持，CUDA程序员可以假设这些线程生成和调度所需的时钟周期非常少。这一假设与传统的CPU线程形成对比，后者通常需要数千个时钟周期来生成和调度。在下一章中，我们将展示如何实现颜色到灰度转换和图像模糊内核。在本章的其余部分，我们将以向量加法为示例进行讲解，以保持简单。

> ### 线程
> 线程是现代计算机中处理器执行顺序程序的一种简化视图。一个线程包括程序的代码、正在执行的代码位置以及其变量和数据结构的值。就用户而言，线程的执行是顺序的。用户可以使用源级调试器通过逐条执行语句、查看将要执行的下一条语句并检查变量和数据结构的值来监控线程的进展。
> 线程已经在编程中使用了很多年。如果程序员希望在应用程序中启动并行执行，他/她可以使用线程库或特殊语言创建和管理多个线程。在CUDA中，每个线程的执行同样是顺序的。CUDA程序通过调用内核函数来启动并行执行，这会导致底层运行机制启动一个线程网格，以并行处理数据的不同部分。

|[top](https://github.com/tunglinwood/CUDA/blob/main/2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97.md#2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97)|
|-|

## 2.3 向量加法内核

我们使用向量加法来演示CUDA C程序的结构。向量加法可以说是最简单的数据并行计算，相当于顺序编程中的 "Hello World"。在展示向量加法的内核代码之前，先回顾一下传统向量加法（主机代码）函数的工作原理是有帮助的。图2.4显示了一个简单的传统C程序，它由一个主函数和一个向量加法函数组成。在我们的所有示例中，若需要区分主机和设备数据，我们将给主机使用的变量名后缀加上“_h”，给设备使用的变量名后缀加上“_d”，以提醒自己这些变量的预期用途。由于图2.4中只有主机代码，因此我们只看到以“_h”结尾的变量。

假设要相加的向量存储在主程序中分配和初始化的数组A和B中。输出向量存储在数组C中，该数组也在主程序中分配。为了简洁起见，我们不显示A、B和C在主函数中如何分配或初始化的细节。指向这些数组的指针与包含向量长度的变量N一起传递给`vecAdd`函数。请注意，`vecAdd`函数的参数加上了“_h”后缀，以强调它们由主机使用。当我们在接下来的步骤中引入设备代码时，这种命名约定将很有帮助。

```cuda
// Compute vector sum C_h = A_h + B_h
void vecAdd(float* A_h, float* B_h, float* C_h, int n) {
  for (int i = 0; i < n; ++i) {
    C_h[i] = A_h[i] + B_h[i];
  }
}
int main() {
    // Memory allocation for arrays A, B, and C
    // I/O to read A and B, N elements each
    ...
    vecAdd(A, B, C, N);
}

```
> 图2.4 传统的向量加法的简单 C 代码示例

图2.4中的`vecAdd`函数使用`for`循环迭代向量元素。在第i次迭代中，输出元素C_h[i]接收A_h[i]和B_h[i]的和。向量长度参数n用于控制循环，以使迭代次数与向量长度匹配。该函数通过指针A_h、B_h和C_h分别读取A和B的元素并写入C的元素。当`vecAdd`函数返回时，主函数中的后续语句可以访问C的新内容。

> C语言中的指针
> 图2.4中的函数参数A、B和C是指针。在C语言中，可以使用指针来访问变量和数据结构。
虽然浮点变量V可以这样声明：
```c
float V;
```
但指针变量P可以这样声明：
```c
float *P;
```
> 通过语句`P = &V`将V的地址赋值给P，我们让P“指向”V。`*P`成为V的同义词。例如，`U = *P`将V的值赋给U。再比如，`*P = 3`将V的值改为3。
> 在C程序中，可以通过指向其第0个元素的指针来访问数组。例如，语句`P = &(A[0])`使P指向数组A的第0个元素。`P[i]`成为`A[i]`的同义词。实际上，数组名A本身就是指向其第0个元素的指针。
> 在图2.4中，将数组名A作为第一个参数传递给函数调用`vecAdd`，使得函数的第一个参数A_h指向A的第0个元素。因此，函数体内的A_h[i]可以用来访问主函数中数组A的A[i]。
> 关于C语言中指针的详细用法，请参阅Patt和Patel（Patt & Patel，2020）的易于理解的解释。

![image](https://github.com/user-attachments/assets/d8a2e5dc-87f4-4fd0-9c9e-db0333b9c14d)

```cuda
void vecAdd(float* A, float* B, float* C, int n) {
    int size = n* sizeof(float);
    float *d_A *d_B, *d_C;
    // Part 1: Allocate device memory for A, B, and C
    // Copy A and B to device memory
    //...
    // Part 2: Call kernel – to launch a grid of threads
    // to perform the actual vector addition
    // ...
    // Part 3: Copy C from the device memory
    // Free device vectors
    //...
  
}
```  
> 图2.5 将工作移至设备的修改后的`vecAdd`函数概述


一种并行执行向量加法的直接方法是修改`vecAdd`函数并将其计算移至设备。图2.5显示了这种修改后的`vecAdd`函数的结构。函数的第1部分在设备（GPU）内存中分配空间以存储A、B和C向量的副本，并将A和B向量从主机内存复制到设备内存。第2部分调用实际的向量加法内核以在设备上启动一个线程网格。第3部分将和向量C从设备内存复制到主机内存，并释放设备内存中的三个数组。

请注意，修改后的`vecAdd`函数实际上是一个外包代理，它将输入数据传输到设备，激活设备上的计算，并从设备收集结果。代理以这样一种方式进行，使得主程序甚至不需要知道向量加法现在实际上是在设备上完成的。实际上，由于所有数据来回复制，这种“透明”的外包模型可能非常低效。人们通常会将大型和重要的数据结构保留在设备上，并简单地从主机代码调用设备函数。然而，现在我们将使用简化的透明模型来介绍基本的CUDA C程序结构。修订函数的详细信息以及内核函数的组成方式将是本章其余部分的主题。

|[top](https://github.com/tunglinwood/CUDA/blob/main/2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97.md#2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97)|
|-|

## 2.4 设备全局内存和数据传输
在当前的 CUDA 系统中，设备通常是带有自己动态随机存取存储器（DRAM）的硬件卡，这种内存被称为设备全局内存，简称为全局内存。例如，NVIDIA Volta V100 带有 16GB 或 32GB 的全局内存。称其为“全局”内存是为了将其与其他类型的设备内存区分开来，其他类型的设备内存也对程序员可访问。关于 CUDA 内存模型和不同类型设备内存的详细信息将在第 5 章“内存架构和数据局部性”中讨论。

对于向量加法内核，在调用内核之前，程序员需要在设备全局内存中分配空间，并将数据从主机内存传输到设备全局内存中的分配空间。这对应于图 2.5 的第 1 部分。同样，在设备执行之后，程序员需要将结果数据从设备全局内存传输回主机内存，并释放设备全局内存中不再需要的分配空间。这对应于图 2.5 的第 3 部分。CUDA 运行时系统（通常在主机上运行）提供了应用程序编程接口（API）函数来代表程序员执行这些活动。从此以后，我们将简单地说数据从主机传输到设备，作为数据从主机内存复制到设备全局内存的简写。相同的适用于相反方向。

在图 2.5 中，`vecAdd` 函数的第 1 部分和第 3 部分需要使用 CUDA API 函数来分配设备全局内存用于 A、B 和 C；将 A 和 B 从主机传输到设备；在向量加法之后将 C 从设备传输到主机；并释放 A、B 和 C 的设备全局内存。我们将首先解释内存分配和释放函数。

`cudaMalloc()`：
- **用途**：在设备（GPU）全局内存中分配内存。
- **参数**：
  1. 指向已分配对象的指针的地址：这是一个指向指针的指针（`void**`），将被设置为指向设备上分配的内存。
  2. 以字节为单位的已分配对象的大小：指定要分配多少内存，以字节为单位。

`cudaFree()`：
- **用途**：释放在设备（GPU）全局内存中分配的内存。
- **参数**：
  - 被释放对象的指针：接受一个指针，该指针指向先前用`cudaMalloc()`分配的内存。它不会更改指针本身的值，但会释放它所指向的内存。
> 图 2.6 cudaMalloc, cudaFree是CUDA编程中管理GPU内存的关键，允许在并行计算期间高效地进行数据传输和资源利用。

图 2.6 显示了两个用于分配和释放设备全局内存的 API 函数。
`cudaMalloc` 函数可以从主机代码中调用，以为对象分配一块设备全局内存。读者应该注意到 `cudaMalloc` 和标准 C 运行时库 malloc 函数之间的惊人相似性。这是有意为之的；CUDA C 是 C 的最小扩展版。CUDA C 使用标准 C 运行时库 malloc 函数来管理主机内存，并将 `cudaMalloc` 作为对 C 运行时库的扩展。通过尽可能接近原始 C 运行时库的接口，CUDA C 最小化了 C 程序员重新学习这些扩展的时间。

`cudaMalloc` 函数的第一个参数是指针变量的地址，该变量将被设置为指向分配的对象。指针变量的地址应该被强制转换为 (void **) 类型，因为函数期望一个通用指针；内存分配函数是一个通用函数，不受限于任何特定类型的对象。这个参数允许 `cudaMalloc` 函数将分配的内存地址写入提供的指针变量中，而不管其类型如何。调用内核的主机代码将此指针值传递给需要访问分配内存对象的内核。`cudaMalloc` 函数的第二个参数给出了要分配的数据大小，以字节数表示。这个第二个参数的使用与 C 的 malloc 函数的 size 参数一致。

我们现在用以下简单代码示例来说明 `cudaMalloc` 和 `cudaFree` 的使用：

```cuda
float *A_d
int size=n*sizeof(float);
cudaMalloc((void**)&`A_d`, size);
. . .
cudaFree(A_d);
```

这是图 2.5 中示例的延续。为了清晰起见，我们给指针变量加上后缀“_d”，以表明它指向设备全局内存中的对象。传递给 `cudaMalloc` 的第一个参数是指针 `A_d` 的地址（即 `&A_d`）并强制转换为 void 指针。当 `cudaMalloc` 返回时，`A_d` 将指向为 A 向量分配的设备全局内存区域。传递给 `cudaMalloc` 的第二个参数是要分配的区域大小。
由于 size 是以字节数表示的，程序员在确定 size 的值时需要将数组中的元素数量转换为字节数。例如，在为包含 n 个单精度浮点元素的数组分配空间时，size 的值将是 n 乘以单精度浮点数的大小，在当前计算机中为 4 字节。因此 size 的值将是 n*4。在计算完成后，调用 `cudaFree` 并以指针 `A_d` 作为参数来释放 A 向量在设备全局内存中的存储空间。
注意 `cudaFree` 不需要改变 `A_d` 的值；它只需要使用 `A_d` 的值将分配的内存返回到可用池。因此，只有 `A_d` 的值而不是地址作为参数传递。`A_d`、`B_d` 和 `C_d` 中的地址指向设备全局内存中的位置。这些地址不应该在主机代码中取消引用。它们应该用于调用 API 函数和内核函数。在主机代码中取消引用设备全局内存指针可能会导致异常或其他类型的运行时错误。
读者应该用 `B_d` 和 `C_d` 指针变量的类似声明以及相应的 `cudaMalloc` 调用来完成图 2.5 中 `vecAdd` 示例的第 1 部分。此外，图 2.5 中的第 3 部分可以用 `cudaFree` 调用 `B_d` 和 `C_d` 来完成。

`cudaMemcpy()`：
- **用途**：用于在主机（CPU）和设备（GPU）之间传输内存数据。
- **参数**：
  1. 目标地址的指针：指向要将数据复制到的目标位置。
  2. 源地址的指针：指向要从中复制数据的源位置。
  3. 复制的字节数：指定要复制的数据量，以字节为单位。
  4. 传输类型/方向：指定数据传输的类型或方向，如从主机到设备（`cudaMemcpyHostToDevice`），从设备到主机（`cudaMemcpyDeviceToHost`），或在设备内存之间传输（`cudaMemcpyDeviceToDevice`）。
> 图 2.7 cudaMemcpy 宿主与设备之间的API 函数

一旦主机代码在设备全局内存中为数据对象分配了空间，它就可以请求将数据从主机传输到设备。这是通过调用 CUDA API 函数之一来实现的。图 2.7 显示了这样一个 API 函数 `cudaMemcpy`。`cudaMemcpy` 函数接受四个参数。第一个参数是指向要复制的数据对象的目标位置的指针。第二个参数指向源位置。第三个参数指定要复制的字节数。第四个参数指示涉及复制的内存类型：从主机到主机、从主机到设备、从设备到主机以及从设备到设备。例如，内存复制函数可以用来将数据从设备全局内存中的一个位置复制到设备全局内存中的另一个位置。`vecAdd` 函数在向量加法之前调用 `cudaMemcpy` 函数将 `A_h` 和 `B_h` 向量从主机内存复制到设备内存中的 `A_d` 和 `B_d`，并在加法完成后将 `C_d` 向量从设备内存复制到主机内存中的 `C_h`。假设 `A_h`、`B_h`、`A_d`、`B_d` 和 size 的值已经设置，如前所述，下面是三个 `cudaMemcpy` 调用。两个符号常量 `cudaMemcpyHostToDevice` 和 `cudaMemcpyDeviceToHost` 是 CUDA 编程环境中已识别的预定义常量。注意，通过适当排列源指针和目标指针并使用适当的传输类型常量，可以使用相同的函数在两个方向上传输数据。

```cuda
cudaMemcpy(A_d, A_h, size, cudaMemcpyHostToDevice);
cudaMemcpy(B_d, B_h, size, cudaMemcpyHostToDevice);
...
cudaMemcpy(C_h, C_d, size, cudaMemcpyDeviceToHost);
```

```cuda
01 void vecAdd(float* A_h, float* B_h, float* C_h, int n) {
02     int size = n * sizeof(float); // 计算需要分配的内存大小（字节数）
03     float *A_d, *B_d, *C_d; // 定义指向设备内存的指针
04
05     cudaMalloc((void **) &A_d, size); // 在设备内存中分配空间给向量 A
06     cudaMalloc((void **) &B_d, size); // 在设备内存中分配空间给向量 B
07     cudaMalloc((void **) &C_d, size); // 在设备内存中分配空间给向量 C
08
09     cudaMemcpy(A_d, A_h, size, cudaMemcpyHostToDevice); // 从主机内存复制向量 A 到设备内存
10     cudaMemcpy(B_d, B_h, size, cudaMemcpyHostToDevice); // 从主机内存复制向量 B 到设备内存
11
12     // 调用内核代码 – 将在后面展示
13     ...
14
15     cudaMemcpy(C_h, C_d, size, cudaMemcpyDeviceToHost); // 从设备内存复制向量 C 到主机内存
16
17     cudaFree(A_d); // 释放设备内存中分配给向量 A 的空间
18     cudaFree(B_d); // 释放设备内存中分配给向量 B 的空间
19     cudaFree(C_d); // 释放设备内存中分配给向量 C 的空间
20 }
```
> 图 2.8 完整`vecAdd()`的示例

总而言之，图 2.4 中的主程序调用 `vecAdd`，该函数也在主机上执行。图 2.5 中概述的 `vecAdd` 函数在设备全局内存中分配空间，请求数据传输，并调用执行实际向量加法的内核。我们将这种类型的主机代码称为调用内核的存根。我们在图 2.8 中展示了 `vecAdd` 函数的更完整版本。
与图 2.5 相比，图 2.8 中的 `vecAdd` 函数在第 1 部分和第 3 部分是完整的。第 1 部分为 `A_d`、`B_d` 和 `C_d` 分配设备全局内存，并将 `A_h` 传输到 `A_d`，将 `B_h` 传输到 `B_d`。这是通过调用 `cudaMalloc` 和 `cudaMemcpy` 函数来完成的。读者被鼓励编写自己的函数调用并填写适当的参数值，并将他们的代码与图 2.8 中显示的代码进行比较。
第 2 部分调用内核，将在以下小节中描述。第 3 部分将向量和数据从设备复制到主机，以便这些值在主函数中可用。这是通过调用 `cudaMemcpy` 函数来完成的。然后它通过调用 `cudaFree` 函数释放设备全局内存中的 `A_d`、`B_d` 和 `C_d` 的内存（图 2.9）。

> ### CUDA 中的错误检查和处理
>在程序中进行错误检查和处理非常重要。CUDA API 函数会返回标志，指示在处理请求时是否发生了错误。大多数错误是由于调用中使用了不适当的参数值造成的。
>为了简洁起见，我们在示例中不会显示错误检查代码。然而，在实际使用中，我们应该在函数调用周围添加代码来测试错误条件，并打印错误信息，以便用户能意识到错误的发生。以下是一个简单的错误检查代码示例：
>
>> ```shell
>> cudaError_t err = cudaMalloc((void**) &A_d, size);
>> if (err != cudaSuccess) {
>>    printf("%s in %s at line %d\n", cudaGetErrorString(err), __FILE__, __LINE__);
>>    exit(EXIT_FAILURE);
>> }
>> ```
> ### 代码解释
>1. **错误检查**：
>   `cudaError_t err = cudaMalloc((void**) &A_d, size);`：调用 `cudaMalloc` 函数，并将返回的错误码存储在 `err` 中。
> 2. **检查错误**：
>   `if (err != cudaSuccess)`：检查 `err` 是否表示成功。如果不是，说明发生了错误。
> 3. **打印错误信息**：
>   `printf("%s in %s at line %d\n", cudaGetErrorString(err), __FILE__, __LINE__);`：打印错误信息。`cudaGetErrorString(err)` 会返回错误的详细描述。`__FILE__` 和 `__LINE__` 分别表示出错的文件名和行号。
> 4. **退出程序**：
>   `exit(EXIT_FAILURE);`：如果发生错误，退出程序并返回失败状态。

|[top](https://github.com/tunglinwood/CUDA/blob/main/2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97.md#2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97)|
|-|

## 2.5 内核函数和线程

现在我们可以更深入地讨论 CUDA C 的内核函数以及调用这些内核函数的效果。在 CUDA C 中，内核函数指定了在并行阶段由所有线程执行的代码。由于所有这些线程执行相同的代码，CUDA C 编程是一种单程序多数据（SPMD）并行编程风格的实例，这是一种在并行计算系统中非常流行的编程风格。

当程序的主机代码调用内核时，CUDA 运行时系统会启动一个线程网格，该网格组织成两级层次结构。每个网格被组织为线程块的数组，我们将简化地称之为块。每个网格的所有块大小相同；在当前系统中，每个块最多可以包含 1024 个线程。图 2.9 显示了一个示例，其中每个块包含 256 个线程。每个线程由一个带有线程索引的方框表示。

> ### 内置变量
> 许多编程语言都有内置变量，这些变量具有特殊的含义和目的。这些变量的值通常由运行时系统预初始化，并且在程序中通常是只读的。程序员应避免将这些变量重新定义为其他用途。
> 每个线程块中的线程总数由主机代码在调用内核时指定。相同的内核可以在主机代码的不同部分使用不同数量的线程进行调用。对于给定的线程网格，线程块中的线程数量可以通过一个名为 `blockDim` 的内置变量获得。

![image](https://github.com/user-attachments/assets/e7c155b4-4164-44b5-bfce-833ff2fe2124)
> 图 2.9 计算全局索引
`blockDim` 变量是一个包含三个无符号整数字段（x、y 和 z）的结构，帮助程序员将线程组织成一维、二维或三维数组。对于一维组织，只使用 x 字段。对于二维组织，使用 x 和 y 字段。对于三维结构，使用 x、y 和 z 三个字段。选择线程组织的维度通常反映了数据的维度。这是因为线程的创建目的是并行处理数据，因此线程的组织反映了数据的组织。在图 2.9 中，由于数据是一维向量，每个线程块组织为一维线程数组。`blockDim.x` 变量的值表示每个块中的线程总数，在图 2.9 中为 256。通常建议线程块中每个维度的线程数量是 32 的倍数，以提高硬件效率。我们将在后面详细讨论这一点。

CUDA 内核具有两个额外的内置变量（`threadIdx` 和 `blockIdx`），允许线程相互区分并确定每个线程处理的数据区域。`threadIdx` 变量为每个线程提供一个在块内的唯一坐标。在图 2.9 中，由于我们使用的是一维线程组织，只使用 `threadIdx.x`。图 2.9 中每个线程的 `threadIdx.x` 值显示在小的阴影框中。每个块中的第一个线程的 `threadIdx.x` 值为 0，第二个线程的值为 1，第三个线程的值为 2，依此类推。

> ### 层次结构组织
> 像 CUDA 线程一样，许多现实世界的系统也是以层次结构组织的。例如，美国电话系统就是一个很好的例子。在顶层，电话系统由“区域”组成，每个区域对应一个地理区域。相同区域内的所有电话线都具有相同的 3 位“区域代码”。一个电话区域有时比一个城市还大。例如，伊利诺伊州中部的许多县和城市都在同一个电话区域内，共享相同的区域代码 217。在一个区域内，每条电话线都有一个七位数的本地电话号码，这允许每个区域最多有大约一千万个号码。
> 可以将每条电话线视为一个 CUDA 线程，将区域代码视为 `blockIdx` 的值，将七位数的本地号码视为 `threadIdx` 的值。这种层次结构组织允许系统拥有非常多的电话线，同时保持“局部性”以拨打相同区域的电话。也就是说，当拨打同一区域的电话时，呼叫者只需拨打本地号码。只要我们大多数通话在本地区域内，就很少需要拨打区域代码。如果偶尔需要拨打另一个区域的电话，我们拨打 1 和区域代码，然后是本地号码。（这就是为什么任何区域的本地号码都不应以 1 开头的原因。）CUDA 线程的层次结构组织也提供了一种局部性。我们将很快研究这种局部性。

`blockIdx` 变量为块中的所有线程提供一个公共的块坐标。在图 2.9 中，第一个块中的所有线程在 `blockIdx.x` 变量中都有值 0，第二个线程块中的线程值为 1，以此类推。使用电话系统的类比，可以将 `threadIdx.x` 视为本地电话号码，将 `blockIdx.x` 视为区域代码。两者结合为整个国家的每个电话线提供一个唯一的电话号码。类似地，每个线程可以将其 `threadIdx` 和 `blockIdx` 值组合起来，在整个网格中为自己创建一个唯一的全局索引。

在图 2.9 中，一个唯一的全局索引 i 计算为 `i = blockIdx.x * blockDim.x + threadIdx.x`。回忆一下，我们的示例中 `blockDim` 为 256。块 0 中线程的 i 值范围从 0 到 255，块 1 中线程的 i 值范围从 256 到 511，块 2 中线程的 i 值范围从 512 到 767。也就是说，这三个块中的线程的 i 值形成了从 0 到 767 的连续覆盖。由于每个线程使用 i 来访问 A、B 和 C，这些线程覆盖了原始循环的前 768 次迭代。通过启动更多块的网格，可以处理更大的向量。通过启动一个具有 n 个或更多线程的网格，可以处理长度为 n 的向量。

```cuda
01 // 计算向量和 C = A + B
02 // 每个线程执行一对一的加法
03 __global__
04 void vecAddKernel(float* A, float* B, float* C, int n) {
05 int i = threadIdx.x + blockDim.x * blockIdx.x;
06 if (i < n) {
07 C[i] = A[i] + B[i];
08 }
09 }
```
> 图 2.10 `vecAddKernel`向量加法的内核函数

图 2.10 显示了一个向量加法的内核函数。注意，在内核中我们没有使用 “_h” 和 “_d” 的约定，因为没有潜在的混淆。我们在示例中不会访问主机内存。内核的语法是 ANSI C，带有一些显著的扩展。首先，在 `vecAddKernel` 函数声明前有一个 CUDA-C 特定的关键字 `__global__`。这个关键字表示该函数是一个内核，可以被调用以在设备上生成一个线程网格。

![image](https://github.com/user-attachments/assets/1edaec02-2a48-4f8b-b662-8b36dfc7adb5)
> 图 2.11 CUDA C 内核函数

通常，CUDA C 扩展了 C 语言，使用三个限定词关键字在函数声明中。这些关键字的含义总结在图 2.11 中。`__global__` 关键字表示被声明的函数是一个 CUDA C 内核函数。注意在单词 “global” 的两边各有两个下划线。这样的内核函数在设备上执行，并可以从主机调用。在支持动态并行性的 CUDA 系统中，它还可以从设备上调用，正如我们将在第 21 章 CUDA 动态并行性中看到的。重要的特性是调用这样的内核函数会在设备上启动一个新的线程网格。

`__device__` 关键字表示被声明的函数是一个 CUDA 设备函数。设备函数在 CUDA 设备上执行，只能从内核函数或其他设备函数调用。设备函数由调用它的设备线程执行，不会启动新的设备线程。

`__host__` 关键字表示被声明的函数是一个 CUDA 主机函数。主机函数仅仅是传统的 C 函数，在主机上执行，只能从另一个主机函数调用。默认情况下，如果函数声明中没有任何 CUDA 关键字，则 CUDA 程序中的所有函数都是主机函数。这是合理的，因为许多 CUDA 应用程序是从仅在 CPU 上执行的环境中移植过来的。程序员会在移植过程中添加内核函数和设备函数。原始函数仍然是主机函数。让所有函数默认成为主机函数可以节省程序员更改所有原始函数声明的繁琐工作。

注意，可以在函数声明中同时使用 `__host__` 和 `__device__`。这种组合告诉编译系统为相同的函数生成两个版本的目标代码。一个在主机上执行，只能从主机函数调用。另一个在设备上执行，只能从设备或内核函数调用。这支持一个常见的用例，即相同的函数源代码可以重新编译以生成设备版本。许多用户库函数可能会落入这种类别。

图 2.10 中的第二个显著扩展是内置变量 `threadIdx`、`blockIdx` 和 `blockDim`。回忆一下，所有线程执行相同的内核代码，需要一种方法使它们能够相互区分，并将每个线程指向数据的特定部分。这些内置变量是线程访问提供识别坐标的硬件寄存器的手段。不同线程将看到它们的 `threadIdx.x`、`blockIdx.x` 和 `blockDim.x` 变量中的不同值。为了可读性，我们在讨论中有时会将线程称为 `threadblockIdx.x` 和 `threadIdx.x`。

图 2.10 中的自动变量 `i` 是一个局部变量。在 CUDA 内核函数中，自动变量对每个线程都是私有的。也就是说，每个线程会生成一个 `i` 的版本。如果网格启动了 10,000 个线程，就会有 10,000 个 `i` 的版本，每个线程一个。线程为其 `i` 变量分配的值对其他线程不可见。我们将在第 5 章《内存架构和数据局部性》中详细讨论这些自动变量。

对比图 2.4 和图 2.10，可以揭示出 CUDA 内核的一个重要见解。图 2.10 中的内核函数没有循环对应于图 2.4 中的循环。读者应该问循环去了哪里。答案是循环现在被线程网格所取代。整个网格形成了相当于循环的内容。网格中的每个线程对应于原始循环的一次迭代。这有时被称为循环并行性，其中原始顺序代码的迭代由线程并行执行。

注意 `addVecKernel` 中的 `if (i < n)` 语句。这是因为并不是所有向量长度都可以表示为块大小的倍数。例如，假设向量长度为 100。最小的高效线程块维度是 32。假设我们选择了 32 作为块大小。为了处理 100 个向量元素，需要启动四个线程块。然而，这四个线程块将有 128 个线程。我们需要禁用线程块 3 中的最后 28 个线程，以避免执行原程序不期望的工作。由于所有线程都将执行相同的代码，所有线程将根据 `i` 值测试与 `n` 的比较。使用 `if (i < n)` 语句，前 100 个线程将执行加法，而最后 28 个线程则不会。这允许内核处理任意长度的向量。

|[top](https://github.com/tunglinwood/CUDA/blob/main/2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97.md#2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97)|
|-|

## 2.6 调用内核函数

在实现了内核函数之后，接下来的步骤是从主机代码调用该函数以启动线程网格。图 2.12 说明了这一点。当主机代码调用一个内核时，它通过执行配置参数设置网格和线程块的维度。配置参数位于传统 C 函数参数之前的“<<<” 和 “>>>” 之间。第一个配置参数给出网格中的块数。第二个指定每个块中的线程数。在这个示例中，每个块有 256 个线程。为了确保网格中有足够的线程来覆盖所有的向量元素，我们需要将网格中的块数设置为期望线程数（在这里是 n）与线程块大小（在这里是 256）的向上取整除法（即将商四舍五入到下一个更高的整数值）。进行向上取整的方法有很多，一种方法是对 n/256.0 应用 C 的 ceiling 函数。使用浮点值 256.0 确保我们生成一个浮点值进行除法，这样 ceiling 函数可以正确地向上取整。例如，如果我们需要 1000 个线程，我们将启动 ceil(1000/256.0)=4 个线程块。结果，这条语句将启动 4*256=1024 个线程。在图 2.10 中的内核中，`if (i < n)` 语句确保前 1000 个线程对 1000 个向量元素进行加法。剩下的 24 个线程不会执行。

```cuda
01 int vectAdd(float* A, float* B, float* C, int n) {
02 // A_d, B_d, C_d 的分配和复制省略
03 ...
04 // 启动 ceil(n/256) 个块，每个块有 256 个线程
05 vecAddKernel<<<ceil(n/256.0), 256>>>(A_d, B_d, C_d, n);
06 }
```
> 图 2.12 `vecAddKernel`向量加法内核调用语句。

图 2.13 显示了 `vecAdd` 函数中的最终主机代码。这段源代码完成了图 2.5 中的骨架。图 2.12 和图 2.13 共同展示了一个简单的 CUDA 程序，该程序包括主机代码和设备内核。代码被硬编码为使用每块 256 个线程的线程块。然而，使用的线程块数量取决于向量的长度（n）。如果 n 为 750，则使用三个线程块。如果 n 为 4000，则使用 16 个线程块。如果 n 为 2,000,000，则使用 7813 个线程块。请注意，所有线程块都操作向量的不同部分。它们可以以任何任意顺序执行。程序员不能对执行顺序做出任何假设。小型 GPU 可能只并行执行一个或两个线程块。较大的 GPU 可能并行执行 64 或 128 个线程块。这使得 CUDA 内核在执行速度上具有可扩展性。也就是说，相同的代码在小型 GPU 上运行速度较慢，在大型 GPU 上运行速度较快。我们将在第 4 章《计算架构与调度》中重新探讨这一点。

重要的是再次指出，向量加法示例由于其简洁性而被使用。在实践中，分配设备内存、将输入数据从主机传输到设备、将输出数据从设备传输到主机以及释放设备内存的开销可能会使最终代码比图 2.4 中的原始顺序代码更慢。这是因为内核执行的计算量相对于处理或传输的数据量较小。对于两个浮点输入操作数和一个浮点输出操作数，只执行了一次加法。实际应用通常具有内核，其中相对于处理的数据量需要更多的工作，这使得额外的开销是值得的。实际应用还倾向于在多次内核调用之间保持设备内存中的数据，以便可以摊销开销。我们将展示几个这样的应用示例。

```cuda
01 void vecAdd(float* A, float* B, float* C, int n) {
02 float *A_d, *B_d, *C_d;
03 int size = n * sizeof(float);
04
05 cudaMalloc((void **) &A_d, size);
06 cudaMalloc((void **) &B_d, size);
07 cudaMalloc((void **) &C_d, size);
08
09 cudaMemcpy(A_d, A, size, cudaMemcpyHostToDevice);
10 cudaMemcpy(B_d, B, size, cudaMemcpyHostToDevice);
11
12 vecAddKernel<<<ceil(n/256.0), 256>>>(A_d, B_d, C_d, n);
13
14 cudaMemcpy(C, C_d, size, cudaMemcpyDeviceToHost);
15
16 cudaFree(A_d);
17 cudaFree(B_d);
18 cudaFree(C_d);
19 }
```
> 图 2.13 `vecAdd` 函数中的完整主机代码版本。

|[top](https://github.com/tunglinwood/CUDA/blob/main/2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97.md#2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97)|
|-|

## 2.7 编译

我们已经看到，实现 CUDA C 内核需要使用 C 语言不包含的各种扩展。一旦这些扩展被使用在代码中，传统的 C 编译器就无法处理这些代码。这些代码需要通过识别和理解这些扩展的编译器进行编译，例如 NVCC（NVIDIA C 编译器）。正如图 2.14 顶部所示，NVCC 编译器处理 CUDA C 程序，利用 CUDA 关键字将主机代码和设备代码分开。主机代码是纯 ANSI C 代码，通过主机的标准 C/C++ 编译器进行编译，并作为传统的 CPU 进程运行。设备代码通过 CUDA 关键字标记，这些标记指定了 CUDA 内核及其相关的辅助函数和数据结构，NVCC 将其编译成称为 PTX 文件的虚拟二进制文件。这些 PTX 文件进一步由 NVCC 的运行时组件编译成真实的目标文件，并在支持 CUDA 的 GPU 设备上执行。

![image](https://github.com/user-attachments/assets/128519be-4d66-450c-941c-6fc2ef3e790e)

> 图 2.14 CUDA C 程序的编译过程概述。

|[top](https://github.com/tunglinwood/CUDA/blob/main/2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97.md#2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97)|
|-|

## 2.8 总结

本章提供了 CUDA C 编程模型的快速简化概述。CUDA C 扩展了 C 语言以支持并行计算。本章讨论了这些扩展的一个基本子集。为了方便起见，我们总结了本章讨论的扩展如下：

### 2.8.1 函数声明

CUDA C 扩展了 C 函数声明语法以支持异构并行计算。扩展总结如图 2.12 所示。使用 `__global__`、`__device__` 或 `__host__` 中的一个，CUDA C 程序员可以指示编译器生成内核函数、设备函数或主机函数。所有没有这些关键字的函数声明默认是主机函数。如果在函数声明中同时使用了 `__host__` 和 `__device__`，编译器将生成两个版本的函数，一个用于设备，一个用于主机。如果函数声明没有任何 CUDA C 扩展关键字，则函数默认为主机函数。

### 2.8.2 内核调用和网格启动

CUDA C 扩展了 C 函数调用语法，增加了被 `<<<` 和 `>>>` 包围的内核执行配置参数。这些执行配置参数仅在调用内核函数以启动网格时使用。我们讨论了定义网格维度和每个块维度的执行配置参数。读者应参考 CUDA 编程指南（NVIDIA, 2021）以获取有关内核启动扩展以及其他类型执行配置参数的更多细节。

### 2.8.3 内置（预定义）变量

CUDA 内核可以访问一组内置的、只读的预定义变量，使每个线程能够区分自己与其他线程，并确定要处理的数据区域。我们在本章讨论了 `threadIdx`、`blockDim` 和 `blockIdx` 变量。第 3 章《多维网格与数据》中将更详细地讨论如何使用这些变量。

### 2.8.4 运行时应用程序编程接口

CUDA 支持一组 API 函数，为 CUDA C 程序提供服务。本章讨论的服务包括 `cudaMalloc`、`cudaFree` 和 `cudaMemcpy` 函数。这些函数由主机代码调用，用于分配设备全局内存、释放设备全局内存，以及在主机和设备之间传输数据。读者可以参考 CUDA C 编程指南获取其他 CUDA API 函数的信息。

我们本章的目标是介绍 CUDA C 的核心概念以及编写简单 CUDA C 程序所需的 CUDA 扩展。本章并不是对所有 CUDA 特性的全面介绍。书的其余部分将覆盖其中的一些特性。然而，我们将重点介绍这些特性所支持的关键并行计算概念。我们将仅介绍在并行编程技术示例中所需的 CUDA C 特性。一般来说，我们鼓励读者始终查阅 CUDA C 编程指南，以获取有关 CUDA C 特性的更多详细信息。

## 练习

1. **如果我们想让每个线程在网格中计算向量加法的一个输出元素，如何将线程/块索引映射到数据索引 (i)？**

   - [ ] (A) `i=threadIdx.x + threadIdx.y;`
   - [ ] (B) `i=blockIdx.x + threadIdx.x;`
   - [ ] (C) `i=blockIdx.x*blockDim.x + threadIdx.x;`
   - [ ] (D) `i=blockIdx.x * threadIdx.x;`

   **答案**: (C) `i=blockIdx.x*blockDim.x + threadIdx.x;`

2. **假设我们想让每个线程计算两个相邻的向量加法元素。将线程/块索引映射到线程处理的第一个元素的数据索引 (i) 的表达式是什么？**

   - [ ] (A) `i=blockIdx.x*blockDim.x + threadIdx.x +2;`
   - [ ] (B) `i=blockIdx.*threadIdx.x*2;`
   - [ ] (C) `i=(blockIdx.x*blockDim.x + threadIdx.x)*2;`
   - [ ] (D) `i=blockIdx.x*blockDim.x*2 + threadIdx.x;`

   **答案**: (C) `i=(blockIdx.x*blockDim.x + threadIdx.x)*2;`

3. **我们想让每个线程计算两个向量加法元素。每个线程块处理 2*blockDim.x 个连续元素，这些元素组成两个部分。每个块中的所有线程将首先处理一个部分，每个线程处理一个元素。然后，他们将转到下一个部分，每个线程处理一个元素。假设变量 i 应该是线程处理的第一个元素的索引。将线程/块索引映射到数据索引的表达式是什么？**

   - [ ] (A) `i=blockIdx.x*blockDim.x + threadIdx.x +2;`
   - [ ] (B) `i=blockIdx.x*threadIdx.x*2;`
   - [ ] (C) `i=(blockIdx.x*blockDim.x + threadIdx.x)*2;`
   - [ ] (D) `i=blockIdx.x*blockDim.x*2 + threadIdx.x;`

   **答案**: (D) `i=blockIdx.x*blockDim.x*2 + threadIdx.x;`

4. **对于向量加法，假设向量长度为 8000，每个线程计算一个输出元素，并且线程块大小为 1024 线程。程序员配置内核调用以具有最少数量的线程块来覆盖所有输出元素。网格中的线程总数是多少？**

   - [ ] (A) 8000
   - [ ] (B) 8196
   - [ ] (C) 8192
   - [ ] (D) 8200

   **答案**: (C) 8192

5. **如果我们想在 CUDA 设备的 `global` 内存中分配一个包含 v 个整数元素的数组，`cudaMalloc` 调用的第二个参数的合适表达式是什么？**

   - [ ] (A) `n`
   - [ ] (B) `v`
   - [ ] (C) `n * sizeof(int)`
   - [ ] (D) `v * sizeof(int)`

   **答案**: (D) `v * sizeof(int)`

6. **如果我们想分配一个包含 n 个浮点元素的数组，并且有一个浮点指针变量 `A_d` 指向分配的内存，那么 `cudaMalloc()` 调用的第一个参数的合适表达式是什么？**

   - [ ] (A) `n`
   - [ ] (B) `(void *) A_d`
   - [ ] (C) `*A_d`
   - [ ] (D) `(void **) &A_d`

   **答案**: (D) `(void **) &A_d`

7. **如果我们想从主机数组 `A_h`（`A_h` 是指向源数组元素 0 的指针）复制 3000 字节的数据到设备数组 `A_d`（`A_d` 是指向目标数组元素 0 的指针），那么用于此数据复制的 CUDA API 调用的合适形式是什么？**

   - [ ] (A) `cudaMemcpy(3000, A_h, A_d, cudaMemcpyHostToDevice);`
   - [ ] (B) `cudaMemcpy(A_h, A_d, 3000, cudaMemcpyDeviceTHost);`
   - [ ] (C) `cudaMemcpy(A_d, A_h, 3000, cudaMemcpyHostToDevice);`
   - [ ] (D) `cudaMemcpy(3000, A_d, A_h, cudaMemcpyHostToDevice);`

   **答案**: (C) `cudaMemcpy(A_d, A_h, 3000, cudaMemcpyHostToDevice);`

8. **如何声明一个变量 err 以适当地接收

 CUDA API 调用的返回值？**

   - [ ] (A) `int err;`
   - [ ] (B) `cudaError err;`
   - [ ] (C) `cudaError_t err;`
   - [ ] (D) `cudaSuccess_t err;`

   **答案**: (C) `cudaError_t err;`

9. **考虑以下 CUDA 内核和相应的主机函数：**

   ```cuda
   01 __global__ void foo_kernel(float* a, float* b, unsigned int N){
   02 				unsigned int i=blockIdx.x*blockDim.x + threadIdx.x;
   03 				if(i < N) {
   04 					b[i]=2.7f*a[i] - 4.3f;
   05 				}
   06 }
   07 void foo(float* a_d, float* b_d) {
   08 		unsigned int N=200000;
   09 		foo_kernel<<<(N + 128-1)/128, 128>>>(a_d, b_d, N);
   10 }
   ```

   - a. 每个块中的线程数是多少？
   - b. 网格中的线程总数是多少？
   - c. 网格中的块数是多少？
   - d. 执行第 02 行代码的线程数量是多少？
   - e. 执行第 04 行代码的线程数量是多少？

   **解答**:
   - a. 128
   - b. 200000
   - c. `(N + 128-1)/128 = (200000 + 128 - 1)/128 = 1563`
   - d. `1563 * 128 = 200000`（假设 N 为 200000）
   - e. `N`（如果 i < N）

10. **一名新来的暑期实习生对 CUDA 感到沮丧。他抱怨说 CUDA 非常繁琐。他不得不将许多他计划在主机和设备上执行的函数声明两次，一次作为主机函数，一次作为设备函数。你会怎么回应？**

    **回应**: CUDA 设计之初确实需要对主机和设备进行分开处理，以适应它们的不同特性和需求。然而，CUDA 也在不断发展，许多库和工具已经简化了开发过程。比如，使用 CUDA 的 Thrust 库可以减少对低级别 CUDA 编程的需求。了解 CUDA 编程的基本原理后，可以利用更高级的工具和库来简化编程工作，减少繁琐的函数声明和重复代码。

|[top](https://github.com/tunglinwood/CUDA/blob/main/2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97.md#2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97)|
|-|


[上一章 第一章 介绍并行运算](https://github.com/tunglinwood/CUDA/blob/main/1_%E4%BB%8B%E7%BB%8D%E5%B9%B6%E8%A1%8C%E8%BF%90%E7%AE%97.md)

[下一章 第三章 多维网格与数据](https://github.com/tunglinwood/CUDA/blob/main/3_%E5%A4%9A%E7%BB%B4%E7%BD%91%E6%A0%BC%E4%B8%8E%E6%95%B0%E6%8D%AE.md)
