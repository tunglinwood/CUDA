# 2_异构数据并行计算

* [2.1 数据并行性](https://github.com/tunglinwood/CUDA/blob/main/2_%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97.md#21-%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E6%80%A7)
* [2.2 CUDA C程序结构]()
* [2.3 向量加法内核]()
* [2.4 设备全局内存和数据传输]()
* [2.5 内核函数和线程]()
* [2.6 调用内核函数]()
* [2.7 编译]()
* [2.8 总结]()
* [练习]()

数据并行性是指对数据集不同部分的计算工作可以相互独立地完成，因此可以并行进行。许多应用程序表现出大量的数据并行性，使它们适合于可扩展的并行执行。因此，并行程序员必须熟悉数据并行性的概念以及用于编写利用数据并行性的代码的并行编程语言构造。在本章中，我们将使用CUDA C语言构造开发一个简单的数据并行程序。

## 2.1 数据并行性
当现代软件应用程序运行缓慢时，问题通常出在数据上——需要处理的数据太多。图像处理应用程序处理具有数百万到万亿像素的图像或视频。科学应用程序使用数十亿网格点建模流体动力学。分子动力学应用程序必须模拟成千上亿原子之间的相互作用。航空公司调度处理数千次航班、机组人员和机场登机口。这些像素、粒子、网格点、相互作用、航班等大多数通常可以独立处理。例如，在图像处理中，将彩色像素转换为灰度仅需要该像素的数据。模糊图像时，将每个像素的颜色与附近像素的颜色平均，只需该小区域内的像素数据。即使是看似全局的操作，例如找到图像中所有像素的平均亮度，也可以分解为许多可以独立执行的小计算。这种对不同数据片段的独立评估是数据并行性的基础。编写数据并行代码意味着围绕数据（重新）组织计算，以便我们可以并行执行结果的独立计算，从而更快地完成整体任务——通常要快得多。

![image](https://github.com/user-attachments/assets/d49864d9-1a50-4d0f-9d8a-e44e33c07d55)
> 图2.1

让我们通过一个彩色转灰度转换的例子来说明数据并行性的概念。图2.1显示了一张由许多像素组成的彩色图像（左侧），每个像素包含一个从0（黑色）到1（全强度）变化的红色、绿色和蓝色分数值（r, g, b）。
要将彩色图像（图2.1的左侧）转换为灰度图像（右侧），我们需要通过应用以下加权和公式来计算每个像素的亮度值L：
$$ L = 0.21r + 0.72g + 0.07b $$

> RGB颜色图像表示
  在RGB表示中，图像中的每个像素被存储为一个(r, g, b)值的元组。图像行的格式为(r g b) (r g b) . . . (r g b)，如下面的概念图所示。每个元组指定了红色（R）、绿色（G）和蓝色（B）的混合比例。换句话说，对于每个像素，r、g和b值代表了在像素呈现时红色、绿色和蓝色光源的强度（0表示黑暗，1表示全强度）。

> ![image](https://github.com/user-attachments/assets/47c6ad49-4007-45a9-9083-1fe5a27f68e1)

> 这三种颜色的实际混合比例在行业指定的颜色空间中有所不同。在这里，AdobeRGB颜色空间中三种颜色的有效组合被显示为三角形的内部区域。
  每种混合的垂直坐标（y值）和水平坐标（x值）显示了像素强度的G和R分量的比例。剩余部分（1-y-x）的像素强度分配给B。为了呈现图像，每个像素的r、g、b值用于计算像素的总强度（亮度）以及混合系数（x、y、1-y-x）。

如果我们将输入视为一个由RGB值组成的数组I，而输出视为相应的亮度值数组O，我们将得到如图2.2所示的简单计算结构。例如，O[0]是通过根据上述公式计算I[0]中的RGB值的加权和生成的；O[1]是通过计算I[1]中的RGB值的加权和生成的；O[2]是通过计算I[2]中的RGB值的加权和生成的；依此类推。这些每像素的计算彼此之间没有依赖关系。所有这些计算都可以独立进行。显然，颜色到灰度的转换表现出丰富的数据并行性。当然，完整应用中的数据并行性可能更为复杂，本书的大部分内容都致力于教授找到和利用数据并行性所需的并行思维。

![image](https://github.com/user-attachments/assets/812f37f6-7c27-43fa-983b-91b0ea98e90b)
> 图2.2

> 任务并行与数据并行
> 数据并行不是并行编程中唯一使用的并行类型。任务并行在并行编程中也被广泛使用。任务并行通常通过应用程序的任务分解来实现。例如，一个简单的应用程序可能需要进行向量加法和矩阵向量乘法。每一个都是一个任务。如果这两个任务可以独立完成，就存在任务并行性。I/O和数据传输也是常见的任务来源。
> 在大型应用程序中，通常有大量的独立任务，因此有大量的任务并行性。例如，在一个分子动力学模拟器中，自然任务的列表包括振动力、旋转力、非键合力的邻居识别、非键合力、速度和位置、以及基于速度和位置的其他物理属性。
> 一般而言，数据并行是并行程序可扩展性的主要来源。对于大数据集，通常可以找到丰富的数据并行性，从而能够利用大规模并行处理器，并随着每一代拥有更多执行资源的硬件的发展提升应用性能。然而，任务并行在实现性能目标方面也可以发挥重要作用。当我们介绍流（streams）时，将会进一步讨论任务并行性。

## 2.2 CUDA C程序结构

现在我们准备学习如何编写CUDA C程序，以利用数据并行性实现更快的执行速度。CUDA C在流行的ANSI C编程语言基础上进行了最小化的新语法和库函数扩展，使程序员能够针对包含CPU核心和大规模并行GPU的异构计算系统进行编程。顾名思义，CUDA C是建立在NVIDIA的CUDA平台上的。

CUDA目前是大规模并行计算中最成熟的框架。它在高性能计算行业中得到了广泛应用，并且在最常见的操作系统上提供了编译器、调试器和性能分析工具等基本工具。

CUDA C程序的结构反映了计算机中主机（CPU）和一个或多个设备（GPU）的共存。每个CUDA C源文件可以包含主机代码和设备代码的混合。默认情况下，任何传统的C程序都是只包含主机代码的CUDA程序。可以在任何源文件中添加设备代码。设备代码通过特殊的CUDA C关键字清晰地标记出来。设备代码包括函数或内核，其代码以数据并行的方式执行。

![image](https://github.com/user-attachments/assets/74b9401b-0903-41b2-8b81-35c52b54d42b)
> CUDA程序执行

CUDA程序的执行如图2.3所示。执行从主机代码（CPU串行代码）开始。当调用内核函数时，会在设备上启动大量线程以执行内核。所有由内核调用启动的线程统称为一个网格。这些线程是CUDA平台上并行执行的主要载体。图2.3展示了两个线程网格的执行。当一个网格的所有线程完成执行时，该网格终止，执行继续在主机上进行，直到启动另一个网格。

请注意，图2.3显示了一个简化的模型，其中CPU执行和GPU执行不重叠。许多异构计算应用程序管理CPU和GPU执行的重叠，以充分利用CPU和GPU。

启动一个网格通常会生成许多线程以利用数据并行性。在颜色到灰度转换的示例中，每个线程可以用于计算输出数组O的一个像素。在这种情况下，网格启动生成的线程数等于图像中的像素数。对于大型图像，将生成大量线程。由于高效的硬件支持，CUDA程序员可以假设这些线程生成和调度所需的时钟周期非常少。这一假设与传统的CPU线程形成对比，后者通常需要数千个时钟周期来生成和调度。在下一章中，我们将展示如何实现颜色到灰度转换和图像模糊内核。在本章的其余部分，我们将以向量加法为示例进行讲解，以保持简单。

> ### 线程
> 线程是现代计算机中处理器执行顺序程序的一种简化视图。一个线程包括程序的代码、正在执行的代码位置以及其变量和数据结构的值。就用户而言，线程的执行是顺序的。用户可以使用源级调试器通过逐条执行语句、查看将要执行的下一条语句并检查变量和数据结构的值来监控线程的进展。
> 线程已经在编程中使用了很多年。如果程序员希望在应用程序中启动并行执行，他/她可以使用线程库或特殊语言创建和管理多个线程。在CUDA中，每个线程的执行同样是顺序的。CUDA程序通过调用内核函数来启动并行执行，这会导致底层运行机制启动一个线程网格，以并行处理数据的不同部分。

## 2.3 向量加法内核

我们使用向量加法来演示CUDA C程序的结构。向量加法可以说是最简单的数据并行计算，相当于顺序编程中的 "Hello World"。在展示向量加法的内核代码之前，先回顾一下传统向量加法（主机代码）函数的工作原理是有帮助的。图2.4显示了一个简单的传统C程序，它由一个主函数和一个向量加法函数组成。在我们的所有示例中，若需要区分主机和设备数据，我们将给主机使用的变量名后缀加上“_h”，给设备使用的变量名后缀加上“_d”，以提醒自己这些变量的预期用途。由于图2.4中只有主机代码，因此我们只看到以“_h”结尾的变量。

假设要相加的向量存储在主程序中分配和初始化的数组A和B中。输出向量存储在数组C中，该数组也在主程序中分配。为了简洁起见，我们不显示A、B和C在主函数中如何分配或初始化的细节。指向这些数组的指针与包含向量长度的变量N一起传递给`vecAdd`函数。请注意，`vecAdd`函数的参数加上了“_h”后缀，以强调它们由主机使用。当我们在接下来的步骤中引入设备代码时，这种命名约定将很有帮助。

图2.4中的`vecAdd`函数使用`for`循环迭代向量元素。在第i次迭代中，输出元素C_h[i]接收A_h[i]和B_h[i]的和。向量长度参数n用于控制循环，以使迭代次数与向量长度匹配。该函数通过指针A_h、B_h和C_h分别读取A和B的元素并写入C的元素。当`vecAdd`函数返回时，主函数中的后续语句可以访问C的新内容。

一种并行执行向量加法的直接方法是修改`vecAdd`函数并将其计算移至设备。图2.5显示了这种修改后的`vecAdd`函数的结构。函数的第1部分在设备（GPU）内存中分配空间以存储A、B和C向量的副本，并将A和B向量从主机内存复制到设备内存。第2部分调用实际的向量加法内核以在设备上启动一个线程网格。第3部分将和向量C从设备内存复制到主机内存，并释放设备内存中的三个数组。

请注意，修改后的`vecAdd`函数实际上是一个外包代理，它将输入数据传输到设备，激活设备上的计算，并从设备收集结果。代理以这样一种方式进行，使得主程序甚至不需要知道向量加法现在实际上是在设备上完成的。实际上，由于所有数据来回复制，这种“透明”的外包模型可能非常低效。人们通常会将大型和重要的数据结构保留在设备上，并简单地从主机代码调用设备函数。然而，现在我们将使用简化的透明模型来介绍基本的CUDA C程序结构。修订函数的详细信息以及内核函数的组成方式将是本章其余部分的主题。

```cuda
// Compute vector sum C_h = A_h + B_h
void vecAdd(float* A_h, float* B_h, float* C_h, int n) {
  for (int i = 0; i < n; ++i) {
    C_h[i] = A_h[i] + B_h[i];
  }
}
int main() {
    // Memory allocation for arrays A, B, and C
    // I/O to read A and B, N elements each
    ...
    vecAdd(A, B, C, N);
}

```
