### 章节概述

1.1 异构并行计算  
1.2 为什么需要更高速度或并行性？  
1.3 加速实际应用  
1.4 并行编程的挑战  
1.5 相关的并行编程接口  
1.6 总体目标  
1.7 本书的组织结构  

自计算机诞生以来，许多高价值的应用一直要求比计算设备所能提供的更高的执行速度和资源。早期的应用依赖于处理器速度、内存速度和内存容量的进步，以增强应用级能力，例如天气预报的及时性、工程结构分析的准确性、计算机生成图形的逼真度、每秒处理的航空预订数量和每秒处理的资金转移数量。近年来，像深度学习这样的新应用要求比最好的计算设备能提供的更高的执行速度和资源。这些应用需求在过去五十年里推动了计算设备能力的快速发展，并将在可预见的未来继续这样做。

基于单个中央处理单元（CPU）的微处理器，通过快速增加的时钟频率和硬件资源，驱动了计算应用在20世纪80年代和90年代的快速性能提高和成本降低。这些单CPU微处理器将GFLOPS（每秒数十亿次浮点运算）带到了桌面，将TFLOPS（每秒数万亿次浮点运算）带到了数据中心。自2003年以来，由于能耗和热量散发问题，这一驱动已经放缓。这些问题限制了时钟频率的增加和单个CPU在每个时钟周期内可执行的有效活动，同时保持顺序执行指令的外观。从那时起，几乎所有的微处理器供应商都转向了一种在每个芯片中使用多个物理CPU（称为处理器核心）来增加处理能力的模式。在这种模式下，传统的CPU可以被视为单核CPU。为了从多个处理器核心中受益，用户必须有多个指令序列，无论是来自同一应用程序还是不同应用程序，可以同时在这些处理器核心上执行。

传统上，大多数软件应用程序都是作为顺序程序编写的，这些程序由1945年冯·诺依曼在其开创性报告中设想的处理器执行。这些程序的执行可以通过基于程序计数器的概念（也称为指令指针）按顺序逐步执行来理解。程序计数器包含处理器将执行的下一条指令的内存地址。这种按顺序逐步执行的应用程序所产生的指令执行活动序列在文献中被称为执行线程，简称线程。线程的概念非常重要，它将在本书的其余部分中被正式定义和广泛使用。

传统上，大多数软件开发人员依赖硬件的进步，例如增加的时钟速度和执行多条指令的能力，以提高其顺序应用程序的速度；随着每一代新处理器的推出，同样的软件只需运行得更快。计算机用户也习惯于期望这些程序在每一代新微处理器上运行得更快。然而，这种期望在过去十多年里已经不再有效。顺序程序只能在一个处理器核心上运行，而这些处理器核心不会随着每一代的推出显著变快。没有性能的提高，应用开发人员将无法在新微处理器推出时在其软件中引入新功能和能力；这减少了整个计算机行业的增长机会。

相反，随着每一代新微处理器的推出，将继续享有显著性能提高的应用软件将是并行程序，其中多个执行线程合作以更快地完成工作。这种并行程序相对于顺序程序的巨大优势被称为并发革命（Sutter和Larus，2005）。并行编程的实践并不新鲜。高性能计算（HPC）社区已经开发并行程序几十年。这些并行程序通常在昂贵的大型计算机上运行。只有少数精英应用程序可以证明使用这些计算机的合理性，从而将并行编程的实践限制在少数应用开发人员中。现在，所有新微处理器都是并行计算机，需要作为并行程序开发的应用程序数量急剧增加。现在，软件开发人员迫切需要了解并行编程，这是本书的重点。

#### 1.1 异构并行计算

自2003年以来，半导体行业在设计微处理器方面已经确立了两条主要轨迹（Hwu等，2008）。多核轨迹旨在保持顺序程序的执行速度，同时转向多核。多核处理器始于双核处理器，核心数量随着每一代半导体工艺的进步而增加。最近的一个例子是英特尔的多核服务器微处理器，最多可达24个处理器核心，每个核心是一个乱序、多指令发射的处理器，支持超线程，设计旨在最大化顺序程序的执行速度。另一个例子是最近的ARM Ampere多核服务器处理器，有128个处理器核心。

相比之下，多线程轨迹更多地关注并行应用程序的执行吞吐量。多线程轨迹始于大量线程，并且每一代的线程数量再次增加。一个最近的例子是NVIDIA Tesla A100图形处理单元（GPU），具有数万个线程，在大量简单的、顺序的流水线中执行。自2003年以来，多线程处理器，尤其是GPU，一直在浮点性能竞赛中领先。到2021年，A100 GPU的峰值浮点吞吐量为64位双精度的9.7 TFLOPS，32位单精度的156 TFLOPS和16位半精度的312 TFLOPS。相比之下，最近的英特尔24核处理器的峰值浮点吞吐量为双精度的0.33 TFLOPS和单精度的0.66 TFLOPS。多线程GPU和多核CPU之间的峰值浮点计算吞吐量比率在过去几年里一直在增加。这些并不一定是应用程序的速度；它们仅仅是这些芯片中执行资源可能支持的原始速度。

这种多核和多线程之间的巨大峰值性能差距已经积累了显著的“电势”，并且在某个时候必须有所突破。我们已经达到了这一点。到目前为止，这种大的峰值性能差距已经激励许多应用开发人员将其软件中的计算密集部分转移到GPU上执行。也许更重要的是，并行执行的显著提高的性能已经使深度学习等革命性的新应用成为可能，这些应用本质上包含计算密集的部分。这些计算密集的部分也是并行编程的主要目标：当有更多工作要做时，就有更多机会在协作的并行工作者（即线程）之间分配工作。

我们可能会问，为什么多线程GPU和多核CPU之间会有如此大的峰值性能差距？答案在于这两种处理器的基本设计理念的差异，如图1.1所示。CPU的设计（如图1.1A所示）优化了顺序代码的性能。算术单元和操作数数据传送逻辑被设计为以增加每单位的芯片面积和功耗为代价来最小化算术运算的有效延迟。大型的最后一级片上缓存被设计为捕获频繁访问的数据，并将一些长延迟的内存访问转换为短延迟的缓存访问。复杂的分支预测逻辑和执行控制逻辑用于减轻条件分支指令的延迟。通过减少操作的延迟，CPU硬件减少了每个线程的执行延迟。然而，低延迟的算术单元、复杂的操作数传送逻辑、大容量缓存和控制逻辑消耗了芯片面积和功耗，这些面积和功耗本可以用于提供更多的算术执行单元和内存访问通道。这种设计方法通常被称为面向延迟的设计。

另一方面，GPU的设计理念受到快速增长的视频游戏行业的影响，该行业对在先进游戏中每帧视频执行大量浮点计算和内存访问的能力施加了巨大的经济压力。这种需求促使GPU供应商寻找方法，以最大化用于浮点计算和内存访问吞吐量的芯片面积和功率预算。
