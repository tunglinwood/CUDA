# 第一章 介绍并行运算

* [1.1 异构并行计算](https://github.com/tunglinwood/CUDA/blob/main/1_%E4%BB%8B%E7%BB%8D%E5%B9%B6%E8%A1%8C%E8%BF%90%E7%AE%97.md#11-%E5%BC%82%E6%9E%84%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97)
* [1.2 为什么需要更高速度或并行性？](https://github.com/tunglinwood/CUDA/blob/main/1_%E4%BB%8B%E7%BB%8D%E5%B9%B6%E8%A1%8C%E8%BF%90%E7%AE%97.md#12-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E6%9B%B4%E9%AB%98%E7%9A%84%E9%80%9F%E5%BA%A6%E6%88%96%E5%B9%B6%E8%A1%8C%E6%80%A7)
* [1.3 加速实际应用](https://github.com/tunglinwood/CUDA/blob/main/1_%E4%BB%8B%E7%BB%8D%E5%B9%B6%E8%A1%8C%E8%BF%90%E7%AE%97.md#13-%E5%8A%A0%E9%80%9F%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F)
* [1.4 并行编程的挑战](https://github.com/tunglinwood/CUDA/blob/main/1_%E4%BB%8B%E7%BB%8D%E5%B9%B6%E8%A1%8C%E8%BF%90%E7%AE%97.md#14-%E5%B9%B6%E8%A1%8C%E7%BC%96%E7%A8%8B%E7%9A%84%E6%8C%91%E6%88%98)
* [1.5 相关的并行编程接口](https://github.com/tunglinwood/CUDA/blob/main/1_%E4%BB%8B%E7%BB%8D%E5%B9%B6%E8%A1%8C%E8%BF%90%E7%AE%97.md#15-%E7%9B%B8%E5%85%B3%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%BC%96%E7%A8%8B%E6%8E%A5%E5%8F%A3)
* [1.6 总体目标](https://github.com/tunglinwood/CUDA/blob/main/1_%E4%BB%8B%E7%BB%8D%E5%B9%B6%E8%A1%8C%E8%BF%90%E7%AE%97.md#16-%E6%80%BB%E4%BD%93%E7%9B%AE%E6%A0%87)
* [1.7 本书的组织结构]()

自计算机诞生以来，许多高价值的应用一直要求比计算设备所能提供的更高的执行速度和资源。早期的应用依赖于处理器速度、内存速度和内存容量的进步，以增强应用级能力，例如天气预报的及时性、工程结构分析的准确性、计算机生成图形的逼真度、每秒处理的航空预订数量和每秒处理的资金转移数量。近年来，像深度学习这样的新应用要求比最好的计算设备能提供的更高的执行速度和资源。这些应用需求在过去五十年里推动了计算设备能力的快速发展，并将在可预见的未来继续这样做。

基于单个中央处理单元（CPU）的微处理器，通过快速增加的时钟频率和硬件资源，驱动了计算应用在20世纪80年代和90年代的快速性能提高和成本降低。这些单CPU微处理器将GFLOPS（每秒数十亿次浮点运算）带到了桌面，将TFLOPS（每秒数万亿次浮点运算）带到了数据中心。自2003年以来，由于能耗和热量散发问题，这一驱动已经放缓。这些问题限制了时钟频率的增加和单个CPU在每个时钟周期内可执行的有效活动，同时保持顺序执行指令的外观。从那时起，几乎所有的微处理器供应商都转向了一种在每个芯片中使用多个物理CPU（称为处理器核心）来增加处理能力的模式。在这种模式下，传统的CPU可以被视为单核CPU。为了从多个处理器核心中受益，用户必须有多个指令序列，无论是来自同一应用程序还是不同应用程序，可以同时在这些处理器核心上执行。

传统上，大多数软件应用程序都是作为顺序程序编写的，这些程序由1945年冯·诺依曼在其开创性报告中设想的处理器执行。这些程序的执行可以通过基于程序计数器的概念（也称为指令指针）按顺序逐步执行来理解。程序计数器包含处理器将执行的下一条指令的内存地址。这种按顺序逐步执行的应用程序所产生的指令执行活动序列在文献中被称为执行线程，简称线程。线程的概念非常重要，它将在本书的其余部分中被正式定义和广泛使用。

传统上，大多数软件开发人员依赖硬件的进步，例如增加的时钟速度和执行多条指令的能力，以提高其顺序应用程序的速度；随着每一代新处理器的推出，同样的软件只需运行得更快。计算机用户也习惯于期望这些程序在每一代新微处理器上运行得更快。然而，这种期望在过去十多年里已经不再有效。顺序程序只能在一个处理器核心上运行，而这些处理器核心不会随着每一代的推出显著变快。没有性能的提高，应用开发人员将无法在新微处理器推出时在其软件中引入新功能和能力；这减少了整个计算机行业的增长机会。

相反，随着每一代新微处理器的推出，将继续享有显著性能提高的应用软件将是并行程序，其中多个执行线程合作以更快地完成工作。这种并行程序相对于顺序程序的巨大优势被称为并发革命（Sutter和Larus，2005）。并行编程的实践并不新鲜。高性能计算（HPC）社区已经开发并行程序几十年。这些并行程序通常在昂贵的大型计算机上运行。只有少数精英应用程序可以证明使用这些计算机的合理性，从而将并行编程的实践限制在少数应用开发人员中。现在，所有新微处理器都是并行计算机，需要作为并行程序开发的应用程序数量急剧增加。现在，软件开发人员迫切需要了解并行编程，这是本书的重点。

## 1.1 异构并行计算

自2003年以来，半导体行业在设计微处理器方面已经确立了两条主要轨迹（Hwu等，2008）。多核轨迹旨在保持顺序程序的执行速度，同时转向多核。多核处理器始于双核处理器，核心数量随着每一代半导体工艺的进步而增加。最近的一个例子是英特尔的多核服务器微处理器，最多可达24个处理器核心，每个核心是一个乱序、多指令发射的处理器，支持超线程，设计旨在最大化顺序程序的执行速度。另一个例子是最近的ARM Ampere多核服务器处理器，有128个处理器核心。

相比之下，多线程轨迹更多地关注并行应用程序的执行吞吐量。多线程轨迹始于大量线程，并且每一代的线程数量再次增加。一个最近的例子是NVIDIA Tesla A100图形处理单元（GPU），具有数万个线程，在大量简单的、顺序的流水线中执行。自2003年以来，多线程处理器，尤其是GPU，一直在浮点性能竞赛中领先。到2021年，A100 GPU的峰值浮点吞吐量为64位双精度的9.7 TFLOPS，32位单精度的156 TFLOPS和16位半精度的312 TFLOPS。相比之下，最近的英特尔24核处理器的峰值浮点吞吐量为双精度的0.33 TFLOPS和单精度的0.66 TFLOPS。多线程GPU和多核CPU之间的峰值浮点计算吞吐量比率在过去几年里一直在增加。这些并不一定是应用程序的速度；它们仅仅是这些芯片中执行资源可能支持的原始速度。

这种多核和多线程之间的巨大峰值性能差距已经积累了显著的“电势”，并且在某个时候必须有所突破。我们已经达到了这一点。到目前为止，这种大的峰值性能差距已经激励许多应用开发人员将其软件中的计算密集部分转移到GPU上执行。也许更重要的是，并行执行的显著提高的性能已经使深度学习等革命性的新应用成为可能，这些应用本质上包含计算密集的部分。这些计算密集的部分也是并行编程的主要目标：当有更多工作要做时，就有更多机会在协作的并行工作者（即线程）之间分配工作。

![image](https://github.com/user-attachments/assets/862c3260-52db-4de4-8097-1297183cb867)
> CPU和GPU有着根本不同的设计理念：（A）CPU设计以延迟为导向；（B）GPU设计以吞吐量为导向。

我们可能会问，为什么多线程GPU和多核CPU之间会有如此大的峰值性能差距？答案在于这两种处理器的基本设计理念的差异，如图1.1所示。CPU的设计（如图1.1A所示）优化了顺序代码的性能。算术单元和操作数数据传送逻辑被设计为以增加每单位的芯片面积和功耗为代价来最小化算术运算的有效延迟。大型的最后一级片上缓存被设计为捕获频繁访问的数据，并将一些长延迟的内存访问转换为短延迟的缓存访问。复杂的分支预测逻辑和执行控制逻辑用于减轻条件分支指令的延迟。通过减少操作的延迟，CPU硬件减少了每个线程的执行延迟。然而，低延迟的算术单元、复杂的操作数传送逻辑、大容量缓存和控制逻辑消耗了芯片面积和功耗，这些面积和功耗本可以用于提供更多的算术执行单元和内存访问通道。这种设计方法通常被称为面向延迟的设计。

另一方面，GPU的设计理念受到快速增长的视频游戏行业的影响，该行业对在先进游戏中每帧视频执行大量浮点计算和内存访问的能力施加了巨大的经济压力。这种需求促使GPU供应商寻找方法，以最大化用于浮点计算和内存访问吞吐量的芯片面积和功率预算。

对图形应用中的任务（如视点变换和对象渲染）每秒执行大量浮点计算的需求是很直观的。此外，每秒进行大量内存访问的需求同样重要，甚至可能更重要。许多图形应用的速度受限于数据从内存系统传递到处理器以及从处理器传回内存系统的速率。GPU必须能够将大量数据快速地移入和移出其DRAM（动态随机存取存储器）的图形帧缓冲区，因为这种数据移动使得视频显示对游戏玩家来说更加丰富和令人满意。游戏应用普遍接受的松散内存模型（系统软件、应用程序和I/O设备期望其内存访问工作的方式）也使得GPU更容易支持大规模并行的内存访问。

相比之下，通用处理器必须满足遗留操作系统、应用程序和I/O设备的要求，这些要求对支持并行内存访问提出了更多挑战，从而更难提高内存访问的吞吐量，通常称为内存带宽。因此，图形芯片的内存带宽约为同时代CPU芯片的10倍，我们预计GPU在内存带宽方面将继续保持优势。

一个重要的观察是，降低延迟在功耗和芯片面积方面比增加吞吐量要昂贵得多。例如，通过增加算术单元的数量可以将算术吞吐量翻倍，代价是增加一倍的芯片面积和功耗。然而，将算术延迟减少一半可能需要将电流加倍，代价是芯片面积增加一倍以上并且功耗增加四倍。因此，GPU中的普遍解决方案是优化大量线程的执行吞吐量，而不是减少单个线程的延迟。这种设计方法通过允许流水线内存通道和算术操作具有长延迟来节省芯片面积和功耗。内存访问硬件和算术单元的面积和功耗的减少使得GPU设计师能够在一个芯片上增加更多这些单元，从而提高总体执行吞吐量。图1.1通过展示CPU设计中较少数量的较大算术单元和较少数量的内存通道（图1.1A）与较多数量的较小算术单元和较多数量的内存通道（图1.1B），直观地展示了设计方法的差异。

这些GPU的应用软件预计将编写大量并行线程。硬件利用大量线程在一些线程等待长延迟内存访问或算术操作时找到要做的工作。图1.1B中的小缓存内存旨在帮助控制这些应用的带宽需求，这样多个访问相同内存数据的线程不需要都访问DRAM。这种设计风格通常称为面向吞吐量的设计，因为它旨在最大化大量线程的总执行吞吐量，同时允许单个线程执行时间可能要长得多。

很明显，GPU被设计为并行、面向吞吐量的计算引擎，而它们在某些任务上表现不佳，而这些任务是CPU设计用于良好执行的。对于只有一个或很少线程的程序，具有较低操作延迟的CPU可以比GPU实现更高的性能。当一个程序有大量线程时，具有更高执行吞吐量的GPU可以比CPU实现更高的性能。因此，许多应用程序可能同时使用CPU和GPU，在CPU上执行顺序部分，在GPU上执行数值密集部分。这就是NVIDIA在2007年引入的统一计算设备架构（CUDA）编程模型旨在支持CPU和GPU联合执行应用程序的原因。

需要注意的是，速度并不是应用开发人员选择处理器运行其应用程序的唯一决定因素。其他几个因素可能更为重要。首先也是最重要的，选择的处理器必须在市场上有非常大的存在，即处理器的安装基数。原因很简单。软件开发的成本最好由一个非常大的客户群来证明是合理的。在市场占有率较小的处理器上运行的应用程序不会有大量的客户基础。这一直是传统并行计算系统的一个主要问题，与通用微处理器相比，传统并行计算系统的市场存在可以忽略不计。只有少数由政府和大型公司资助的精英应用程序在这些传统并行计算系统上成功开发。随着多线程GPU的普及，这种情况已经发生了变化。由于在PC市场的普及，GPU已经销售了数亿个。几乎所有的台式电脑和高端笔记本电脑中都有GPU。目前已有超过10亿个支持CUDA的GPU在使用中。如此大的市场存在使得这些GPU对应用开发者来说在经济上具有吸引力。

另一个重要的决定因素是实用的形式因素和易于访问性。直到2006年，并行软件应用程序都在数据中心服务器或部门集群上运行。但是这种执行环境往往限制了这些应用程序的使用。例如，在医学成像这样的应用中，基于64节点集群机器发表一篇论文是可以的。但实际临床应用中的磁共振成像（MRI）机器基于某种组合的PC和特殊硬件加速器。原因很简单，像GE和西门子这样的制造商不能在临床环境中销售需要机架式计算机服务器的MRI，而这种情况在学术部门环境中很常见。事实上，美国国立卫生研究院（NIH）曾一度拒绝资助并行编程项目；他们认为并行软件的影响将是有限的，因为巨大的集群机器在临床环境中无法工作。今天，许多公司已经开始销售带有GPU的MRI产品，NIH也资助使用GPU计算的研究。

直到2006年，图形芯片非常难以使用，因为程序员必须使用等同于图形API（应用程序编程接口）函数的方法来访问处理单元，这意味着需要使用OpenGL或Direct3D技术来编程这些芯片。简而言之，计算必须表示为以某种方式绘制像素的函数，以便在这些早期的GPU上执行。这种技术被称为GPGPU，即使用GPU进行通用编程。即使在更高层次的编程环境中，底层代码仍然需要适应设计用于绘制像素的API。这些API限制了可以为早期GPU编写的应用类型。因此，GPGPU没有成为一种广泛的编程现象。尽管如此，这项技术足够令人兴奋，激发了一些英勇的努力和出色的研究成果。

2007年CUDA的发布改变了一切（NVIDIA，2007）。CUDA不仅仅是软件上的变化；芯片上还增加了额外的硬件。NVIDIA实际上投入了硅片面积以促进并行编程的容易性。在G80及其后续的并行计算芯片中，GPGPU程序不再通过图形接口，而是通过芯片上的新的通用并行编程接口来服务CUDA程序的请求。通用编程接口大大扩展了可以轻松开发的GPU应用类型。所有其他软件层也进行了重新设计，使程序员可以使用熟悉的C/C++编程工具。

尽管GPU是异构并行计算中的一个重要计算设备类别，但还有其他重要类型的计算设备在异构计算系统中用作加速器。例如，现场可编程门阵列（FPGA）已被广泛用于加速网络应用。本书中使用GPU作为学习工具所介绍的技术同样适用于这些加速器的编程任务。

## 1.2 为什么需要更高的速度或并行性？

正如我们在第1.1节所述，大规模并行编程的主要动机是使应用程序在未来的硬件世代中享受持续的速度提升。正如我们将在并行模式、高级模式和应用程序（第II部分和第III部分，第7至第19章）中讨论的那样，当一个应用程序适合并行执行时，GPU上的良好实现可以比在单个CPU核心上顺序执行的速度提升超过100倍。如果应用程序包含我们所称的“数据并行性”，通常只需几个小时的工作即可实现103倍的速度提升。

有人可能会问，为什么应用程序会继续需要更高的速度。我们今天的许多应用程序似乎已经运行得足够快了。尽管当今世界有无数的计算应用程序，但许多未来令人兴奋的大众市场应用程序以前被认为是超级计算应用程序或超应用程序。例如，生物学研究社区正越来越多地转向分子水平。显微镜——无疑是分子生物学中最重要的仪器——过去依赖于光学或电子仪器。然而，这些仪器在分子级别的观察方面有其局限性。这些局限性可以通过结合计算模型来模拟传统仪器设定的边界条件下的分子活动来有效解决。通过模拟，我们可以测量更多细节并测试比传统仪器单独使用时更多的假设。在可预见的未来，这些模拟将继续从不断增加的计算速度中受益，这体现在可以建模的生物系统的规模和可以在可容忍的响应时间内模拟的反应时间长度上。这些改进将对科学和医学产生巨大影响。

对于诸如视频和音频编码与处理的应用程序，考虑一下我们对数字高清（HD）电视与旧的NTSC电视的满意度。一旦我们体验到HDTV画面中的细节水平，很难回到旧技术。但考虑一下HDTV所需的所有处理。这是一个高度并行的过程，就像三维（3D）成像和可视化一样。未来，诸如视图合成和低分辨率视频的高分辨率显示等新功能将需要电视中的更多计算能力。在消费层面，我们将看到越来越多的视频和图像处理应用程序，这些应用程序可以改善图片和视频的焦点、照明和其他关键方面。

更高的计算速度带来的好处之一是更好的用户界面。智能手机用户现在享受着具有高分辨率触摸屏的更加自然的界面，这些触摸屏的质量可与大屏幕电视相媲美。毫无疑问，未来版本的这些设备将结合具有3D视角的传感器和显示器、将虚拟和物理空间信息结合以增强可用性的应用程序，以及基于语音和计算机视觉的接口，这些都需要更高的计算速度。

类似的发展正在消费电子游戏中进行。过去，驾驶游戏中的汽车只是预先安排的一组场景。如果你的车碰到障碍物，你的车辆路线不会改变；只有游戏分数会改变。你的车轮不会弯曲或损坏，即使你丢了一个车轮，驾驶也不会更困难。随着计算速度的提高，游戏可以基于动态模拟而不是预先安排的场景。我们可以预期未来会体验到更多这些现实效果。事故将损坏你的车轮，你的在线驾驶体验将更加真实。准确建模物理现象的能力已经激发了数字孪生的概念，其中物理对象在模拟空间中具有准确的模型，以便可以以更低的成本彻底进行应力测试和劣化预测。现实的物理效果建模和模拟是众所周知的需要非常大的计算能力。

一个通过显著增加的计算吞吐量启用的新应用程序的重要例子是基于人工神经网络的深度学习。尽管自1970年代以来神经网络已经被积极研究，但由于训练这些网络需要太多的标记数据和太多的计算，它们在实际应用中一直无效。互联网的兴起提供了大量的标记图片，GPU的兴起提供了计算吞吐量的激增。因此，自2012年以来，基于神经网络的应用程序在计算机视觉和自然语言处理领域得到了快速采用。这种采用彻底改变了计算机视觉和自然语言处理应用，并触发了自动驾驶汽车和家用助手设备的快速发展。

我们提到的所有新应用程序都涉及以不同方式和在不同级别模拟和/或表示物理和并发世界，并处理大量数据。处理这种庞大的数据量时，许多计算可以并行进行，尽管在某些时候需要进行整合。在大多数情况下，有效管理数据传输对并行应用程序的可实现速度有重大影响。尽管每天处理这些应用的少数专家通常熟知这些技术，但大多数应用程序开发人员可以从对这些技术的更直观理解和实际操作知识中受益。

我们旨在以直观的方式向正式教育可能不是计算机科学或计算机工程的应用程序开发人员介绍数据管理技术。我们还旨在提供许多实用的代码示例和动手练习，以帮助读者获取工作知识，这需要一种实用的编程模型，促进并行实现并支持适当的数据传输管理。CUDA提供了这样一种编程模型，并已被庞大的开发者社区充分测试。

## 1.3 加速实际应用程序

我们能从并行化应用程序中期望获得多少加速？计算系统A对计算系统B的应用程序加速定义为在系统B中执行应用程序所需时间与在系统A中执行相同应用程序所需时间的比率。例如，如果一个应用程序在系统A中执行需要10秒，但在系统B中需要200秒，那么系统A相对于系统B的加速为200/10=20，这被称为20倍加速。

通过并行计算系统相对于串行计算系统可以实现的加速取决于应用程序中可并行化的部分比例。例如，如果在可并行化部分上花费的时间占总时间的30%，那么并行部分获得100倍加速将使得应用程序的总执行时间最多减少29.7%。也就是说，整个应用程序的加速仅约为1/(1-0.297)=1.42倍。实际上，即使并行部分获得无限加速，也只能减少30%的执行时间，最多实现1.43倍的加速。这种通过并行执行实现的加速程度会受到应用程序中可并行部分的限制，这一事实被称为Amdahl定律 (Amdahl, 2013)。另一方面，如果99%的执行时间在并行部分，那么并行部分获得100倍加速将把应用程序的执行时间减少到原来的1.99%。这使整个应用程序获得50倍加速。因此，应用程序的大部分执行时间必须在并行部分，以便大规模并行处理器能够有效加速其执行。

研究人员在某些应用程序上实现了超过100倍的加速。然而，这通常是在对算法进行优化和调整后，使得99.9%以上的应用工作在并行部分之后才实现的。

另一个影响应用程序可实现加速程度的重要因素是数据从内存中访问和写入的速度。在实际操作中，直接并行化应用程序往往会饱和内存（DRAM）的带宽，导致仅约10倍的加速。解决内存带宽限制的方法之一是利用专门的GPU片上内存来大幅减少对DRAM的访问次数。然而，还必须进一步优化代码，以克服片上内存容量有限等限制。本书的一个重要目标是帮助读者充分理解这些优化，并熟练使用它们。

请记住，相对于单核CPU执行所实现的加速程度也反映了CPU对应用程序的适用性。在某些应用程序中，CPU表现非常好，使得使用GPU加速性能变得更加困难。大多数应用程序都有一些部分可以通过CPU更好地执行。必须给CPU一个公平的表现机会，并确保代码编写使得GPU能够补充CPU的执行，从而正确利用CPU/GPU组合系统的异构并行计算能力。截至目前，结合多核CPU和多核GPU的大规模市场计算系统已经将T级计算带到了笔记本电脑，并将E级计算带到了集群中。

图1.2展示了典型应用程序的主要部分。大部分实际应用程序的代码往往是顺序的。这些顺序部分被描绘为桃子的“核”区域；试图对这些部分应用并行计算技术就像咬桃核一样——感觉不好！这些部分非常难以并行化。CPU在处理这些部分时往往表现得非常好。好消息是，尽管这些部分可能占据大量代码，但它们通常只占超级应用程序执行时间的一小部分。

![image](https://github.com/user-attachments/assets/cb1d10ac-483d-44b0-a2b0-d4a8e4bd974e)

图1.2显示了顺序和并行应用程序部分的覆盖范围。顺序部分和传统单核CPU覆盖部分相互重叠。传统的GPGPU技术对数据并行部分的覆盖非常有限，因为它仅限于可以转化为绘制像素的计算。功率限制是扩展单核CPU以覆盖更多数据并行部分的主要障碍。

接下来是我们称之为“桃肉”的部分。这些部分很容易并行化，早期的一些图形应用程序也是如此。在异构计算系统中进行并行编程可以显著提高这些应用程序的速度。如图1.2所示，早期的GPGPU编程接口仅覆盖桃肉部分的一小部分，这类似于一些最令人兴奋的应用程序的小部分。正如我们将看到的，CUDA编程接口旨在覆盖更大范围的令人兴奋的应用程序的桃肉部分。并行编程模型及其底层硬件仍在快速发展，以便能够高效并行化更大部分的应用程序。

## 1.4 并行编程的挑战

是什么让并行编程变得困难？曾有人说，如果你不关心性能，并行编程是非常容易的。你可以在一个小时内写出一个并行程序。但如果你不关心性能，为什么还要写一个并行程序呢？

本书将讨论在实现高性能并行编程时面临的几个挑战。首先，设计具有与顺序算法相同水平的算法复杂度的并行算法可能具有挑战性。许多并行算法执行的工作量与其顺序算法相同。然而，有些并行算法的工作量比其顺序算法更多。实际上，有时它们可能会执行更多的工作，以至于在处理大输入数据集时反而运行得更慢。这尤其是一个问题，因为快速处理大输入数据集是并行编程的重要动机。

例如，许多现实世界的问题最自然的描述是通过数学递归。并行化这些问题通常需要非直观的思考方式，并且在执行过程中可能需要冗余工作。有一些重要的算法原语，如前缀和，可以促进将问题的顺序、递归形式转化为更并行的形式。我们将在第11章“前缀和（扫描）”中更正式地介绍工作效率的概念，并使用重要的并行模式如前缀和来说明设计与其顺序算法具有相同计算复杂度的并行算法所涉及的方法和权衡。

其次，许多应用程序的执行速度受到内存访问延迟和/或吞吐量的限制。我们将这些应用程序称为内存绑定应用程序；相比之下，计算绑定应用程序受每字节数据执行的指令数限制。要在内存绑定应用程序中实现高性能并行执行，通常需要提高内存访问速度的方法。我们将在第5章“内存架构和数据局部性”和第6章“性能考虑”中介绍内存访问优化技术，并将在几个章节中应用这些技术来介绍并行模式和应用。

第三，并行程序的执行速度通常比顺序程序对输入数据特性的敏感度更高。许多现实世界的应用程序需要处理具有广泛变化特性的输入，如不规则或不可预测的数据大小和不均匀的数据分布。这些大小和分布的变化会导致并行线程分配不均的工作量，并显著降低并行执行的有效性。

并行程序的性能有时会因这些特性而显著变化。我们将在介绍并行模式和应用的章节中介绍正则化数据分布和/或动态细化线程数的方法，以应对这些挑战。

第四，有些应用程序可以并行化，同时需要线程之间几乎没有合作。这些应用程序通常被称为“令人尴尬的并行”。其他应用程序则需要线程相互协作，这需要使用同步操作如障碍或原子操作。这些同步操作会对应用程序造成开销，因为线程经常会发现自己在等待其他线程而不是执行有用的工作。我们将在本书中讨论减少这些同步开销的各种策略。

幸运的是，大多数这些挑战已经被研究人员解决。跨应用领域存在一些通用模式，使我们可以将一个领域中衍生的解决方案应用于其他领域的挑战。这是我们将在重要的并行计算模式和应用的背景下介绍应对这些挑战的关键技术的主要原因。

## 1.5 相关的并行编程接口

在过去的几十年中，已经提出了许多并行编程语言和模型（Mattson等，2004）。其中最广泛使用的是用于共享内存多处理器系统的OpenMP（Open，2005）和用于可扩展集群计算的消息传递接口（MPI）（MPI，2009）。这两者都已成为主要计算机厂商支持的标准化编程接口。

一个OpenMP实现包括编译器和运行时。程序员通过向OpenMP编译器指定关于循环的指令（命令）和编译指示（提示）。通过这些指令和编译指示，OpenMP编译器生成并行代码。运行时系统通过管理并行线程和资源来支持并行代码的执行。OpenMP最初是为CPU执行而设计的，后来扩展支持GPU执行。OpenMP的主要优点在于它提供了编译器自动化和运行时支持，抽象掉了许多并行编程细节，使得应用代码在不同厂商生产的系统以及同一厂商不同代的系统之间更具可移植性。我们将这种特性称为性能可移植性。然而，在OpenMP中进行有效编程仍然需要程序员理解所有涉及的并行编程概念。由于CUDA让程序员可以显式控制这些并行编程细节，因此即使对于想要以OpenMP作为主要编程接口的人来说，CUDA也是一个很好的学习工具。此外，根据我们的经验，OpenMP编译器仍在不断发展和改进。许多程序员可能需要使用CUDA风格的接口来处理OpenMP编译器不足的部分。

另一方面，MPI是一个在集群中计算节点不共享内存的编程接口（MPI，2009）。所有数据共享和交互都必须通过显式消息传递来完成。MPI已在高性能计算（HPC）中广泛使用。用MPI编写的应用程序已经成功地在拥有超过100,000个节点的集群计算系统上运行。如今，许多HPC集群使用异构的CPU/GPU节点。由于计算节点之间缺乏共享内存，将应用程序移植到MPI所需的努力可能相当大。程序员需要执行域分解以在各个节点之间分配输入和输出数据。在域分解的基础上，程序员还需要调用消息发送和接收函数来管理节点之间的数据交换。相比之下，CUDA提供了共享内存来在GPU中进行并行执行，以应对这一难题。虽然CUDA是每个节点的有效接口，但大多数应用程序开发人员需要使用MPI在集群级别进行编程。此外，通过NVIDIA集体通信库（NCCL）等API，对多GPU编程的支持也在不断增加。因此，在使用多GPU节点的现代计算集群中进行联合MPI/CUDA编程是HPC并行程序员需要理解的一个重要主题，我们将在第20章“编程异构计算集群”中介绍这个主题。

2009年，包括Apple、Intel、AMD/ATI和NVIDIA在内的几家主要行业公司共同开发了一种称为Open Compute Language（OpenCL）的标准化编程模型（The Khronos Group，2009）。与CUDA类似，OpenCL编程模型定义了语言扩展和运行时API，以允许程序员管理大规模并行处理器中的并行性和数据传递。与CUDA相比，OpenCL更多地依赖API，而较少依赖语言扩展。这使得厂商可以迅速调整其现有的编译器和工具来处理OpenCL程序。OpenCL是一个标准化的编程模型，即用OpenCL开发的应用程序可以在支持OpenCL语言扩展和API的所有处理器上正确运行而无需修改。然而，为了在新处理器上实现高性能，可能需要修改应用程序。

熟悉OpenCL和CUDA的开发者会发现，OpenCL和CUDA的关键概念和特性非常相似。也就是说，CUDA程序员可以以最小的努力学习OpenCL编程。更重要的是，几乎所有在使用CUDA时学到的技术都可以轻松应用于OpenCL编程。

## 1.6 总体目标

我们的主要目标是教会读者如何编程大规模并行处理器以实现高性能。因此，本书的大部分内容致力于开发高性能并行代码的技术。我们的方法不需要大量的硬件专业知识。尽管如此，你仍需要对并行硬件架构有一个良好的概念性理解，以便能够推理你的代码的性能行为。

因此，我们将用一些篇幅来直观理解基本的硬件架构特性，并用大量篇幅来讨论开发高性能并行程序的技术。特别是，我们将重点介绍计算思维（Wing，2006）技术，这些技术将使你能够以适合在大规模并行处理器上高效执行的方式思考问题。

在大多数处理器上进行高性能并行编程需要一些关于硬件工作原理的知识。可能需要多年时间来构建能够使程序员无需这些知识就能开发高性能代码的工具和机器。即使我们拥有这样的工具，我们也怀疑那些了解硬件知识的程序员将比那些不了解硬件知识的程序员更有效地使用这些工具。出于这个原因，我们在第4章“计算架构和调度”中介绍GPU架构的基本原理。我们还在讨论高性能并行编程技术的过程中介绍了更多专业化的架构概念。

我们的第二个目标是教会并行编程的正确功能性和可靠性，这是并行计算中的一个微妙问题。过去在并行系统上工作的程序员知道，取得初步性能还不够。挑战在于以一种你可以调试代码并支持用户的方式来实现它。CUDA编程模型鼓励使用简单的障碍同步、内存一致性和原子性形式来管理并行性。此外，它提供了一系列强大的工具，允许调试不仅是功能方面，还有性能瓶颈。我们将展示，通过专注于数据并行性，可以在一个应用程序中同时实现高性能和高可靠性。

我们的第三个目标是通过探索并行编程的方法来实现未来硬件代际的可扩展性，使得未来更为并行的机器能够比今天的机器更快地运行你的代码。我们希望帮助你掌握并行编程，以便你的程序能够扩展到新一代机器的性能水平。实现这种可扩展性的关键在于使内存数据访问正规化和局部化，以最小化关键资源的消耗和数据结构更新中的冲突。因此，开发高性能并行代码的技术对于确保应用程序未来的可扩展性也非常重要。

为了实现这些目标，需要大量的技术知识，因此我们将在本书中介绍许多并行编程的原理和模式（Mattson等，2004）。我们不会单独教授这些原理和模式。我们将在并行化有用应用程序的背景下教授它们。

尽管我们不能涵盖所有内容，但我们选择了最有用和经过验证的技术来详细介绍。事实上，本版书中关于并行模式的章节数量显著增加。我们现在准备给你一个快速的本书概览。
