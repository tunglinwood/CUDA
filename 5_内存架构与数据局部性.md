# 5_内存架构和数据局部性

* 5.1 [内存访问效率的重要性]
* 5.2 [CUDA 内存类型]
* 5.3 [通过分块减少内存流量]
* 5.4 [分块矩阵乘法核]
* 5.5 [边界检查]
* 5.6 [内存使用对占用率的影响]
* 5.7 [总结]
* [练习]

到目前为止，我们已经学习了如何编写 CUDA 内核函数以及如何配置和协调大量线程的执行。我们还研究了当前 GPU 硬件的计算架构以及线程如何在硬件上调度执行。在本章中，我们将重点关注 GPU 的片上内存架构，并开始研究如何组织和定位数据，以便大量线程高效访问。到目前为止，我们研究的 CUDA 内核可能只能达到底层硬件潜在速度的一小部分。这种性能较差的原因是全局内存（通常由片外 DRAM 实现）通常具有较长的访问延迟（数百个时钟周期）和有限的访问带宽。尽管可以通过大量可用线程来理论上容忍长时间的内存访问延迟，但很容易遇到全局内存访问路径上的流量拥堵情况，这会阻止大多数线程取得进展，从而使流式多处理器（SM）中的一些内核处于空闲状态。为了绕过这种拥堵，GPU 提供了许多额外的片上内存资源来访问数据，这些资源可以消除大部分进出全局内存的流量。在本章中，我们将研究如何使用不同类型的内存来提升 CUDA 内核的执行性能。

## 5.1 内存访问效率的重要性
我们可以通过计算图 3.11 中矩阵乘法内核代码执行最多部分的预期性能水平来说明内存访问效率的影响，如图 5.1 部分复制的代码所示。就执行时间而言，内核中最重要的部分是执行 M 的一行与 N 的一列点积的 for 循环。

在循环的每次迭代中，执行一次浮点乘法和一次浮点加法需要两次全局内存访问。全局内存访问从 M 和 N 数组中获取元素。浮点乘法操作将这两个元素相乘，浮点加法操作将积累结果累加到 `Pvalue` 中。因此，浮点运算（FLOP）与从全局内存访问的字节数（B）的比率为 2 FLOP 对 8 B，即 0.25 FLOP/B。我们将此比率称为计算与全局内存访问比率，定义为程序区域内从全局内存访问每个字节时执行的 FLOP 数量。此比率有时在文献中也称为算术强度或计算强度。

计算与全局内存访问比率对 CUDA 内核的性能有重大影响。例如，Ampere A100 GPU 的峰值全局内存带宽为 1555 GB/秒。由于矩阵乘法内核执行 0.25 OP/B，全局内存带宽限制了内核可以执行的单精度 FLOP 的吞吐量为 389 GFLOPS（每秒千兆浮点运算），即 1555 GB/秒乘以 0.25 FLOP/B 得出。然而，389 GFLOPS 仅是 A100 GPU 峰值单精度操作吞吐量的 2%，即 19,500 GFLOPS。A100 还配备了用于加速矩阵乘法操作的专用单元，称为张量核心。如果考虑到 A100 的张量核心峰值单精度浮点吞吐量为 156,000 GFLOPS，那么 389 GFLOPS 仅是峰值的 0.25%。因此，矩阵乘法内核的执行严重受到将数据从内存传输到 GPU 核心的速率的限制。我们称那些执行速度受内存带宽限制的程序为内存受限程序。

```cuda
for (int k = 0; k < Width; ++k){
	Pvalue += M[row*Width+k] * N[k*Width+col];
}
```
> 图 5.1 常见矩阵乘法操作内核

> ### 屋顶线模型
> 屋顶线模型是一种评估应用程序相对于其运行硬件的限制所实现的性能的视觉模型。下图显示了一个基本的屋顶线模型。在 x 轴上，我们有算术或计算强度，单位是 FLOP/B。它反映了应用程序在加载每字节数据时完成的工作量。在 y 轴上，我们有计算吞吐量，单位是 GFLOPS。图中的两条线反映了硬件的限制。水平线由硬件可以维持的峰值计算吞吐量（GFLOPS）决定。从原点开始的正斜线由硬件可以维持的峰值内存带宽决定。图中的点代表应用程序，其操作强度在 x 轴上，其实现的计算吞吐量在 y 轴上。当然，这些点将在两条线下方，因为它们不能超过硬件的峰值吞吐量。
> ![image](https://github.com/user-attachments/assets/da47c1a3-b7ab-48a0-8bc9-ab966da3a5fb)
> 点相对于这两条线的位置告诉我们应用程序的效率。接近两条线的点表明应用程序有效地使用了内存带宽或计算单元，而远低于两条线的应用程序表明资源使用效率低。两条线交点表示应用程序从内存受限转换为计算受限的计算强度值。计算强度较低的应用程序是内存受限的，无法达到峰值吞吐量，因为它们受内存带宽限制。计算强度较高的应用程序是计算受限的，不受内存带宽限制。
> 例如，点 A1 和 A2 都代表内存受限的应用程序，而 A3 代表计算受限的应用程序。A1 有效地使用了资源，接近峰值内存带宽，而 A2 则没有。对于 A2，可能还有进一步优化的空间，通过提高内存带宽利用率来提高吞吐量。然而，对于 A1，提高吞吐量的唯一方法是提高应用程序的计算强度。

要实现该内核的更高性能，我们需要通过减少执行的全局内存访问次数来增加内核的计算与全局内存访问比率。例如，要充分利用 A100 GPU 提供的 19,500 GFLOPS，至少需要 12.5 OP/B 的比率，即（19,500 GOP/秒）/（1555 GB/秒）。这意味着每访问 4 字节的浮点值，必须执行约 50 次浮点操作！实现这种比率的程度取决于当前计算中的固有数据重用程度。我们建议读者参考“屋顶线模型”边栏来分析程序的计算强度及其潜在性能。

正如我们将看到的，矩阵乘法提供了减少全局内存访问次数的机会，可以通过相对简单的技术来实现。矩阵乘法函数的执行速度可能因全局内存访问次数的减少而有数量级的差异。因此，矩阵乘法提供了一个很好的初始例子来展示这些技术。本章介绍了一种常用的减少全局内存访问次数的技术，并展示了该技术在矩阵乘法中的应用。

## 5.2 CUDA 内存类型

CUDA 设备包含几种类型的内存，帮助程序员改善计算与全局内存访问的比例。图 5.2 展示了这些 CUDA 设备内存。在图的底部，我们可以看到全局内存和常量内存。这两种内存类型都可以被主机写入（W）和读取（R）。全局内存还可以被设备写入和读取，而常量内存支持设备的短延迟、高带宽的只读访问。我们在第 2 章《异构数据并行计算》中介绍了全局内存，而常量内存在第 7 章《卷积》中将会详细讨论。

![image](https://github.com/user-attachments/assets/3ed9ce9a-9f9c-4e1d-8736-faf209d9c604)
> FIGURE 5.2 CUDA 设备内存模型的（不完整）概述。图中未显示的一个重要类型的 CUDA 内存是纹理内存，因为本书不涉及其使用。

另一种内存类型是局部内存，也可以读写。局部内存实际上位于全局内存中，具有类似的访问延迟，但在线程之间不共享。每个线程有自己的一部分全局内存，作为其私有局部内存，用于存放线程私有的数据，但这些数据不能分配到寄存器中。这些数据包括静态分配的数组、溢出的寄存器以及线程调用栈的其他元素。

图 5.2 中的寄存器和共享内存是片上内存。驻留在这些类型内存中的变量可以以非常高的速度以高度并行的方式访问。寄存器分配给单个线程；每个线程只能访问自己的寄存器（参见“CPU 与 GPU 寄存器架构”侧边栏）。一个内核函数通常使用寄存器来保存每个线程私有的、频繁访问的变量。共享内存分配给线程块；块中的所有线程可以访问为块声明的共享内存变量。共享内存是线程通过共享输入数据和中间结果进行合作的有效手段。通过在 CUDA 变量声明中使用其中一种 CUDA 内存类型，CUDA 程序员可以指定变量的可见性和访问速度。

> ### CPU 与 GPU 寄存器架构
> CPU 和 GPU 之间不同的设计目标导致了不同的寄存器架构。正如我们在第 4 章《计算架构与调度》中看到的，当 CPU 在不同线程之间进行上下文切换时，它们将外出线程的寄存器保存到内存中，并从内存中恢复入驻线程的寄存器。相比之下，GPU 通过在处理块的寄存器文件中保持所有调度线程的寄存器来实现零开销调度。这样，线程之间的切换是瞬时的，因为入驻线程的寄存器已经在寄存器文件中。因此，GPU 的寄存器文件需要比 CPU 的寄存器文件大得多。
>
> 我们在第 4 章《计算架构与调度》中还看到，GPU 支持动态资源分配，其中一个 SM 可以为每个线程分配少量寄存器并执行大量线程，或者为每个线程分配更多寄存器并执行较少线程。因此，GPU 寄存器文件需要设计以支持这种动态寄存器分配。相比之下，CPU 寄存器架构为每个线程分配固定数量的寄存器，而不考虑线程对寄存器的实际需求。

为了全面理解寄存器、共享内存和全局内存之间的差异，我们需要详细了解这些不同内存类型在现代处理器中的实现和使用。如第 4 章《计算架构与调度》的“线程束和 SIMD 硬件”侧边栏中所讨论的，几乎所有现代处理器都源于 1945 年约翰·冯·诺依曼提出的模型，如图 5.3 所示。CUDA 设备也不例外。

![image](https://github.com/user-attachments/assets/e0632545-f06c-4ba6-b94a-ccf91020aaba)
> FIGURE 5.3 基于冯·诺依曼模型的现代计算机中的内存与寄存器。

CUDA 设备中的全局内存对应于图 5.3 中的“内存”框。处理器框对应于我们今天常见的处理器芯片边界。全局内存位于处理器芯片之外，使用 DRAM 技术实现，这意味着访问延迟较长且带宽相对较低。寄存器对应于冯·诺依曼模型中的“寄存器文件”。寄存器文件位于处理器芯片上，这意味着与全局内存相比，访问延迟非常短且带宽极高。在典型设备中，所有 SM 的寄存器文件的总访问带宽至少比全局内存高两个数量级。此外，每当一个变量存储在寄存器中时，其访问不再消耗离芯片的全局内存带宽。这将反映为计算与全局内存访问比的提高。

更微妙的一点是，每次访问寄存器涉及的指令比访问全局内存的指令要少。大多数现代处理器中的算术指令具有“内置”的寄存器操作数。例如，一个浮点加法指令可能如下：

```cuda
fadd r1, r2, r3
```

其中 r2 和 r3 是指定寄存器文件中输入操作数值位置的寄存器号。存储浮点加法结果值的位置由 r1 指定。因此，当算术指令的操作数在寄存器中时，无需额外的指令将操作数值提供给算术和逻辑单元（ALU），在 ALU 中进行算术计算。

另一方面，如果操作数值在全局内存中，处理器需要执行内存加载操作以使操作数值可用。例如，如果浮点加法指令的第一个操作数在全局内存中，涉及的指令可能如下：

```cuda
load r2, r4, offset
fadd r1, r2, r3
```

其中 `load` 指令将偏移值加到 r4 的内容上，以形成操作数值的地址。然后，它访问全局内存并将值放入寄存器 r2 中。一旦操作数值在 r2 中，`fadd` 指令使用 r2 和 r3 中的值进行浮点加法，并将结果放入 r1 中。由于处理器每个时钟周期只能获取和执行有限数量的指令，因此带有额外加载的版本可能比没有额外加载的版本需要更多时间来处理。这是将操作数放入寄存器中可以提高执行速度的另一个原因。

最后，还有另一个微妙的理由说明为什么将操作数值放入寄存器中是更好的。在现代计算机中，从寄存器文件中访问值消耗的能量至少比从全局内存中访问值少一个数量级。从寄存器中访问值在能效方面比从全局内存中访问值具有巨大的优势。我们将很快详细讨论访问这两种硬件结构在现代计算机中的速度和能量差异。另一方面，正如我们很快将学到的，今天的 GPU 中每个线程可用的寄存器数量非常有限。正如我们在第 4 章《计算架构与调度》中看到的，如果应用程序的占用率在满占用场景中超出了限制，则会降低。因此，我们也需要尽可能避免过度使用这一有限资源。

图 5.4 展示了 CUDA 设备中的共享内存和寄存器。尽管它们都是片上内存，但在功能和访问成本上有很大差异。共享内存设计为处理器芯片上的内存空间的一部分。当处理器访问共享内存中的数据时，需要执行内存加载操作，就像访问全局内存中的数据一样。然而，由于共享内存在芯片上，因此它的访问延迟和吞吐量远远高于全局内存。由于需要执行加载操作，共享内存的延迟较长且带宽低于寄存器。在计算机架构术语中，共享内存是一种刮痕内存。

![image](https://github.com/user-attachments/assets/164dde7a-c4a4-4674-b6f4-dc74e02b5a6c)
> FIGURE 5.4 CUDA 设备 SM 中的共享内存与寄存器的比较。

CUDA 中共享内存和寄存器之间的一个重要区别是，驻留在共享内存中的变量可以被块中的所有线程访问。这与寄存器数据不同，寄存器数据是线程私有的。也就是说，共享内存设计用于支持线程块中线程之间的数据高效、高带宽共享。如图 5.4 所示，CUDA 设备 SM 通常使用多个处理单元，以允许多个线程在这些处理单元上同时进展（参见第 2 章《异构数据并行计算》中的“线程”侧边栏）。线程在块中可以分布在这些处理单元上。因此，这些 CUDA 设备中共享内存的硬件实现通常设计为允许多个处理单元同时访问其内容，以支持线程块中线程之间的高效数据共享。我们将学习几种重要的并行算法，这

些算法可以从线程之间的高效数据共享中获得巨大的好处。

到现在为止应该很清楚，寄存器、局部内存、共享内存和全局内存各有不同的功能、延迟和带宽。因此，了解如何声明一个变量以使其驻留在目标内存类型中是非常重要的。表 5.1 展示了将程序变量声明到各种内存类型中的 CUDA 语法。每种声明也为声明的 CUDA 变量赋予了作用域和生命周期。作用域确定可以访问变量的线程集：仅一个线程、块中的所有线程或所有网格中的所有线程。如果变量的作用域是单个线程，则为每个线程创建该变量的私有版本；每个线程只能访问其私有版本的变量。例如，如果一个内核声明了一个作用域为线程的变量，并且它以一百万个线程启动，则会创建一百万个该变量的版本，以便每个线程初始化并使用自己的变量版本。

|Variable declaration |Memory |Scope |Lifetime|
|:-|:-|:-|:-|
|Automatic variables other than arrays |Register |Thread |Grid|
|Automatic array variables |Local |Thread |Grid|
|__device__ __shared__ int SharedVar; |Shared |Block |Grid|
|__device__ int GlobalVar; |Global |Grid |Application|
|__device__ __constant__ int ConstVar; |Constant |Grid |Application|

| 变量声明 | 内存 | 作用域 | 生命周期 |
|:--|:--|:--|:--|
| 除数组外的自动变量 | 寄存器 | 线程 | 网格 |
| 自动数组变量 | 局部 | 线程 | 网格 |
| __device__ __shared__ int SharedVar; | 共享 | 块 | 网格 |
| __device__ int GlobalVar; | 全局 | 网格 | 应用 |
| __device__ __constant__ int ConstVar; | 常量 | 网格 | 应用 |

> 表 5.1 CUDA 变量声明类型限定符及其属性。

生命周期告诉我们变量在程序执行期间可用的时段：是在网格执行期间还是在整个应用程序期间。如果变量的生命周期在网格执行期间，则必须在内核函数体内声明，并且仅对内核的代码可用。如果内核被多次调用，则变量的值不会在这些调用之间保持。每次调用都必须初始化变量才能使用。另一方面，如果变量的生命周期是整个应用程序，则必须在任何函数体之外声明。这些变量的内容在应用程序的整个执行期间保持，并对所有内核可用。

我们称没有数组的变量为标量变量。如表 5.1 所示，所有在内核和设备函数中声明的自动标量变量都被存储在寄存器中。这些自动变量的作用域在单个线程内。当内核函数声明一个自动变量时，为每个执行该内核函数的线程生成该变量的私有副本。当线程终止时，所有其自动变量也会消失。

在图 5.1 中，变量 `blurRow`、`blurCol`、`curRow`、`curCol`、`pixels` 和 `pixVal` 都是自动变量，属于这一类别。注意，这些变量的访问速度极快且并行，但必须小心不要超出硬件实现中的寄存器存储限制。使用大量寄存器可能会对每个 SM 的占用率产生负面影响，正如我们在第 4 章《计算架构与调度》中看到的那样。

自动数组变量不存储在寄存器中。而是存储在线程的局部内存中，可能会导致较长的访问延迟和潜在的访问拥堵。这些数组的作用域与自动标量变量相同，限于单个线程。也就是说，为每个线程创建并使用一个自动数组的私有版本。一旦线程终止执行，其自动数组变量的内容也会消失。从我们的经验来看，在内核函数和设备函数中很少需要使用自动数组变量。

如果变量声明之前有 `__shared__` 关键字（每个 "__" 由两个 "_" 字符组成），则声明了一个 CUDA 共享变量。在声明中也可以在 `__shared__` 前添加一个可选的 `__device__` 以达到相同的效果。这种声明通常在内核函数或设备函数中进行。共享变量驻留在共享内存中。共享变量的作用域在线程块内；即块中的所有线程看到相同版本的共享变量。在内核执行期间，为每个块创建并使用共享变量的私有版本。共享变量的生命周期是内核执行的持续时间。当内核终止网格执行时，其共享变量的内容也会消失。如前所述，共享变量是线程在块内合作的有效手段。从共享内存中访问共享变量非常快且高度并行。CUDA 程序员经常使用共享变量来保存全局内存数据中在内核执行阶段频繁使用和重用的部分。可能需要调整用于创建执行阶段的算法，重点关注全局内存数据的较小部分，如第 5.4 节中的矩阵乘法所示。

如果变量声明之前有 `__constant__` 关键字（每个 "__" 由两个 "_" 字符组成），则声明了一个 CUDA 常量变量。在 `__constant__` 前添加一个可选的 `__device__` 也能达到相同的效果。常量变量的声明必须在任何函数体之外。常量变量的作用域是所有网格，即所有网格中的所有线程看到相同版本的常量变量。常量变量的生命周期是整个应用程序执行。常量变量通常用于提供输入值给内核函数。常量变量的值不能被内核函数代码更改。常量变量存储在全局内存中，但会被缓存以便高效访问。在适当的访问模式下，访问常量内存非常快且并行。目前，应用程序中常量变量的总大小限制为 65,536 字节。可能需要将输入数据量拆分以适应这一限制。我们将在第 7 章《卷积》中演示常量内存的使用。

一个变量声明之前仅有 `__device__` 关键字（每个 "__" 由两个 "_" 字符组成）的是一个全局变量，将被放置在全局内存中。访问全局变量的速度较慢。访问全局变量的延迟和吞吐量在较新的设备中得到了改进。全局变量的一个重要优点是它们对所有内核的所有线程可见。它们的内容在整个执行过程中也会保持。因此，全局变量可以作为线程跨块协作的手段。然而，需要注意的是，目前没有简单的方法来在不同线程块之间同步或确保全局内存访问中的数据一致性，除非使用原子操作或终止当前的内核执行。因此，全局变量通常用于将信息从一个内核调用传递到另一个内核调用。

在 CUDA 中，指针可以用来指向全局内存中的数据对象。指针在内核和设备函数中有两种典型的使用方式。首先，如果一个对象由主机函数分配，指向该对象的指针由内存分配 API 函数（如 `cudaMalloc`）初始化，并可以作为参数传递给内核函数，如第 2 章《异构数据并行计算》和第 3 章《多维网格与数据》中所示。第二种使用方式是将声明在全局内存中的变量的地址分配给指针变量。例如，内核函数中的语句 `{float* ptr=&GlobalVar;}` 将 `GlobalVar` 的地址分配给自动指针变量 `ptr`。有关如何在其他内存类型中使用指针，请参考 CUDA 编程指南。

## 5.3 瓦片技术以减少内存流量

在 CUDA 中使用设备内存时，我们面临一个内在的权衡：全局内存大但速度慢，而共享内存小但速度快。一个常见的策略是将数据划分为称为“瓦片”的子集，使每个瓦片可以适应共享内存。瓦片这个术语源于这样一个类比：一个大墙（即全局内存数据）可以被小瓦片（即每个可以适应共享内存的子集）覆盖。一个重要的标准是这些瓦片上的内核计算可以相互独立地进行。需要注意的是，并不是所有数据结构都可以根据任意内核函数被划分为瓦片。

通过第 3 章《多维网格与数据》中的矩阵乘法示例可以说明瓦片的概念。图 3.13 显示了一个小的矩阵乘法示例。它对应于图 3.11 中的内核函数。为了方便参考，我们在图 5.5 中复制了这个示例。为了简洁起见，我们将 `P[y*Width+x]`、`M[y*Width+x]` 和 `N[y*Width+x]` 简写为 `P<sub>y,x</sub>`、`M<sub>y,x</sub>` 和 `N<sub>y,x</sub>`。这个示例假设我们使用 4*2 的块来计算 P 矩阵。P 矩阵中的重框定义了每个块处理的 P 元素。图 5.5 突出了块 `<sub>0,0</sub>` 的四个线程所做的计算。这四个线程计算 `P<sub>0,0</sub>`、`P<sub>0,1</sub>`、`P<sub>1,0</sub>` 和 `P<sub>1,1</sub>`。线程 `<sub>0,0</sub>` 和线程 `<sub>0,1</sub>` 访问 M 和 N 元素的操作用黑色箭头突出显示。例如，线程 `<sub>0,0</sub>` 读取 `M<sub>0,0</sub>` 和 `N<sub>0,0</sub>`，接着是 `M<sub>0,1</sub>` 和 `N<sub>1,0</sub>`，然后是 `M<sub>0,2</sub>` 和 `N<sub>2,0</sub>`，最后是 `M<sub>0,3</sub>` 和 `N<sub>3,0</sub>`。图 5.6 显示了块 `<sub>0,0</sub>` 中所有线程执行的全局内存访问。

![image](https://github.com/user-attachments/assets/81c68995-49ef-42f1-b531-54bb305c674c)
> FIGURE 5.5 矩阵乘法的小示例。为了简洁，我们将 `M[y*Width+x]`、`N[y*Width+x]` 和 `P[y*Width+x]` 显示为 `M<sub>y,x</sub>`、`N<sub>y,x</sub>` 和 `P<sub>y,x</sub>`。

![image](https://github.com/user-attachments/assets/0ab45d48-7bd1-44b7-8964-78e96b25dfbb)
> FIGURE 5.6 块 `<sub>0,0</sub>` 中线程执行的全局内存访问。

线程按垂直方向列出，访问时间从左到右增加。请注意，每个线程在执行期间访问了四个 M 元素和四个 N 元素。在这四个线程中，它们访问的 M 和 N 元素之间存在显著的重叠。例如，线程 `<sub>0,0</sub>` 和线程 `<sub>0,1</sub>` 都访问了 `M<sub>0,0</sub>` 以及 M 的第 0 行的其余部分。同样，线程 `<sub>0,1</sub>` 和线程 `<sub>1,1</sub>` 都访问了 `N<sub>0,1</sub>` 以及 N 的第 1 列的其余部分。

图 3.11 中的内核被编写成使线程 `<sub>0,0</sub>` 和线程 `<sub>0,1</sub>` 从全局内存中访问 M 的第 0 行元素。如果我们可以设法让线程 `<sub>0,0</sub>` 和线程 `<sub>0,1</sub>` 协作，使这些 M 元素仅从全局内存中加载一次，我们可以将全局内存访问的总数减少一半。事实上，我们可以看到每个 M 和 N 元素在块 `<sub>0,0</sub>` 执行期间被访问了两次。因此，如果我们可以让所有四个线程在对全局内存的访问中进行协作，我们可以将对全局内存的流量减少一半。

读者应该验证，在矩阵乘法示例中，全局内存流量的潜在减少与使用的块的维度成正比。使用 `Width*Width` 块时，全局内存流量的潜在减少将是 `Width`。也就是说，如果我们使用 16*16 的块，我们可以通过线程之间的协作将全局内存流量减少到原始水平的 1/16。

我们现在介绍一个瓦片矩阵乘法算法。基本思想是让线程协作地将 M 和 N 元素的子集加载到共享内存中，然后每个线程在计算其点积时单独使用这些元素。请记住，共享内存的大小相当小，因此在将 M 和 N 元素加载到共享内存时，必须小心不要超过共享内存的容量。这可以通过将 M 和 N 矩阵划分为更小的瓦片来实现。这些瓦片的大小选择使它们可以适应共享内存。在最简单的形式中，瓦片的维度等于块的维度，如图 5.7 所示。

![image](https://github.com/user-attachments/assets/4e02e83a-9c6a-49ec-85d8-935b3442a7c2)
> FIGURE 5.7 划分 M 和 N 以利用共享内存。

![image](https://github.com/user-attachments/assets/4168c732-33e8-42c8-baf6-681314230c79)
> FIGURE 5.8 瓦片矩阵乘法的执行阶段。

在图 5.7 中，我们将 M 和 N 划分为 2*2 的瓦片，如粗线所示。每个线程执行的点积计算现在被分为几个阶段。在每个阶段，块中的所有线程协作将一个 M 瓦片和一个 N 瓦片加载到共享内存中。这可以通过让块中的每个线程将一个 M 元素和一个 N 元素加载到共享内存中来实现，如图 5.8 所示。图 5.8 的每一行显示了一个线程的执行活动。注意时间从左到右推进。我们只需显示块 `<sub>0,0</sub>` 中线程的活动；其他块具有相同的行为。M 元素的共享内存数组称为 Mds。N 元素的共享内存数组称为 Nds。在第 1 阶段开始时，块 `<sub>0,0</sub>` 的四个线程协作将 M 的一个瓦片加载到共享内存中：线程 `<sub>0,0</sub>` 将 `M<sub>0,0</sub>` 加载到 `Mds<sub>0,0</sub>` 中，线程 `<sub>0,1</sub>` 将 `M<sub>0,1</sub>` 加载到 `Mds<sub>0,1</sub>` 中，线程 `<sub>1,0</sub>` 将 `M<sub>1,0</sub>` 加载到 `Mds<sub>1,0</sub>` 中，线程 `<sub>1,1</sub>` 将 `M<sub>1,1</sub>` 加载到 `Mds<sub>1,1</sub>` 中。这些加载操作显示在图 5.8 的第二列。N 的一个瓦片也以类似的方式加载，如图 5.8 的第三列所示。

在两个 M 和 N 瓦片加载到共享内存后，这些元素将在点积计算中使用。请注意，共享内存中的每个值被使用了两次。例如，由线程 `<sub>1,1</sub>` 加载到 `Mds<sub>1,1</sub>` 的 `M<sub>1,1</sub>` 值被线程 `<sub>1,0</sub>` 和线程 `<sub>1,1</sub>` 各使用一次。通过将每个全局内存值加载到共享内存中，使其可以被多次使用，我们减少了对全局内存的访问次数。在这种情况下，我们将对全局内存的访问次数减少了 2 倍。读者应该验证，如果瓦片是 `N*N` 元素，则减少的倍数为 N。

请注意，每个点积的计算现在分为两个阶段，如图 5.8 中的阶段 1 和阶段 2 所示。在每个阶段，每个线程将输入矩阵元素的两个对的乘积累加到 `Pvalue` 变量中。请注意，`Pvalue` 是一个自动变量，因此为每个线程生成了一个私有版本。我们添加了下标

以澄清这些是为每个线程创建的不同 `Pvalue` 变量实例。第一个阶段的计算显示在图 5.8 的第四列，第二个阶段显示在第七列。

一般来说，如果输入矩阵的维度是 Width，且瓦片大小为 `TILE_WIDTH`，则点积计算将分为 `Width/TILE_WIDTH` 个阶段。

这些阶段的创建是减少对全局内存访问的关键。每个阶段关注输入矩阵值的小子集，线程可以协作地将子集加载到共享内存中，并使用共享内存中的值来满足它们在该阶段的重叠输入需求。

还要注意，Mds 和 Nds 在各阶段之间被重复使用。在每个阶段，相同的 Mds 和 Nds 被重复使用来存放该阶段使用的 M 和 N 元素的子集。这允许更小的共享内存服务于大多数全局内存访问。这是因为每个阶段关注输入矩阵元素的小子集。这种集中访问行为称为局部性。当算法表现出局部性时，有机会使用小的高速内存来服务大多数访问，并将这些访问从全局内存中移除。局部性对于实现多核 CPU 和许多线程 GPU 的高性能同样重要。我们将在第 6 章《性能考虑》中回到局部性概念。
