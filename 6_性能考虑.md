# 6_性能考虑

* [6.1 内存合并]
* [6.2 隐藏内存延迟]
* [6.3 线程粗化]
* [6.4 优化检查清单]
* [6.5 了解计算瓶颈]
* [6.6 总结]
* [练习]

并行程序的执行速度可能会因程序对资源的需求与硬件资源约束之间的相互作用而大相径庭。管理并行代码与硬件资源约束之间的相互作用对于在几乎所有并行编程模型中实现高性能至关重要。这是一项需要深入理解硬件架构的实践技能，最有效的学习方式是通过在为高性能设计的并行编程模型中进行实践练习。

到目前为止，我们已经了解了 GPU 架构的各个方面及其对性能的影响。在第 4 章《计算架构与调度》中，我们学习了 GPU 的计算架构及相关性能考虑因素，如控制分歧和占用率。在第 5 章《内存架构与数据局部性》中，我们了解了 GPU 的片上内存架构以及使用共享内存瓦片来实现更多数据重用的方法。在本章中，我们将简要介绍片外内存（DRAM）架构，并讨论相关的性能考虑因素，如内存合并和内存延迟隐藏。随后，我们将讨论一种重要的优化类型——线程粗化——这种优化可能针对架构的不同方面，具体取决于应用场景。最后，我们将用一份常见性能优化的检查清单来总结本部分内容，这将作为优化书中第二和第三部分讨论的并行模式性能的指南。

在不同的应用中，不同的架构约束可能主导性能，并成为性能的限制因素，通常被称为瓶颈。通过将一种资源的使用权衡为另一种资源的使用，通常可以显著改善在特定 CUDA 设备上的应用性能。如果缓解的资源约束在策略应用前是主要约束，而加重的资源约束对并行执行没有负面影响，那么这种策略效果很好。如果没有这样的理解，性能调优将变成猜测；可行的策略可能会导致性能提升，也可能不会。

## 6.1 内存合并

CUDA 内核性能的一个重要因素是对全局内存的访问，其带宽限制可能成为瓶颈。CUDA 应用程序广泛利用数据并行性。自然地，CUDA 应用程序倾向于在短时间内处理大量的全局内存数据。在第 5 章《内存架构与数据局部性》中，我们研究了利用共享内存的瓦片技术，以减少每个线程块中的线程必须从全局内存访问的总数据量。在本章中，我们将进一步讨论内存合并技术，以高效地在全局内存和共享内存或寄存器之间移动数据。内存合并技术通常与瓦片技术结合使用，使 CUDA 设备能够通过有效利用全局内存带宽来实现其性能潜力。

CUDA 设备的全局内存是使用 DRAM 实现的。数据位被存储在 DRAM 单元中，这些单元是小型电容器，通过电荷的存在或缺失来区分 1 和 0。读取 DRAM 单元中的数据需要小电容器利用其微小的电荷来驱动一个高电容的线路，经过传感器的检测机制来确定电容器中是否有足够的电荷来被识别为“1”。在现代 DRAM 芯片中，这个过程需要数十纳秒（参见“为什么 DRAM 这么慢？”侧栏）。这与现代计算设备的亚纳秒时钟周期时间形成鲜明对比。由于这个过程相对于所需的数据访问速度（亚纳秒每字节）非常慢，现代 DRAM 设计使用并行性来提高数据访问速率，通常称为内存访问吞吐量。

> ### 为什么 DRAM 这么慢？
> 下图展示了一个 DRAM 单元及其访问内容的路径。解码器是一个电子电路，它使用晶体管来驱动一个连接到成千上万单元的出口门的线路。这个线路的充电或放电到所需水平需要很长时间。一个更严峻的挑战是让单元驱动垂直线路到达传感器，并让传感器检测其内容。这基于电荷共享。门释放出单元中存储的微小电荷。如果单元内容为“1”，这微小的电荷必须将长位线的大电容的电势提高到足够高的水平，以触发传感器的检测机制。一个好的类比是，有人把一杯小咖啡放在长走廊的一端，而走廊另一端的人通过沿走廊传播的香气来判断咖啡的味道。
> ![image](https://github.com/user-attachments/assets/b2544f78-fcb8-4978-a9f1-3cc9e4753862)
> 通过使用更大、更强的电容器，可以加快这一过程。然而，DRAM 的发展方向却是相反的。每个单元的电容器的尺寸逐渐缩小，从而降低了其强度，以便在每个芯片上存储更多的位。这就是为什么 DRAM 的访问延迟随着时间的推移没有减少的原因。

每次访问 DRAM 位置时，都会访问包括请求位置在内的一系列连续位置。每个 DRAM 芯片中都提供了许多传感器，它们同时工作。每个传感器感知这些连续位置中的一个位的内容。一旦传感器检测到数据，来自所有这些连续位置的数据可以以高速传输到处理器。这些连续位置的访问和传输被称为 DRAM 突发。如果应用程序集中使用这些突发数据，DRAM 可以以远高于随机访问位置的速度提供数据。

认识到现代 DRAM 的突发组织方式，当前的 CUDA 设备采用了一种技术，使程序员能够通过将线程的内存访问组织成有利的模式来实现高效的全局内存访问。这项技术利用了 warp 中线程在任何给定时间执行相同指令的事实。当 warp 中的所有线程执行加载指令时，硬件检测它们是否访问连续的全局内存位置。换句话说，当 warp 中的所有线程访问连续的全局内存位置时，最有利的访问模式就会实现。在这种情况下，硬件将所有这些访问合并或合并成对连续 DRAM 位置的集中访问。例如，对于 warp 的给定加载指令，如果线程 0 访问全局内存位置 X，线程 1 访问位置 X + 1，线程 2 访问位置 X + 2，以此类推，所有这些访问将合并，或在访问 DRAM 时合并成对连续位置的单个请求。这种合并访问允许 DRAM 以突发方式传递数据。

为了有效地利用合并硬件，我们需要回顾在访问 C 多维数组元素时内存地址是如何形成的。回顾第 3 章《多维网格与数据》，（图 3.3 在这里作为图 6.1 方便起见复制），C 和 CUDA 中的多维数组元素是根据行主序约定放置到线性地址内存空间中的。回顾一下，行主序的意思是数据的放置保留了行的结构：一行中的所有相邻元素被放置在地址空间中的连续位置。在图 6.1 中，第 0 行的四个元素首先按其在行中的出现顺序放置。然后放置第 1 行的元素，接着是第 2 行的元素，然后是第 3 行的元素。应该清楚的是，虽然 M<sub>0,0</sub> 和 M<sub>1,0</sub> 在二维矩阵中看似是连续的，但在按行主序的线性地址内存中，它们之间相隔四个位置。

![image](https://github.com/user-attachments/assets/4667d540-1291-40c3-831b-33b96bc1c0e1)
> 图 6.1 基于行主序将矩阵元素放置到线性数组中。

假设图 6.1 中的多维数组是矩阵乘法中的第二个输入矩阵。在这种情况下，warp 中的连续线程将迭代输入矩阵的连续列。图 6.2 的左上部分显示了该计算的代码，右上部分显示了访问模式的逻辑视图：连续线程遍历连续列。可以通过检查代码来判断对 M 的访问是否可以合并。数组 `M` 的索引是 `k*Width+col`。变量 `k` 和 `Width` 在 warp 中所有线程中都具有相同的值。变量 `col` 被定义为 `blockIdx.x*blockDim.x+threadIdx.x`，这意味着连续线程（具有连续的 `threadIdx.x` 值）将具有连续的 `col` 值，因此将访问 M 的连续元素。

![image](https://github.com/user-attachments/assets/ba17c503-d062-4602-83fa-4851a1228d19)
> 图 6.2 合并访问模式。

图 6.2 的底部部分显示了访问模式的物理视图。在迭代 0 中，连续线程将访问内存中相邻的第 0 行的连续元素，如图 6.2 中的“迭代 0 的加载”所示。在迭代 1 中，连续线程将访问内存中相邻的第 1 行的连续元素，如图 6.2 中的“迭代 1 的加载”所示。这一过程对所有行都适用。如我们所见，线程在此过程中形成的内存访问模式是有利的，可以进行合并。实际上，在我们迄今实现的所有内核中，我们的内存访问自然地得到了合并。

现在假设矩阵以列主序存储，而不是行主序。可能有各种原因导致这种情况。例如，我们可能在乘以存储为行主序的矩阵的转置。在线性代数中，我们经常需要同时使用矩阵的原始形式和转置形式。最好避免创建和存储这两种形式。常见的做法是创建矩阵的一种形式，比如原始形式。当需要转置形式时，可以通过切换行和列索引的角色来访问原始形式中的元素。在 C 中，这等同于将转置矩阵视为原始矩阵的列主序布局。无论出于何种原因，让我们观察当矩阵乘法示例中的第二个输入矩阵以列主序存储时，所实现的内存访问模式。

图 6.3 说明了当矩阵以列主序存储时，连续线程如何遍历连续列。图 6.3 的左上部分显示了代码，右上部分显示了内存访问的逻辑视图。程序仍试图让每个线程访问矩阵 `M` 的一列。通过检查代码可以看出，对 `M` 的访问不利于合并。数组 `M` 的索引是 `col*Width+k`。如前所述，`col` 定义为 `blockIdx.x*blockDim.x+threadIdx.x`，这意味着连续线程（具有连续的 `threadIdx.x` 值）将具有连续的 `col` 值。然而，在 `M` 的索引中，`col` 乘以 `Width`，这意味着连续线程将访问间隔为 `Width` 的 `M` 元素。因此，这些访问不利于合并。

在图 6.3 的底部部分，我们可以看到内存访问的物理视图与图 6.2 中的情况大相径庭。在迭代 0 中，连续线程将逻辑上访问第 0 行的连续元素，但由于列主序布局，这些元素在内存中并不相邻。这些加载在图 6.3 中标记为“迭代 0 的加载”。类似地，在迭代 1 中，连续线程将访问第 1 行的连续元素，这些元素在内存中也不相邻。对于实际的矩阵，每个维度中的元素通常有数百或甚至数千个。在每次迭代中，邻近线程访问的 `M` 元素可能相隔数百或甚至数千个元素。硬件将确定这些元素的访问距离较远，无法进行合并。

![image](https://github.com/user-attachments/assets/e66e4ce8-a936-4364-a7c4-e7f5c4b0b722)
> 图 6.3 不合并的访问模式。

有多种策略可以优化代码以实现内存合并，当计算不自然适合合并时。一种策略是重新排列线程到数据的映射；另一种策略是重新排列数据本身的布局。我们将在第 6.4 节中讨论这些策略，并在本书中看到如何应用它们的示例。另一种策略是以合并的方式在全局内存和共享内存之间传输数据，并在共享内存中执行不利的访问模式，这提供了更快的访问延迟。我们将在本书中看到使用此策略的优化示例，包括我们现在将应用的一个优化，即在第二个输入矩阵以列主序布局时进行的矩阵乘法优化。这种优化称为“拐角转换”。

图 6.4 说明了如何应用拐角转换。在此示例中，矩阵 A 是以行主序布局存储在全局内存中的输入矩阵，矩阵 B 是以列主序布局存储在全局内存中的输入矩阵。它们相乘生成一个以行主序布局存储在全局内存中的输出矩阵 C。示例说明了负责输出块顶部四个连续元素的四个线程如何加载输入块元素。

![image](https://github.com/user-attachments/assets/dd220e6c-d5e0-4446-8953-7b45e8a565f3)
> 图 6.4 应用拐角转换以合并对存储在列主序布局中的矩阵 B 的访问。

对输入块 A 的访问类似于第 5 章《内存架构与数据局部性》中的情况。四个线程加载输入块顶部边缘的四个元素。每个线程加载一个输入元素，其局部行和列索引与其在输出块中的输出元素相同。这些访问是合并的，因为连续线程访问相邻的第 A 行的元素，这些元素在内存中是相邻的，按照行主序布局存储。

另一方面，对输入块 B 的访问需要不同于第 5 章《内存架构与数据局部性》的安排。图 6.4(A) 显示了如果我们使用与第 5 章相同的安排，访问模式将是什么样的。即使四个线程逻辑上加载输入块顶部边缘的四个连续元素，由于 B 元素的列主序布局，连续线程加载的元素在内存中并不相邻。换句话说，负责输出块中连续元素的连续线程加载内存中非连续位置，导致内存访问不合并。

这个问题可以通过将四个连续线程分配为加载输入块左边缘（相同列）中的四个连续元素来解决，如图 6.4(B) 所示。直观地，我们在计算线性化索引时交换了 `threadIdx.x` 和 `threadIdx.y` 的角色。由于 B 以列主序布局存储，因此同一列中的连续元素在内存中是相邻的。因此，连续线程加载的输入元素在内存中是相邻的，从而确保了内存访问的合并。代码可以编写成将 B 元素的块放置在共享内存中，采用列主序布局或行主序布局。无论哪种方式，在输入块加载后，每个线程可以以较小的性能损失访问其输入。这是因为共享内存采用 SRAM 技术实现，不需要合并。

![image](https://github.com/user-attachments/assets/0bedb718-08be-4908-8f5d-b69589fffafd)
> 图 6.5 减少高速公路系统的交通拥堵。

内存合并的主要优点是通过将多个内存访问合并为单个访问来减少全局内存流量。当多个访问同时发生且访问相邻的内存位置时，可以合并这些访问。交通拥堵不仅在计算中存在。我们大多数人都经历过高速公路系统的交通拥堵，如图 6.5 所示。高速公路交通拥堵的根本原因是道路上有太多的汽车，而道路设计只能容纳较少的车辆。当发生拥堵时，每辆车的旅行时间会大大增加。当交通拥堵时，上班通勤时间可以轻松加倍或三倍。

减少交通拥堵的大多数解决方案涉及减少道路上的汽车数量。假设通勤人数保持不变，人们需要拼车以减少道路上的汽车数量。一种常见的拼车方式是拼车，在这种方式中，一组通勤者轮流驾车到工作地点。政府通常需要制定政策以鼓励拼车。在一些国家，政府直接禁止某些类型的汽车在特定的日子上路。例如，拥有奇数车牌号的汽车可能不允许在周一、周三或周五上路。这鼓励那些在不同日子允许上路的汽车车主组建拼车小组。在一些国家，政府可能提供激励措施以鼓励减少道路上的汽车数量。例如，在某些国家，一些拥堵的高速公路车道被指定为拼车车道；只有乘客超过两人或三人的汽车才能使用这些车道。在一些国家，政府使汽油价格昂贵，从而促使人们组成拼车团体以节省开支。所有这些鼓励拼车的措施都旨在克服拼车需要额外努力的事实，如图 6.6 所示。

拼车需要希望拼车的工人妥协并达成一个共同的通勤时间安排。图 6.6 的上半部分展示了拼车的良好时间安排模式。时间从左到右。工人 A 和工人 B 的睡眠、工作和晚餐时间表相似。这使得这两位工人更容易在一辆车中一起去上班和回家。他们相似的时间表使他们更容易达成共同的出发时间和回家时间。在这种情况下，工人 A 和工人 B 的时间表非常不同。工人 A 彻夜狂欢，白天睡觉，晚上上班。工人 B 晚上睡觉，早上上班，6:00 下午回家吃晚餐。在这种情况下，时间表差异如此之大，以至于这两位工人根本无法协调共同的上班和回家的时间安排。

内存合并与拼车安排非常相似。我们可以将数据视为通勤者，将 DRAM 访问请求视为车辆。当 DRAM 请求的速率超过 DRAM 系统提供的访问带宽时，交通拥堵会增加，算术单元会变得空闲。如果多个线程访问相同的 DRAM 位置的数据，它们可能会形成一个“拼车”并将它们的访问合并为一个 DRAM 请求。然而，这需要线程具有类似的执行时间表，以便它们的数据访问可以合并成一个请求。在同一个 warp 中的线程是完美的候选者，因为它们都以 SIMD 执行方式同时执行加载指令。

## 6.2 隐藏内存延迟

正如我们在第 6.1 节中所解释的，DRAM 突发是一种并行组织形式：多个位置在 DRAM 核心阵列中并行访问。然而，仅仅依靠突发还不足以实现现代处理器所需的 DRAM 访问带宽。DRAM 系统通常还采用了另外两种并行组织形式：银行和通道。在最高层级，处理器包含一个或多个通道。每个通道是一个内存控制器，其总线连接一组 DRAM 银行到处理器。图 6.7 说明了一个包含四个通道的处理器，每个通道都有一条总线，将四个 DRAM 银行连接到处理器。在实际系统中，处理器通常有一个到八个通道，每个通道连接大量银行。

总线的数据传输带宽由其宽度和时钟频率定义。现代的双倍数据速率（DDR）总线每个时钟周期进行两次数据传输：一次在上升沿，一次在下降沿。例如，时钟频率为 1 GHz 的 64 位 DDR 总线具有 8B*2*1 GHz=16GB/s 的带宽。这看起来是一个很大的数字，但对于现代 CPU 和 GPU 通常还不够大。一个现代 CPU 可能需要至少 32 GB/s 的内存带宽，而现代 GPU 可能需要 256 GB/s。以这个例子来说，CPU 需要 2 个通道，而 GPU 需要 16 个通道。

对于每个通道，连接到其上的银行数量由完全利用总线的数据传输带宽所需的银行数量决定。这在图 6.8 中有所说明。每个银行包含一个 DRAM 单元阵列、访问这些单元的传感放大器，以及将数据突发传送到总线的接口（第 6.1 节）。

图 6.8(A) 说明了当单个银行连接到通道时的数据传输时序。它显示了对银行中 DRAM 单元的两个连续内存读取访问的时序。回想第 6.1 节，每次访问涉及解码器启用单元的长延迟，以及单元与传感放大器共享存储电荷的过程。这个延迟显示为时间框左端的灰色部分。一旦传感放大器完成工作，突发数据就会通过总线传送。通过总线传送突发数据的时间显示为图 6.8 中时间框的左侧黑色部分。第二次内存读取访问将经历类似的长访问延迟（时间框黑色部分之间的灰色部分），然后其突发数据才能被传输（右侧黑色部分）。

实际上，访问延迟（灰色部分）远长于数据传输时间（黑色部分）。显然，单银行组织的访问传输时序将大大低于通道总线的数据传输带宽的利用率。例如，如果 DRAM 单元阵列访问延迟与数据传输时间的比率为 20:1，那么通道总线的最大利用率将是 1/21=4.8%；即一个 16 GB/s 的通道将以不超过 0.76 GB/s 的速率向处理器传送数据。这显然不可接受。这个问题通过将多个银行连接到一个通道总线来解决。

当两个银行连接到一个通道总线时，可以在第一个银行服务另一个访问时启动对第二个银行的访问。因此，可以重叠访问 DRAM 单元阵列的延迟。图 6.8(B) 显示了两个银行组织的时序。我们假设银行 0 在图 6.8 所示的窗口之前开始访问。第一个银行开始访问其单元阵列后不久，第二个银行也开始访问其单元阵列。当银行 0 的访问完成时，它传输突发数据（时间框左侧最黑的部分）。一旦银行 0 完成数据传输，银行 1 可以传输其突发数据（第二个黑色部分）。这种模式会对后续访问重复。

从图 6.8(B) 可以看到，通过拥有两个银行，可以将通道总线的数据传输带宽利用率翻倍。通常，如果单元阵列访问延迟与数据传输时间的比率为 R，我们需要至少 R + 1 个银行，如果希望充分利用通道总线的数据传输带宽。例如，如果比率为 20，则每个通道总线需要至少 21 个银行。通常，连接到每个通道总线的银行数量需要大于 R，原因有两个。一是拥有更多的银行可以减少多个同时访问目标相同银行的概率，这种现象称为银行冲突。由于每个银行一次只能服务一个访问，因此无法重叠这些冲突访问的单元阵列访问延迟。拥有更多的银行增加了这些访问分散到多个银行的概率。第二个原因是每个单元阵列的大小设定以实现合理的延迟和制造能力。这限制了每个银行可以提供的单元数量。可能需要许多银行来支持所需的内存大小。

线程的并行执行与 DRAM 系统的并行组织之间存在重要联系。为了实现设备指定的内存访问带宽，必须有足够数量的线程同时进行内存访问。这一观察反映了最大化占用的另一个好处。回想第 4 章《计算架构与调度》，我们看到最大化占用确保流处理器（SMs）上有足够的线程驻留，以隐藏核心管线延迟，从而有效利用指令吞吐量。现在我们看到，最大化占用还有额外的好处，即确保进行足够的内存访问请求以隐藏 DRAM 访问延迟，从而有效利用内存带宽。当然，为了实现最佳带宽利用，这些内存访问必须均匀分布在各个通道和银行之间，并且每个对银行的访问也必须是合并的访问。

图 6.9 显示了将数组 M 的元素分配到通道和银行的一个简单示例。我们假设突发大小为两个元素（8 字节）。分配由硬件设计完成。通道和银行的地址安排如下：数组的前 8 字节（M[0] 和 M[1]）存储在通道 0 的银行 0 中，接下来的 8 字节（M[2] 和 M[3]）存储在通道 1 的银行 0 中，接下来的 8 字节（M[4] 和 M[5]）存储在通道 2 的银行 0 中，接下来的 8 字节（M[6] 和 M[7]）存储在通道 3 的银行 0 中。

此时，分配回绕到通道 0，但将使用银行 1 存储接下来的 8 字节（M[8] 和 M[9]）。因此，元素 M[10] 和 M[11] 将在通道 1 的银行 1 中，M[12] 和 M[13] 将在通道 2 的银行 1 中，M[14] 和 M[15] 将在通道 3 的银行 1 中。虽然图中没有显示，但任何额外的元素将回绕并从通道 0 的银行 0 开始。例如，如果有更多元素，M[16] 和 M[17] 将存储在通道 0 的银行 0 中，M[18] 和 M[19] 将存储在通道 1 的银行 0 中，依此类推。

图 6.9 中所示的分配方案，通常称为交错数据分配，将元素分布在系统中的银行和通道之间。此方案确保即使是相对较小的数组也能得到很好的分布。因此，我们仅分配足够的元素以充分利用通道 0 的银行 0 的 DRAM 突发，然后再移动到通道 1 的银行 0。在我们的简单示例中，只要我们至少有 16 个元素，分配就会涉及到所有的通道和银行来存储这些元素。

我们现在将说明并行线程执行与并行内存组织之间的互动。我们将使用图 5.5 的示例，复制为图 6.10。我们假设乘法将使用 2*2 的线程块和 2*2 的块。

在内核执行的阶段 0 中，所有四个线程块将加载它们的第一个块。图 6.11 显示了每个块中涉及的 M 元素。第 2 行显示了阶段 0 中访问的 M 元素及其 2D 索引。第 3 行显示了相同的 M 元素及其线性化索引。假设所有线程块并行执行。我们看到每个块将进行两个合并的访问。

根据图 6.9 中的分配，这些合并

的访问将发生在通道 0 的两个银行以及通道 2 的两个银行中。这四个访问将并行进行，以利用两个通道，同时提高每个通道的数据传输带宽利用率。

我们还看到 Block0,0 和 Block0,1 将加载相同的 M 元素。大多数现代设备都配备了缓存，这些缓存会将这些访问合并为一个，只要这些块的执行时间足够接近。实际上，GPU 设备中的缓存内存主要设计用于合并此类访问，减少对 DRAM 系统的访问次数。

第 4 行和第 5 行显示了内核执行阶段 1 中加载的 M 元素。我们看到这些访问现在发生在通道 1 和通道 3 的银行中。再次，这些访问将并行进行。读者应该清楚线程的并行执行与 DRAM 系统的并行结构之间存在共生关系。一方面，良好的 DRAM 系统访问带宽利用要求许多线程同时访问 DRAM 中的数据。另一方面，设备的执行吞吐量依赖于 DRAM 系统的并行结构，即银行和通道的良好利用。例如，如果同时执行的线程都访问相同通道中的数据，则内存访问吞吐量和设备的整体执行速度将大大降低。

读者可以验证，将两个更大的矩阵（如 8*8）相乘，使用相同的 2*2 线程块配置，将会利用图 6.9 中的所有四个通道。另一方面，增加 DRAM 突发大小将需要乘法计算更大的矩阵，以充分利用所有通道的数据传输带宽。
