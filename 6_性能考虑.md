# 6_性能考虑

* [6.1 内存合并](https://github.com/tunglinwood/CUDA/blob/main/6_%E6%80%A7%E8%83%BD%E8%80%83%E8%99%91.md#61-%E5%86%85%E5%AD%98%E5%90%88%E5%B9%B6)
* [6.2 隐藏内存延迟](https://github.com/tunglinwood/CUDA/blob/main/6_%E6%80%A7%E8%83%BD%E8%80%83%E8%99%91.md#62-%E9%9A%90%E8%97%8F%E5%86%85%E5%AD%98%E5%BB%B6%E8%BF%9F)
* [6.3 线程粗化](https://github.com/tunglinwood/CUDA/blob/main/6_%E6%80%A7%E8%83%BD%E8%80%83%E8%99%91.md#63-%E7%BA%BF%E7%A8%8B%E7%B2%97%E5%8C%96)
* [6.4 优化检查清单](https://github.com/tunglinwood/CUDA/blob/main/6_%E6%80%A7%E8%83%BD%E8%80%83%E8%99%91.md#64-%E4%BC%98%E5%8C%96%E6%B8%85%E5%8D%95)
* [6.5 了解计算瓶颈](https://github.com/tunglinwood/CUDA/blob/main/6_%E6%80%A7%E8%83%BD%E8%80%83%E8%99%91.md#65-%E4%BA%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E7%9A%84%E7%93%B6%E9%A2%88)
* [6.6 总结](https://github.com/tunglinwood/CUDA/blob/main/6_%E6%80%A7%E8%83%BD%E8%80%83%E8%99%91.md#66-%E6%80%BB%E7%BB%93)
* [练习](https://github.com/tunglinwood/CUDA/blob/main/6_%E6%80%A7%E8%83%BD%E8%80%83%E8%99%91.md#%E7%BB%83%E4%B9%A0)

并行程序的执行速度可能会因程序对资源的需求与硬件资源约束之间的相互作用而大相径庭。管理并行代码与硬件资源约束之间的相互作用对于在几乎所有并行编程模型中实现高性能至关重要。这是一项需要深入理解硬件架构的实践技能，最有效的学习方式是通过在为高性能设计的并行编程模型中进行实践练习。

到目前为止，我们已经了解了 GPU 架构的各个方面及其对性能的影响。在第 4 章《计算架构与调度》中，我们学习了 GPU 的计算架构及相关性能考虑因素，如控制分歧和占用率。在第 5 章《内存架构与数据局部性》中，我们了解了 GPU 的片上内存架构以及使用共享内存瓦片来实现更多数据重用的方法。在本章中，我们将简要介绍片外内存（DRAM）架构，并讨论相关的性能考虑因素，如内存合并和内存延迟隐藏。随后，我们将讨论一种重要的优化类型——线程粗化——这种优化可能针对架构的不同方面，具体取决于应用场景。最后，我们将用一份常见性能优化的检查清单来总结本部分内容，这将作为优化书中第二和第三部分讨论的并行模式性能的指南。

在不同的应用中，不同的架构约束可能主导性能，并成为性能的限制因素，通常被称为瓶颈。通过将一种资源的使用权衡为另一种资源的使用，通常可以显著改善在特定 CUDA 设备上的应用性能。如果缓解的资源约束在策略应用前是主要约束，而加重的资源约束对并行执行没有负面影响，那么这种策略效果很好。如果没有这样的理解，性能调优将变成猜测；可行的策略可能会导致性能提升，也可能不会。

|[top](https://github.com/tunglinwood/CUDA/blob/main/6_%E6%80%A7%E8%83%BD%E8%80%83%E8%99%91.md#6_%E6%80%A7%E8%83%BD%E8%80%83%E8%99%91)|
|-|

## 6.1 内存合并

CUDA 内核性能的一个重要因素是对全局内存的访问，其带宽限制可能成为瓶颈。CUDA 应用程序广泛利用数据并行性。自然地，CUDA 应用程序倾向于在短时间内处理大量的全局内存数据。在第 5 章《内存架构与数据局部性》中，我们研究了利用共享内存的瓦片技术，以减少每个线程块中的线程必须从全局内存访问的总数据量。在本章中，我们将进一步讨论内存合并技术，以高效地在全局内存和共享内存或寄存器之间移动数据。内存合并技术通常与瓦片技术结合使用，使 CUDA 设备能够通过有效利用全局内存带宽来实现其性能潜力。

CUDA 设备的全局内存是使用 DRAM 实现的。数据位被存储在 DRAM 单元中，这些单元是小型电容器，通过电荷的存在或缺失来区分 1 和 0。读取 DRAM 单元中的数据需要小电容器利用其微小的电荷来驱动一个高电容的线路，经过传感器的检测机制来确定电容器中是否有足够的电荷来被识别为“1”。在现代 DRAM 芯片中，这个过程需要数十纳秒（参见“为什么 DRAM 这么慢？”侧栏）。这与现代计算设备的亚纳秒时钟周期时间形成鲜明对比。由于这个过程相对于所需的数据访问速度（亚纳秒每字节）非常慢，现代 DRAM 设计使用并行性来提高数据访问速率，通常称为内存访问吞吐量。

> ### 为什么 DRAM 这么慢？
> 下图展示了一个 DRAM 单元及其访问内容的路径。解码器是一个电子电路，它使用晶体管来驱动一个连接到成千上万单元的出口门的线路。这个线路的充电或放电到所需水平需要很长时间。一个更严峻的挑战是让单元驱动垂直线路到达传感器，并让传感器检测其内容。这基于电荷共享。门释放出单元中存储的微小电荷。如果单元内容为“1”，这微小的电荷必须将长位线的大电容的电势提高到足够高的水平，以触发传感器的检测机制。一个好的类比是，有人把一杯小咖啡放在长走廊的一端，而走廊另一端的人通过沿走廊传播的香气来判断咖啡的味道。
> ![image](https://github.com/user-attachments/assets/b2544f78-fcb8-4978-a9f1-3cc9e4753862)
> 通过使用更大、更强的电容器，可以加快这一过程。然而，DRAM 的发展方向却是相反的。每个单元的电容器的尺寸逐渐缩小，从而降低了其强度，以便在每个芯片上存储更多的位。这就是为什么 DRAM 的访问延迟随着时间的推移没有减少的原因。

每次访问 DRAM 位置时，都会访问包括请求位置在内的一系列连续位置。每个 DRAM 芯片中都提供了许多传感器，它们同时工作。每个传感器感知这些连续位置中的一个位的内容。一旦传感器检测到数据，来自所有这些连续位置的数据可以以高速传输到处理器。这些连续位置的访问和传输被称为 DRAM 突发。如果应用程序集中使用这些突发数据，DRAM 可以以远高于随机访问位置的速度提供数据。

认识到现代 DRAM 的突发组织方式，当前的 CUDA 设备采用了一种技术，使程序员能够通过将线程的内存访问组织成有利的模式来实现高效的全局内存访问。这项技术利用了 warp 中线程在任何给定时间执行相同指令的事实。当 warp 中的所有线程执行加载指令时，硬件检测它们是否访问连续的全局内存位置。换句话说，当 warp 中的所有线程访问连续的全局内存位置时，最有利的访问模式就会实现。在这种情况下，硬件将所有这些访问合并或合并成对连续 DRAM 位置的集中访问。例如，对于 warp 的给定加载指令，如果线程 0 访问全局内存位置 X，线程 1 访问位置 X + 1，线程 2 访问位置 X + 2，以此类推，所有这些访问将合并，或在访问 DRAM 时合并成对连续位置的单个请求。这种合并访问允许 DRAM 以突发方式传递数据。

为了有效地利用合并硬件，我们需要回顾在访问 C 多维数组元素时内存地址是如何形成的。回顾第 3 章《多维网格与数据》，（图 3.3 在这里作为图 6.1 方便起见复制），C 和 CUDA 中的多维数组元素是根据行主序约定放置到线性地址内存空间中的。回顾一下，行主序的意思是数据的放置保留了行的结构：一行中的所有相邻元素被放置在地址空间中的连续位置。在图 6.1 中，第 0 行的四个元素首先按其在行中的出现顺序放置。然后放置第 1 行的元素，接着是第 2 行的元素，然后是第 3 行的元素。应该清楚的是，虽然 M<sub>0,0</sub> 和 M<sub>1,0</sub> 在二维矩阵中看似是连续的，但在按行主序的线性地址内存中，它们之间相隔四个位置。

![image](https://github.com/user-attachments/assets/4667d540-1291-40c3-831b-33b96bc1c0e1)
> 图 6.1 基于行主序将矩阵元素放置到线性数组中。

假设图 6.1 中的多维数组是矩阵乘法中的第二个输入矩阵。在这种情况下，warp 中的连续线程将迭代输入矩阵的连续列。图 6.2 的左上部分显示了该计算的代码，右上部分显示了访问模式的逻辑视图：连续线程遍历连续列。可以通过检查代码来判断对 M 的访问是否可以合并。数组 `M` 的索引是 `k*Width+col`。变量 `k` 和 `Width` 在 warp 中所有线程中都具有相同的值。变量 `col` 被定义为 `blockIdx.x*blockDim.x+threadIdx.x`，这意味着连续线程（具有连续的 `threadIdx.x` 值）将具有连续的 `col` 值，因此将访问 M 的连续元素。

![image](https://github.com/user-attachments/assets/ba17c503-d062-4602-83fa-4851a1228d19)
> 图 6.2 合并访问模式。

图 6.2 的底部部分显示了访问模式的物理视图。在迭代 0 中，连续线程将访问内存中相邻的第 0 行的连续元素，如图 6.2 中的“迭代 0 的加载”所示。在迭代 1 中，连续线程将访问内存中相邻的第 1 行的连续元素，如图 6.2 中的“迭代 1 的加载”所示。这一过程对所有行都适用。如我们所见，线程在此过程中形成的内存访问模式是有利的，可以进行合并。实际上，在我们迄今实现的所有内核中，我们的内存访问自然地得到了合并。

现在假设矩阵以列主序存储，而不是行主序。可能有各种原因导致这种情况。例如，我们可能在乘以存储为行主序的矩阵的转置。在线性代数中，我们经常需要同时使用矩阵的原始形式和转置形式。最好避免创建和存储这两种形式。常见的做法是创建矩阵的一种形式，比如原始形式。当需要转置形式时，可以通过切换行和列索引的角色来访问原始形式中的元素。在 C 中，这等同于将转置矩阵视为原始矩阵的列主序布局。无论出于何种原因，让我们观察当矩阵乘法示例中的第二个输入矩阵以列主序存储时，所实现的内存访问模式。

图 6.3 说明了当矩阵以列主序存储时，连续线程如何遍历连续列。图 6.3 的左上部分显示了代码，右上部分显示了内存访问的逻辑视图。程序仍试图让每个线程访问矩阵 `M` 的一列。通过检查代码可以看出，对 `M` 的访问不利于合并。数组 `M` 的索引是 `col*Width+k`。如前所述，`col` 定义为 `blockIdx.x*blockDim.x+threadIdx.x`，这意味着连续线程（具有连续的 `threadIdx.x` 值）将具有连续的 `col` 值。然而，在 `M` 的索引中，`col` 乘以 `Width`，这意味着连续线程将访问间隔为 `Width` 的 `M` 元素。因此，这些访问不利于合并。

在图 6.3 的底部部分，我们可以看到内存访问的物理视图与图 6.2 中的情况大相径庭。在迭代 0 中，连续线程将逻辑上访问第 0 行的连续元素，但由于列主序布局，这些元素在内存中并不相邻。这些加载在图 6.3 中标记为“迭代 0 的加载”。类似地，在迭代 1 中，连续线程将访问第 1 行的连续元素，这些元素在内存中也不相邻。对于实际的矩阵，每个维度中的元素通常有数百或甚至数千个。在每次迭代中，邻近线程访问的 `M` 元素可能相隔数百或甚至数千个元素。硬件将确定这些元素的访问距离较远，无法进行合并。

![image](https://github.com/user-attachments/assets/e66e4ce8-a936-4364-a7c4-e7f5c4b0b722)
> 图 6.3 不合并的访问模式。

有多种策略可以优化代码以实现内存合并，当计算不自然适合合并时。一种策略是重新排列线程到数据的映射；另一种策略是重新排列数据本身的布局。我们将在第 6.4 节中讨论这些策略，并在本书中看到如何应用它们的示例。另一种策略是以合并的方式在全局内存和共享内存之间传输数据，并在共享内存中执行不利的访问模式，这提供了更快的访问延迟。我们将在本书中看到使用此策略的优化示例，包括我们现在将应用的一个优化，即在第二个输入矩阵以列主序布局时进行的矩阵乘法优化。这种优化称为“拐角转换”。

图 6.4 说明了如何应用拐角转换。在此示例中，矩阵 A 是以行主序布局存储在全局内存中的输入矩阵，矩阵 B 是以列主序布局存储在全局内存中的输入矩阵。它们相乘生成一个以行主序布局存储在全局内存中的输出矩阵 C。示例说明了负责输出块顶部四个连续元素的四个线程如何加载输入块元素。

![image](https://github.com/user-attachments/assets/dd220e6c-d5e0-4446-8953-7b45e8a565f3)
> 图 6.4 应用拐角转换以合并对存储在列主序布局中的矩阵 B 的访问。

对输入块 A 的访问类似于第 5 章《内存架构与数据局部性》中的情况。四个线程加载输入块顶部边缘的四个元素。每个线程加载一个输入元素，其局部行和列索引与其在输出块中的输出元素相同。这些访问是合并的，因为连续线程访问相邻的第 A 行的元素，这些元素在内存中是相邻的，按照行主序布局存储。

另一方面，对输入块 B 的访问需要不同于第 5 章《内存架构与数据局部性》的安排。图 6.4(A) 显示了如果我们使用与第 5 章相同的安排，访问模式将是什么样的。即使四个线程逻辑上加载输入块顶部边缘的四个连续元素，由于 B 元素的列主序布局，连续线程加载的元素在内存中并不相邻。换句话说，负责输出块中连续元素的连续线程加载内存中非连续位置，导致内存访问不合并。

这个问题可以通过将四个连续线程分配为加载输入块左边缘（相同列）中的四个连续元素来解决，如图 6.4(B) 所示。直观地，我们在计算线性化索引时交换了 `threadIdx.x` 和 `threadIdx.y` 的角色。由于 B 以列主序布局存储，因此同一列中的连续元素在内存中是相邻的。因此，连续线程加载的输入元素在内存中是相邻的，从而确保了内存访问的合并。代码可以编写成将 B 元素的块放置在共享内存中，采用列主序布局或行主序布局。无论哪种方式，在输入块加载后，每个线程可以以较小的性能损失访问其输入。这是因为共享内存采用 SRAM 技术实现，不需要合并。

![image](https://github.com/user-attachments/assets/0bedb718-08be-4908-8f5d-b69589fffafd)
> 图 6.5 减少高速公路系统的交通拥堵。

内存合并的主要优点是通过将多个内存访问合并为单个访问来减少全局内存流量。当多个访问同时发生且访问相邻的内存位置时，可以合并这些访问。交通拥堵不仅在计算中存在。我们大多数人都经历过高速公路系统的交通拥堵，如图 6.5 所示。高速公路交通拥堵的根本原因是道路上有太多的汽车，而道路设计只能容纳较少的车辆。当发生拥堵时，每辆车的旅行时间会大大增加。当交通拥堵时，上班通勤时间可以轻松加倍或三倍。

减少交通拥堵的大多数解决方案涉及减少道路上的汽车数量。假设通勤人数保持不变，人们需要拼车以减少道路上的汽车数量。一种常见的拼车方式是拼车，在这种方式中，一组通勤者轮流驾车到工作地点。政府通常需要制定政策以鼓励拼车。在一些国家，政府直接禁止某些类型的汽车在特定的日子上路。例如，拥有奇数车牌号的汽车可能不允许在周一、周三或周五上路。这鼓励那些在不同日子允许上路的汽车车主组建拼车小组。在一些国家，政府可能提供激励措施以鼓励减少道路上的汽车数量。例如，在某些国家，一些拥堵的高速公路车道被指定为拼车车道；只有乘客超过两人或三人的汽车才能使用这些车道。在一些国家，政府使汽油价格昂贵，从而促使人们组成拼车团体以节省开支。所有这些鼓励拼车的措施都旨在克服拼车需要额外努力的事实，如图 6.6 所示。

拼车需要希望拼车的工人妥协并达成一个共同的通勤时间安排。图 6.6 的上半部分展示了拼车的良好时间安排模式。时间从左到右。工人 A 和工人 B 的睡眠、工作和晚餐时间表相似。这使得这两位工人更容易在一辆车中一起去上班和回家。他们相似的时间表使他们更容易达成共同的出发时间和回家时间。在这种情况下，工人 A 和工人 B 的时间表非常不同。工人 A 彻夜狂欢，白天睡觉，晚上上班。工人 B 晚上睡觉，早上上班，6:00 下午回家吃晚餐。在这种情况下，时间表差异如此之大，以至于这两位工人根本无法协调共同的上班和回家的时间安排。

![image](https://github.com/user-attachments/assets/c3ceace2-022d-4e5a-ab0f-5d84dc5b7edf)  
> 图 6.6 拼车的良好时间安排模式

内存合并与拼车安排非常相似。我们可以将数据视为通勤者，将 DRAM 访问请求视为车辆。当 DRAM 请求的速率超过 DRAM 系统提供的访问带宽时，交通拥堵会增加，算术单元会变得空闲。如果多个线程访问相同的 DRAM 位置的数据，它们可能会形成一个“拼车”并将它们的访问合并为一个 DRAM 请求。然而，这需要线程具有类似的执行时间表，以便它们的数据访问可以合并成一个请求。在同一个 warp 中的线程是完美的候选者，因为它们都以 SIMD 执行方式同时执行加载指令。

|[top](https://github.com/tunglinwood/CUDA/blob/main/6_%E6%80%A7%E8%83%BD%E8%80%83%E8%99%91.md#6_%E6%80%A7%E8%83%BD%E8%80%83%E8%99%91)|
|-|

## 6.2 隐藏内存延迟
正如我们在第6.1节所解释的，DRAM突发传输是一种并行组织形式：在DRAM核心阵列中，多个位置被并行访问。然而，单靠突发传输不足以实现现代处理器所需的DRAM访问带宽。DRAM系统通常采用另外两种并行组织形式：储存体和通道。在最高层次上，一个处理器包含一个或多个通道。每个通道都是一个带有总线的内存控制器，该总线将一组DRAM储存体连接到处理器。图6.7展示了一个包含四个通道的处理器，每个通道通过总线将四个DRAM储存体连接到处理器。在实际系统中，处理器通常有一到八个通道，并且每个通道连接大量的储存体。

总线的数据传输带宽由其宽度和时钟频率定义。现代双倍数据速率（DDR）总线在每个时钟周期执行两次数据传输：一次在时钟周期的上升沿，另一次在下降沿。例如，一个时钟频率为1 GHz的64位DDR总线的带宽为8B * 2 * 1 GHz = 16GB/s。这看起来是一个很大的数值，但对现代CPU和GPU来说往往不够。一个现代CPU可能需要至少32 GB/s的内存带宽，而一个现代GPU可能需要256 GB/s。对于这个例子，CPU需要2个通道，GPU则需要16个通道。

对于每个通道，连接到它的储存体数量取决于完全利用总线数据传输带宽所需的储存体数量。如图6.8所示，每个储存体包含一个DRAM单元阵列、用于访问这些单元的感应放大器以及用于将突发数据传输到总线的接口（第6.1节）。

![image](https://github.com/user-attachments/assets/97107978-d53f-494a-a512-00a81a3f8f8f)
> 图6.7 DRAM系统中的通道和储存体。

图6.8(A)展示了当单个储存体连接到通道时的数据传输时序。它显示了对储存体中DRAM单元的两个连续内存读取访问的时序。回忆第6.1节，每次访问都涉及解码器启用单元和单元与感应放大器共享其存储电荷的长延迟。这个延迟显示为时间帧左端的灰色部分。一旦感应放大器完成其工作，突发数据就会通过总线传输。图6.8中的时间帧左侧黑色部分显示了通过总线传输突发数据的时间。第二次内存读取访问将经历类似的长访问延迟（时间帧中黑色部分之间的灰色部分）后，其突发数据才能传输（右侧黑色部分）。

实际上，访问延迟（灰色部分）远长于数据传输时间（黑色部分）。显然，单储存体组织的访问传输时序会严重低效地利用通道总线的数据传输带宽。例如，如果DRAM单元阵列访问延迟与数据传输时间的比率是20:1，那么通道总线的最大利用率将是1/21=4.8%；即一个16 GB/s的通道只能以不超过0.76 GB/s的速度向处理器传输数据。这是完全不可接受的。这个问题通过将多个储存体连接到通道总线来解决。

当两个储存体连接到通道总线时，可以在第一个储存体处理另一个访问时启动第二个储存体的访问。因此，可以重叠访问DRAM单元阵列的延迟。图6.8(B)展示了两储存体组织的时序。我们假设储存体0在图6.8中显示的窗口时间之前已经启动访问。不久之后，第二个储存体也开始访问其单元阵列。当储存体0的访问完成后，它传输突发数据（时间帧最左边的黑色部分）。一旦储存体0完成数据传输，储存体1可以传输其突发数据（第二个黑色部分）。这种模式重复进行下一次访问。

![image](https://github.com/user-attachments/assets/a460f91c-bbfe-4384-97dd-fb49986c549c)
> 图6.8 储存体的使用提高了通道的数据传输带宽利用率。

从图6.8(B)中可以看出，通过拥有两个储存体，我们可以潜在地将通道总线的数据传输带宽利用率提高一倍。一般来说，如果单元阵列访问延迟与数据传输时间的比率是R，我们需要至少R + 1个储存体才能充分利用通道总线的数据传输带宽。例如，如果比率是20，我们将需要至少21个储存体连接到每个通道总线上。通常，每个通道总线上连接的储存体数量需要大于R，原因有二。一是拥有更多储存体减少了多个同时访问目标同一个储存体的概率，这种现象称为储存体冲突。由于每个储存体一次只能服务一个访问，这些冲突访问的单元阵列访问延迟不能再被重叠。拥有更多的储存体增加了这些访问分布在多个储存体中的概率。第二个原因是，每个单元阵列的大小设定为实现合理的延迟和可制造性。这限制了每个储存体能提供的单元数量。为了支持所需的内存大小，可能需要很多储存体。

并行执行线程与DRAM系统的并行组织之间存在重要联系。为了实现设备指定的内存访问带宽，必须有足够数量的线程进行同时内存访问。这一观察反映了最大化占用率的另一个好处。回想第4章“计算架构和调度”中，我们看到最大化占用率确保有足够多的线程驻留在流处理器（SMs）上以隐藏核心流水线延迟，从而高效地利用指令吞吐量。正如我们现在看到的，最大化占用率还有一个额外好处，即确保有足够多的内存访问请求以隐藏DRAM访问延迟，从而高效地利用内存带宽。当然，要实现最佳带宽利用率，这些内存访问必须均匀分布在通道和储存体之间，并且每次对储存体的访问也必须是合并访问。

图6.9展示了将数组M的元素分配到通道和储存体的示例。我们假设突发大小为两个元素（8字节）。分配由硬件设计完成。通道和储存体的地址分配如下：数组的前8字节（M[0]和M[1]）存储在通道0的储存体0中，接下来的8字节（M[2]和M[3]）存储在通道1的储存体0中，再接下来的8字节（M[4]和M[5]）存储在通道2的储存体0中，接下来的8字节（M[6]和M[7]）存储在通道3的储存体0中。

![image](https://github.com/user-attachments/assets/ccd1b525-1ea8-417e-9b0a-3f1e0985acb3)
> 图6.9 将数组元素分配到通道和储存体中。

此时，分配回到通道0，但会使用储存体1来存储接下来的8字节（M[8]和M[9]）。因此，元素M[10]和M[11]将存储在通道1的储存体1中，M[12]和M[13]将存储在通道2的储存体1中，M[14]和M[15]将存储在通道3的储存体1中。尽管图中没有显示，任何额外的元素将会回到通道0的储存体0。例如，如果有更多元素，M[16]和M[17]将存储在通道0的储存体0中，M[18]和M[19]将存储在通道1的储存体0中，依此类推。

图6.9所示的分配方案，通常称为交错数据分配，将元素分布在系统中的储存体和通道之间。这个方案确保即使是相对较小的数组也能很好地分布。因此，我们只需分配足够的元素以充分利用通道0的储存体0的DRAM突发，然后再移动到通道1的储存体0。在我们的示例中，只要我们有至少16个元素，分配就会涉及到所有通道和储存体来存储这些元素。

现在我们说明并行线程执行与并行内存组织之间的相互作用。我们将使用图5.5中的示例，复制为图6.10。我们假设乘法将使用2*2线程块和2*2瓦片进行。


![image](https://github.com/user-attachments/assets/73241896-649b-49dc-a5c0-ddc844ffc19b)
> 图6.10 矩阵乘法的小示例（复制自图5.5）。

在内核执行的第0阶段，所有四个线程块将加载它们的第一个瓦片。图6.11显示了每个瓦片中涉及的M元素。第2行显示了第0阶段访问的M元素及其二维索引。第3行显示了相同的M元素及其线性索引。假设所有线程块并行执行。我们看到每个块将进行两次合并访问。

![image](https://github.com/user-attachments/assets/12ec57af-79a2-4068-9ba4-1ede1d9d13ea)
> 图6.11 线程块在每个阶段加载的M元素。

根据图6.9中的分配，这些合并访问将进行到通道0的两个储存体以及通道2的两个储存体。这四次访问将并行进行，以利用两个通道并提高每个通道的数据传输带宽利用率。

我们还看到Block0,0和Block0,1将加载相同的M元素。大多数现代设备配备了缓存，只要这些块的执行时序足够接近，就会将这些访问合并为一次。事实上，GPU设备中的缓存主要设计用于合并这种访问并减少对DRAM系统的访问次数。

第4和第5行显示了内核执行第1阶段加载的M元素。我们看到这些访问现在进行到通道1和通道3的储存体。再次强调，这些访问将并行进行。读者应该清楚，并行线程执行和DRAM系统的并行结构之间存在共生关系。一方面，充分利用DRAM系统潜在访问带宽需要许多线程同时访问DRAM中的数据。另一方面，设备的执行吞吐量依赖于DRAM系统并行结构的良好利用，即储存体和通道。例如，如果同时执行的线程都访问同一通道中的数据，内存访问吞吐量和整体设备执行速度将大大降低。

我们邀请读者验证用相同的2*2线程块配置来相乘两个较大的矩阵（如8*8）将利用图6.9中的所有四个通道。另一方面，增加的DRAM突发大小需要更大的矩阵乘法来充分利用所有通道的数据传输带宽。

|[top](https://github.com/tunglinwood/CUDA/blob/main/6_%E6%80%A7%E8%83%BD%E8%80%83%E8%99%91.md#6_%E6%80%A7%E8%83%BD%E8%80%83%E8%99%91)|
|-|

## 6.3 线程粗化

到目前为止，在我们所看到的所有内核中，工作都是在最细的粒度上进行线程并行化的。也就是说，每个线程被分配了最小的工作单元。例如，在向量加法内核中，每个线程被分配了一个输出元素。在RGB到灰度转换和图像模糊内核中，每个线程被分配了输出图像中的一个像素。在矩阵乘法内核中，每个线程被分配了输出矩阵中的一个元素。

以最细粒度在线程间并行化工作的优势在于，它增强了透明的可扩展性，如第4章《计算架构和调度》中所讨论的。如果硬件有足够的资源来并行执行所有工作，那么应用程序就暴露了足够的并行性来充分利用硬件。否则，如果硬件没有足够的资源来并行执行所有工作，硬件可以通过一个接一个地执行线程块来串行化工作。

以最细粒度并行化工作的缺点在于，当并行化工作需要付出“代价”时，这种代价可能有多种形式，例如不同线程块对数据的冗余加载、冗余工作、同步开销等。当线程被硬件并行执行时，这种并行代价往往是值得付出的。然而，如果由于资源不足而导致硬件串行化工作，那么这种代价就是不必要的。在这种情况下，程序员最好部分串行化工作，减少并行的代价。这可以通过为每个线程分配多个工作单元来实现，这通常被称为线程粗化。

我们通过第5章《内存架构和数据局部性》中的瓦片矩阵乘法示例来演示线程粗化优化。图6.12展示了计算输出矩阵`P`的两个水平相邻输出瓦片的内存访问模式。对于每个这些输出瓦片，我们观察到需要加载矩阵`N`的不同输入瓦片。然而，对于两个输出瓦片，矩阵`M`的相同输入瓦片被加载。

![image](https://github.com/user-attachments/assets/93c8ebd6-f2ad-49a0-aaf2-81eca59e6830)
> 图6.12 瓦片矩阵乘法的线程粗化。

在第5章《内存架构和数据局部性》的瓦片实现中，每个输出瓦片由不同的线程块处理。由于共享内存内容不能在块间共享，每个块必须加载自己的一份矩阵M的输入瓦片。尽管让不同的线程块加载相同的输入瓦片是冗余的，但这是我们为了能够使用不同的块并行处理两个输出瓦片而付出的代价。如果这些线程块并行运行，这个代价可能是值得的。另一方面，如果这些线程块被硬件串行化，这个代价就白白付出了。在后一种情况下，程序员最好让一个线程块处理两个输出瓦片，这样块中的每个线程处理两个输出元素。这样，粗化的线程块将只需加载一次矩阵M的输入瓦片，并将其用于多个输出瓦片。

图6.13展示了如何将线程粗化应用到第5章《内存架构和数据局部性》的瓦片矩阵乘法代码中。在第02行添加了一个常量`COARSE_FACTOR`来表示粗化因子，即每个粗化线程将负责的原始工作单元数量。在第13行中，列索引的初始化被替换为`colStart`的初始化，`colStart`是线程负责的第一列的索引，因为线程现在负责多个具有不同列索引的元素。在计算`colStart`时，块索引bx被乘以`TILE_WIDTH*COARSE_FACTOR`，而不是仅仅是TILE_WIDTH，因为每个线程块现在负责`TILE_WIDTH*COARSE_FACTOR`列。在第16-19行中，声明并初始化了多个`Pvalue`实例，每个实例对应一个粗化线程负责的元素。在第17行的循环中，迭代了粗化线程负责的不同工作单元，这个循环有时被称为粗化循环。在第22行的循环中，只加载了矩阵M的一个瓦片，每次循环迭代时与原始代码相同。然而，对于每个加载的矩阵M的瓦片，粗化循环在第27行加载并使用了多个矩阵N的瓦片。这个循环首先确定粗化线程负责的当前瓦片的列（第29行），然后加载矩阵N的瓦片（第32行）并使用该瓦片计算并更新不同的`Pvalue`（第35-37行）。最后，在第44-47行，另一个粗化循环用于每个粗化线程来更新其负责的输出元素。

```cuda
#define TILE_WIDTH    32
#define COARSE_FACTOR 4
__global__ void matrixMulKernel(float* M, float* N, float* P, int width){
    __shared__ float Mds[TILE_WIDTH][TILE_WIDTH];
    __shared__ float Nds[TILE_WIDTH][TILE_WIDTH];

    int bx = blockIdx.x; int by = blockIdx.y;
    int tx = threadIdx.x; int ty = threadIdx.y;

    // 确定要处理的P元素的行和列
    int row = by*TILE_WIDTH + ty;
    int colStart = bx*TILE_WIDTH*COARSE_FACTOR + tx;

    // 为所有输出元素初始化Pvalue
    float Pvalue[COARSE_FACTOR];
    for(int c = 0; c < COARSE_FACTOR; ++c){
        Pvalue[c] = 0.0f;
    }

    // 循环遍历计算P元素所需的M和N瓦片
    for(int ph = 0; ph < width/TILE_WIDTH; ++ph){

        // 协同加载M瓦片到共享内存
        Mds[ty][tx] = M[row*width + ph*TILE_WIDTH + tx];

        for(int c = 0; c < COARSE_FACTOR; ++c){

          int col = colStart + c*TILE_WIDTH;

          // 协同加载N瓦片到共享内存
          Nds[ty][tx] = N[(ph*TILE_WIDTH + ty)*width + col];
          __syncthreads();

          for(int k = 0; k < TILE_WIDTH; ++k){
              Pvalue[c] += Mds[ty][k]*Nds[k][tx];
          }
          __syncthreads();
        }
    }

    for(int c = 0; c < COARSE_FACTOR; ++c){
        int col = colStart + c*TILE_WIDTH;
        P[row*width + col] = Pvalue[c];
    }

}
```
> 图6.13 瓦片矩阵乘法的线程粗化代码。

线程粗化是一种强大的优化技术，可以显著提高许多应用的性能。这是一种常见的优化方法。然而，在应用线程粗化时需要避免几个陷阱。首先，必须小心在不必要的时候不要应用此优化。回顾一下，线程粗化在并行化的代价可以通过粗化来减少时是有益的，例如冗余的数据加载、冗余工作、同步开销等。并不是所有计算都有这样的代价。例如，在第2章《异构数据并行计算》的向量加法内核中，处理不同向量元素并行并不会付出代价。因此，将线程粗化应用于向量加法内核不会显著改变性能。同样适用于第3章《多维网格和数据》的RGB到灰度转换内核。

第二个要避免的陷阱是不要应用过多的粗化，以至于硬件资源变得未被充分利用。回顾一下，向硬件暴露尽可能多的并行性可以实现透明的可扩展性。这为硬件提供了根据其执行资源的多少来并行化或串行化工作的灵活性。当程序员粗化线程时，他们减少了向硬件暴露的并行性。如果粗化因子过高，将不会向硬件暴露足够的并行性，导致一些并行执行资源未被利用。实际上，不同的设备有不同的执行资源，因此最佳粗化因子通常是特定于设备和数据集的，并需要针对不同的设备和数据集进行重新调整。因此，当应用线程粗化时，可扩展性变得不那么透明。

应用线程粗化的第三个陷阱是避免增加资源消耗到损害占用率的程度。根据内核的不同，线程粗化可能需要每个线程使用更多的寄存器或每个线程块使用更多的共享内存。如果是这种情况，程序员必须小心不要使用过多的寄存器或共享内存，以至于占用率降低。减少占用率所带来的性能损失可能比线程粗化所提供的性能提升更为严重。

|[top](https://github.com/tunglinwood/CUDA/blob/main/6_%E6%80%A7%E8%83%BD%E8%80%83%E8%99%91.md#6_%E6%80%A7%E8%83%BD%E8%80%83%E8%99%91)|
|-|

## 6.4 优化清单

在本书的第一部分中，我们已经涵盖了各种常见的优化技术。我们将这些优化整合成一个清单，如表 6.1 所示。这个清单不是详尽无遗的，但包含了许多普遍适用的优化，程序员应首先考虑。在本书的第二部分和第三部分，我们将把这个清单中的优化应用于各种并行模式和应用，以理解它们在不同上下文中的作用。在本节中，我们将简要回顾每种优化及其应用策略。

|优化|对计算核心的好处|对内存的好处|策略|
|:-|:-|:-|:-|
|最大化占用率|更多的工作来隐藏流水线延迟|更多的并行内存访问来隐藏 DRAM 延迟|调整 SM 资源的使用，如每块线程数、每块共享内存和每个线程的寄存器|
|启用合并的全局内存访问|减少等待全局内存访问的流水线停顿|减少全局内存流量，更好地利用突发/缓存行|以合并的方式在全局内存和共享内存之间传输，执行共享内存中的未合并访问（例如角转）;重新排列线程到数据的映射;重新排列数据的布局|
|最小化控制分歧|提高 SIMD 效率（在 SIMD 执行期间更少的空闲核心）|...|重新安排线程到工作的映射和/或数据;重新安排数据的布局|
|数据的块状重用|减少等待全局内存访问的流水线停顿|减少全局内存流量|将块内重复使用的数据放置在共享内存或寄存器中，以便仅在全局内存和 SM 之间传输一次|
|私有化（稍后介绍）|减少等待原子更新的流水线停顿|减少原子更新的争用和序列化|对数据的私有副本进行部分更新，然后在完成时更新通用副本|
|线程粗化|减少冗余工作、分歧或同步|减少冗余全局内存流量|为每个线程分配多个并行单元，以减少并行化的成本（如果硬件原本会序列化线程）|

表 6.1 优化清单

表 6.1 中的第一个优化是最大化 SM 上线程的占用率。这种优化在第 4 章《计算架构与调度》中引入，强调了拥有比核心更多的线程对于隐藏核心流水线中的长延迟操作的重要性。为了最大化占用率，程序员可以调整内核的资源使用，以确保最大数量的块或寄存器不会限制同时分配给 SM 的线程数量。在第 5 章《内存架构与数据局部性》中，引入了共享内存作为另一个需要仔细调整的资源，以免限制占用率。在本章中，最大化占用率的重要性被讨论为隐藏内存延迟的手段，而不仅仅是核心流水线延迟。拥有足够多的线程同时执行，确保生成足够的内存访问，以充分利用内存带宽。

表 6.1 中的第二个优化是使用合并的全局内存访问，通过确保同一 warp 中的线程访问相邻的内存位置。这种优化在本章中介绍，强调了硬件将对相邻内存位置的访问合并为单个内存请求的能力，从而减少全局内存流量并提高 DRAM 突发的利用率。到目前为止，我们在本书中看到的内核自然地展现了合并的访问模式。然而，我们将在本书的第二部分和第三部分中看到许多例子，其中内存访问模式更为不规则，因此需要更多的努力来实现合并。

实现合并的策略包括在全局内存到共享内存的传输中采用合并的方式，然后在共享内存中执行不规则访问。我们已经在本章中看到这种策略的一个例子，即角转。在第 12 章《合并》中，我们将看到另一个例子。在这个模式中，同一个块中的线程需要在相同数组中执行二分查找，因此它们协作以合并的方式将数组从全局内存加载到共享内存，然后每个线程在共享内存中执行二分查找。在第 13 章《排序》中，我们将看到该策略的另一个例子。在这个模式中，线程以分散的方式将结果写入数组，因此它们可以协作在共享内存中执行分散的访问，然后将结果从共享内存写回到全局内存，以便元素具有相邻的目标位置时启用更多的合并。

另一种实现合并的策略是重新排列线程到数据元素的映射。我们将在第 10 章《减少和最小化分歧》中看到这种策略的一个例子，在这个模式中，我们将讨论减少模式。另一种策略是重新排列数据的布局。我们将在第 14 章《稀疏矩阵计算》中看到这种策略的一个例子，特别是在讨论 ELL 和 JDS 格式时。

表 6.1 中的第三个优化是最小化控制分歧。控制分歧在第 4 章《计算架构与调度》中介绍，强调了确保同一 warp 中的线程执行相同控制路径的重要性，以确保在 SIMD 执行期间所有核心都被有效利用。到目前为止，我们在本书中看到的内核没有表现出控制分歧，除了在边界条件下不可避免的分歧。然而，我们将在本书的第二部分和第三部分中看到许多例子，其中控制分歧可能对性能造成重大损害。

最小化控制分歧的策略包括重新安排工作和/或数据的分布，以确保一个 warp 中的线程都在使用之前再使用其他 warp 的线程。在第 10 章《减少和最小化分歧》中，我们将看到这种策略的一个例子。在第 11 章《前缀和（扫描）》中，我们将看到这种策略的另一个例子。通过重新安排工作和/或数据的分布，也可以确保同一 warp 中的线程具有类似的工作负载。我们将在第 15 章《图遍历》中看到这种策略的一个例子，在讨论 vertex-centric 和 edge-centric 并行化方案的权衡时。另一种最小化控制分歧的策略是重新排列数据的布局，以确保处理相邻数据的线程在同一 warp 中具有类似的工作负载。我们将在第 14 章《稀疏矩阵计算》中看到这种策略的一个例子，特别是在讨论 JDS 格式时。

表 6.1 中的第四个优化是块状数据重用，将块内重复使用的数据放置在共享内存或寄存器中，以便仅在全局内存和 SM 之间传输一次。块状优化在第 5 章《内存架构与数据局部性》中引入，以矩阵乘法为例，其中处理同一输出块的线程协作加载相应的输入块到共享内存中，然后重复访问这些输入块。在本书的第二部分和第三部分中，我们将看到这一优化的应用。我们将观察到当输入和输出块具有不同尺寸时应用块状优化的挑战。这一挑战在第 7 章《卷积》和第 8 章《模板》中出现。我们还将观察到，数据的块状存储不仅可以在共享内存中进行，还可以在寄存器中进行。这一观察在第 8 章《模板》中最为明显。我们还将观察到，块状优化适用于重复访问的输出数据，而不仅仅是输入数据。

表 6.1 中的第五个优化是私有化。这种优化尚未介绍，但我们在此提及以求完整。私有化涉及到多个线程或块需要更新通用输出的情况。为了避免同时更新相同数据的开销，可以创建数据的私有副本并进行部分更新，然后在完成时从私有副本更新通用副本。我们将在第 9 章《并行直方图》中看到这种优化的一个例子，其中多个线程需要更新相同的直方图计数器。在第 15 章《图遍历》中，我们也将看到这种优化的一个例子，其中多个线程需要向同一个队列添加条目。

表 6.1 中的第六个优化是线程粗化，其中将多个并行单元分配给一个线程，以减少并行化的成本（如果硬件原本会序列化线程）。线程粗化在本章中介绍，以块状矩阵乘法为例，其中并行化的成本是多个线程块重复加载相同的输入块。通过为多个相邻的输出块分配一个线程块，可以仅加载一次输入块。我们将在本书的第二部分和第三部分中看到线程粗化在不同上下文中的应用，每次都有不同的并行

化成本。在第 8 章《模板》中，线程粗化有助于减少与私有化优化相关的需要提交的私有副本数量。在第 10 章《减少和最小化分歧》中，以及第 11 章《前缀和（扫描）》中，线程粗化也有助于减少并行算法相对于顺序算法所执行的冗余工作。在第 12 章《合并》中，线程粗化减少了识别每个线程的输入段所需的二分查找操作数量。在第 13 章《排序》中，线程粗化有助于提高内存的合并效率。

再次强调，表 6.1 中的清单并非详尽无遗，但包含了不同计算模式中常见的主要优化类型。这些优化在本书第二部分和第三部分的多个章节中出现。我们还将看到其他特定章节中出现的优化。例如，在第 7 章《卷积》中，我们将介绍常量内存的使用。在第 10 章《减少和最小化分歧》中，我们将介绍双缓冲优化。

|[top](https://github.com/tunglinwood/CUDA/blob/main/6_%E6%80%A7%E8%83%BD%E8%80%83%E8%99%91.md#6_%E6%80%A7%E8%83%BD%E8%80%83%E8%99%91)|
|-|

## 6.5 了解计算的瓶颈

在决定对特定计算应用哪种优化时，首先了解限制该计算性能的资源是很重要的。限制计算性能的资源通常被称为性能瓶颈。优化通常通过增加一种资源的使用量来减轻对另一种资源的压力。如果应用的优化没有针对瓶颈资源，可能不会从中受益。更糟糕的是，优化尝试可能会实际降低性能。

例如，共享内存的块状优化增加了共享内存的使用，以减少对全局内存带宽的压力。当瓶颈资源是全局内存带宽且正在加载的数据被重用时，这种优化效果很好。然而，如果性能被占用率限制，而占用率又受到已经使用过多共享内存的限制，那么应用共享内存的块状优化可能会使情况更糟。

为了了解限制计算性能的资源，GPU 计算平台通常提供各种分析工具。我们建议读者参考 CUDA 文档，以获取有关如何使用分析工具来识别计算的性能瓶颈的更多信息（NVIDIA，Profiler）。性能瓶颈可能是硬件特定的，这意味着相同的计算在不同设备上可能会遇到不同的瓶颈。因此，识别性能瓶颈和应用性能优化的过程需要对 GPU 架构以及不同 GPU 设备之间的架构差异有良好的理解。

|[top](https://github.com/tunglinwood/CUDA/blob/main/6_%E6%80%A7%E8%83%BD%E8%80%83%E8%99%91.md#6_%E6%80%A7%E8%83%BD%E8%80%83%E8%99%91)|
|-|

## 6.6 总结

在本章中，我们涵盖了 GPU 的离芯内存（DRAM）架构，并讨论了相关的性能考虑因素，如全局内存访问合并和通过内存并行性隐藏内存延迟。然后，我们介绍了一个重要的优化：线程粒度粗化。通过本章和之前章节提供的见解，读者应该能够对他们遇到的任何内核代码的性能进行推理。我们通过呈现广泛用于优化许多计算的常见性能优化清单来结束本部分。我们将在本书的接下来的两部分中继续研究这些优化在并行计算模式和应用案例研究中的实际应用。

## 练习

1. 编写一个矩阵乘法内核函数，与图 6.4 中的设计相对应。
2. 对于块状矩阵乘法，在可能的 `BLOCK_SIZE` 范围内，对于哪些 `BLOCK_SIZE` 值，内核将完全避免对全局内存的未合并访问？（只考虑正方块）
3. 考虑以下 CUDA 内核：
```cuda
__global__ void foo_kernel(float* a, float* b, float* c, float* d, float* e){
    unsigned int i = blockIdx.x*blockDim.x + threadIdx.x;
    __shared__ float a_s[256];
    __shared__ float bc_s[4*256];
    a_s[threadIdx.x] = a[i];
    for(unsigned int j = 0; j < 4; ++j){
        bc_s[j*256 + threadIdx.x] = b[j*blockDim.x*gridDim.x + i] + c[i*4 + j];
    }
    __syncthreads();
    d[i + 8] = a_s[threadIdx.x];
    e[i*8] = bc_s[threadIdx.x*4];
}
```
对于以下每个内存访问，指定它们是合并的还是未合并的，或者合并不适用：
a. 第 05 行对数组 `a` 的访问
b. 第 05 行对数组 `a_s` 的访问
c. 第 07 行对数组 `b` 的访问
d. 第 07 行对数组 `c` 的访问
e. 第 07 行对数组 `bc_s` 的访问
f. 第 10 行对数组 `a_s` 的访问
g. 第 10 行对数组 `d` 的访问
h. 第 11 行对数组 `bc_s` 的访问
i. 第 11 行对数组 `e` 的访问
4. 对于以下矩阵乘法内核，每个内核的浮点到全局内存访问的比率（以 OP/B 为单位）是多少？
a. 第 3 章《多维网格与数据》中描述的简单内核，没有应用任何优化。
b. 第 5 章《内存架构与数据局部性》中描述的内核，应用了共享内存块状优化，块大小为 32*32。
c. 本章中描述的内核，应用了共享内存块状优化，块大小为 32*32，并且应用了线程粗化，粗化因子为 4。

|[top](https://github.com/tunglinwood/CUDA/blob/main/6_%E6%80%A7%E8%83%BD%E8%80%83%E8%99%91.md#6_%E6%80%A7%E8%83%BD%E8%80%83%E8%99%91)|
|-|
