# 4_计算架构与调度

* 4.1 [现代GPU的架构]()
* 4.2 [块调度]()
* 4.3 [同步与透明扩展性]()
* 4.4 [扭曲和SIMD硬件]()
* 4.5 [控制分歧]()
* 4.6 [扭曲调度与延迟容忍]()
* 4.7 [资源分配与占用]()
* 4.8 [查询设备属性]()
* 4.9 [总结]()
* [练习]()

在第1章，介绍中，我们看到CPU的设计目标是最小化指令执行的延迟，而GPU的设计目标是最大化指令执行的吞吐量。
在第2章，异构数据并行计算和第3章，多维网格和数据中，我们学习了用于创建和调用内核以启动和执行线程的CUDA编程接口的核心特性。
在接下来的三章中，我们将讨论现代GPU的架构，包括计算架构和内存架构，以及基于对这些架构理解的性能优化技术。
本章介绍了CUDA C程序员理解和推理其内核代码性能行为所需了解的几个GPU计算架构方面。我们将从展示计算架构的高层次、简化视图开始，探讨灵活的资源分配、块调度和占用的概念。
然后，我们将深入到线程调度、延迟容忍、控制分歧和同步。我们将以描述可以用来查询GPU可用资源的API函数和帮助估算GPU在执行内核时占用率的工具来结束本章。
在接下来的两章中，我们将介绍GPU内存架构的核心概念和编程考虑事项。
特别是，第5章，内存架构和数据局部性，重点介绍了片上内存架构，第6章，性能考虑，简要介绍了片外内存架构，并详细讨论了整个GPU架构的各种性能考虑。
掌握这些概念的CUDA C程序员能够编写和理解高性能并行内核。

## 4.1 现代GPU的架构
图4.1展示了典型的支持CUDA的GPU架构的高层次视图，这是CUDA C程序员的视角。它被组织成一组高度线程化的流处理器（SM）数组。每个SM都有几个处理单元，称为流处理器或CUDA核心（以下简称为核心，为了简洁），如图4.1中SM内的小方块所示，它们共享控制逻辑和内存资源。例如，Ampere A100 GPU有108个SM，每个有64个核心，总共6912个核心。
SM还带有不同的片上内存结构，统称为图4.1中的“内存”。这些片上内存结构将在第5章，内存架构和数据局部性中讨论。GPU还带有数GB的片外设备内存，称为图4.1中的“全局内存”。虽然较旧的GPU使用图形双倍数据速率同步DRAM，最近的GPU从NVIDIA的Pascal架构开始可能使用HBM（高带宽内存）或HBM2，这些都由与GPU紧密集成在同一封装中的DRAM（动态随机存取内存）模块组成。为了简洁起见，本书将广泛地将所有这些类型的内存统称为DRAM。我们将在第6章，性能考虑中讨论访问GPU DRAM的最重要概念。

## 4.2 块调度
当内核被调用时，CUDA运行时系统启动一个线程网格来执行内核代码。这些线程按块分配到SM上。即，一个块中的所有线程同时分配到同一个SM上。

![image](https://github.com/user-attachments/assets/59f7a188-e333-45f3-9d16-bfa361361f25)
> 图4.1 支持CUDA的GPU架构

图4.2说明了块分配到SM上的过程。多个块可能同时分配到同一个SM上。例如，在图4.2中，每个SM上分配了三个块。然而，块需要保留硬件资源来执行，因此只能同时分配有限数量的块到一个SM上。块数量的限制取决于第4.6节讨论的各种因素。
由于SM数量有限，并且每个SM可以同时分配的块数量有限，因此在CUDA设备中可以同时执行的块总数是有限的。大多数网格包含的块数量远超过这个数量。为了确保网格中的所有块都能执行，运行时系统维护一个需要执行的块列表，并在先前分配的块完成执行后，将新块分配给SM。
按块分配线程到SM上保证了同一个块中的线程同时在同一个SM上调度。这一保证使得同一个块中的线程可以相互交互，而不同块中的线程不能相互交互。1 这包括障碍同步，第4.3节中讨论。还包括访问驻留在SM上的低延迟共享内存，第5章，内存架构和数据局部性中讨论。

## 4.3 同步与透明扩展性
CUDA允许同一个块中的线程使用障碍同步函数`__syncthreads()`来协调它们的活动。注意“__”由两个“_”字符组成。当线程调用`__syncthreads()`时，它将停留在调用的位置，直到同一个块中的每个线程都到达该位置。
这确保了一个块中的所有线程在任何一个线程进入下一阶段之前都已完成其执行阶段。

![image](https://github.com/user-attachments/assets/5216e68d-cf80-4a2f-aa20-7486e3e08d42)
> 图4.2 线程块分配到流处理器（SM）上的示意图

障碍同步是一种简单而流行的并行活动协调方法。在现实生活中，我们经常使用障碍同步来协调多个人的并行活动。例如，假设四个朋友开车去购物中心。他们可以去不同的商店买自己的衣服。这是一种并行活动，比他们作为一个组一起依次访问所有感兴趣的商店效率高得多。然而，在他们离开购物中心之前需要障碍同步。他们必须等到四个朋友都回到车上才能离开。那些比其他人早完成购物的人必须等待那些较晚完成的人。如果没有障碍同步，当车离开时可能会有一个或多个人被留在购物中心，这会严重损害他们的友谊！
图4.3说明了障碍同步的执行。块中有N个线程。时间从左向右移动。一些线程很早到达障碍同步语句，一些线程则晚很多。那些早到达障碍的线程将等待那些晚到达的线程。当最后一个到达障碍时，所有线程都可以继续执行。通过障碍同步，“没有人被落下。”随着时间的推移。垂直曲线标志着每个线程执行`__syncthreads`语句的时间。垂直线右侧的空白区域描绘了每个线程等待所有线程完成的时间。垂直线标志着最后一个线程执行`__syncthreads`语句的时间，之后所有线程都可以继续执行`__syncthreads`语句之后的语句。

![image](https://github.com/user-attachments/assets/28fe9df6-0a1d-4b8d-92eb-7820a0861207)
> 图4.3 障碍同步的执行示例。箭头表示执行活动

在CUDA中，如果存在`__syncthreads()`语句，块中的所有线程都必须执行它。当`__syncthreads()`语句放置在if语句中时，块中的所有线程要么执行包含`__syncthreads()`的路径，要么都不执行。对于if-then-else语句，如果每个路径都有一个`__syncthreads()`语句，块中的所有线程要么执行then路径，要么执行else路径。两个`__syncthreads()`是不同的障碍同步点。例如，在图4.4中，if语句第4行开始有两个`__syncthreads()`。所有线程中偶数`threadIdx.x`值的线程执行then路径，而剩余线程执行else路径。第6行和第10行的`__syncthreads()`调用定义了两个不同的障碍。由于不能保证块中的所有线程执行任一个障碍，同步代码违反了使用`__syncthreads()`的规则，并导致未定义的执行行为。一般来说，不正确使用障碍同步可能导致结果不正确，或者线程永远等待彼此，这被称为死锁。程序员有责任避免这种不恰当的障碍同步使用。
障碍同步对块内的线程施加了执行约束。这些线程应该在时间上接近同步执行，以避免过长的等待时间。更重要的是，系统需要确保所有涉及障碍同步的线程都有必要的资源，最终到达障碍。否则，一个从未到达障碍同步点的线程可能会导致死锁。CUDA运行时系统通过将执行资源分配给块中的所有线程作为一个单元来满足这一约束，如第4.2节中所见。
不仅块中的所有线程必须分配到同一个SM，而且需要同时分配到该SM。即，只有当运行时系统已经为块中的所有线程提供了完成执行所需的所有资源时，块才可以开始执行。这确保了块中所有线程的时间接近性，并防止在障碍同步期间出现过长甚至无限的等待时间。
这导致了CUDA障碍同步设计中的一个重要权衡。
通过不允许不同块中的线程彼此进行障碍同步，CUDA运行时系统可以相对于彼此以任何顺序执行块，因为它们不需要相互等待。这种灵活性使得扩展实现成为可能，如图4.5所示。图中的时间从上到下进行。在一个低成本系统中，只有少量的执行资源，一个可以同时执行少量的块，如图4.5左侧显示的同时执行两个块。在一个高端实现中，有更多的执行资源，一个可以同时执行许多块，如图4.5右侧显示的同时执行四个块。一个今天的高端GPU可以同时执行数百个块。

```cuda
void incorrect_barrier_example(int i){
    ...
    if (threadIdx.x % 2 == 0){
        ...
        __syncthreads{};
    }
    else {
        ...
        __syncthreads{};
    }
}
```  
> 图4.4 不正确使用`__syncthreads()`的示例

能够以各种速度执行相同的应用代码使得根据不同市场细分的成本、功耗和性能要求生产各种实现成为可能。例如，一个移动处理器可能以极低的功耗执行应用程序，而一个台式处理器可能以更高的速度执行相同的应用程序，同时消耗更多的功耗。两者都无需改变代码执行相同的应用程序。
在不同硬件上以不同数量的执行资源执行相同应用代码的能力被称为透明扩展性，这减少了应用开发者的负担并提高了应用的可用性。
