# 4_计算架构与调度

* 4.1 [现代GPU的架构]()
* 4.2 [块调度]()
* 4.3 [同步与透明扩展性]()
* 4.4 [扭曲和SIMD硬件]()
* 4.5 [控制分歧]()
* 4.6 [扭曲调度与延迟容忍]()
* 4.7 [资源分配与占用]()
* 4.8 [查询设备属性]()
* 4.9 [总结]()
* [练习]()

在第1章，介绍中，我们看到CPU的设计目标是最小化指令执行的延迟，而GPU的设计目标是最大化指令执行的吞吐量。
在第2章，异构数据并行计算和第3章，多维网格和数据中，我们学习了用于创建和调用内核以启动和执行线程的CUDA编程接口的核心特性。
在接下来的三章中，我们将讨论现代GPU的架构，包括计算架构和内存架构，以及基于对这些架构理解的性能优化技术。
本章介绍了CUDA C程序员理解和推理其内核代码性能行为所需了解的几个GPU计算架构方面。我们将从展示计算架构的高层次、简化视图开始，探讨灵活的资源分配、块调度和占用的概念。
然后，我们将深入到线程调度、延迟容忍、控制分歧和同步。我们将以描述可以用来查询GPU可用资源的API函数和帮助估算GPU在执行内核时占用率的工具来结束本章。
在接下来的两章中，我们将介绍GPU内存架构的核心概念和编程考虑事项。
特别是，第5章，内存架构和数据局部性，重点介绍了片上内存架构，第6章，性能考虑，简要介绍了片外内存架构，并详细讨论了整个GPU架构的各种性能考虑。
掌握这些概念的CUDA C程序员能够编写和理解高性能并行内核。

## 4.1 现代GPU的架构
图4.1展示了典型的支持CUDA的GPU架构的高层次视图，这是CUDA C程序员的视角。它被组织成一组高度线程化的流处理器（SM）数组。每个SM都有几个处理单元，称为流处理器或CUDA核心（以下简称为核心，为了简洁），如图4.1中SM内的小方块所示，它们共享控制逻辑和内存资源。例如，Ampere A100 GPU有108个SM，每个有64个核心，总共6912个核心。
SM还带有不同的片上内存结构，统称为图4.1中的“内存”。这些片上内存结构将在第5章，内存架构和数据局部性中讨论。GPU还带有数GB的片外设备内存，称为图4.1中的“全局内存”。虽然较旧的GPU使用图形双倍数据速率同步DRAM，最近的GPU从NVIDIA的Pascal架构开始可能使用HBM（高带宽内存）或HBM2，这些都由与GPU紧密集成在同一封装中的DRAM（动态随机存取内存）模块组成。为了简洁起见，本书将广泛地将所有这些类型的内存统称为DRAM。我们将在第6章，性能考虑中讨论访问GPU DRAM的最重要概念。

## 4.2 块调度
当内核被调用时，CUDA运行时系统启动一个线程网格来执行内核代码。这些线程按块分配到SM上。即，一个块中的所有线程同时分配到同一个SM上。

![image](https://github.com/user-attachments/assets/59f7a188-e333-45f3-9d16-bfa361361f25)
> 图4.1 支持CUDA的GPU架构

图4.2说明了块分配到SM上的过程。多个块可能同时分配到同一个SM上。例如，在图4.2中，每个SM上分配了三个块。然而，块需要保留硬件资源来执行，因此只能同时分配有限数量的块到一个SM上。块数量的限制取决于第4.6节讨论的各种因素。
由于SM数量有限，并且每个SM可以同时分配的块数量有限，因此在CUDA设备中可以同时执行的块总数是有限的。大多数网格包含的块数量远超过这个数量。为了确保网格中的所有块都能执行，运行时系统维护一个需要执行的块列表，并在先前分配的块完成执行后，将新块分配给SM。
按块分配线程到SM上保证了同一个块中的线程同时在同一个SM上调度。这一保证使得同一个块中的线程可以相互交互，而不同块中的线程不能相互交互。1 这包括障碍同步，第4.3节中讨论。还包括访问驻留在SM上的低延迟共享内存，第5章，内存架构和数据局部性中讨论。

## 4.3 同步与透明扩展性
CUDA允许同一个块中的线程使用障碍同步函数`__syncthreads()`来协调它们的活动。注意“__”由两个“_”字符组成。当线程调用`__syncthreads()`时，它将停留在调用的位置，直到同一个块中的每个线程都到达该位置。
这确保了一个块中的所有线程在任何一个线程进入下一阶段之前都已完成其执行阶段。

![image](https://github.com/user-attachments/assets/5216e68d-cf80-4a2f-aa20-7486e3e08d42)
> 图4.2 线程块分配到流处理器（SM）上的示意图

障碍同步是一种简单而流行的并行活动协调方法。在现实生活中，我们经常使用障碍同步来协调多个人的并行活动。例如，假设四个朋友开车去购物中心。他们可以去不同的商店买自己的衣服。这是一种并行活动，比他们作为一个组一起依次访问所有感兴趣的商店效率高得多。然而，在他们离开购物中心之前需要障碍同步。他们必须等到四个朋友都回到车上才能离开。那些比其他人早完成购物的人必须等待那些较晚完成的人。如果没有障碍同步，当车离开时可能会有一个或多个人被留在购物中心，这会严重损害他们的友谊！
图4.3说明了障碍同步的执行。块中有N个线程。时间从左向右移动。一些线程很早到达障碍同步语句，一些线程则晚很多。那些早到达障碍的线程将等待那些晚到达的线程。当最后一个到达障碍时，所有线程都可以继续执行。通过障碍同步，“没有人被落下。”随着时间的推移。垂直曲线标志着每个线程执行`__syncthreads`语句的时间。垂直线右侧的空白区域描绘了每个线程等待所有线程完成的时间。垂直线标志着最后一个线程执行`__syncthreads`语句的时间，之后所有线程都可以继续执行`__syncthreads`语句之后的语句。

![image](https://github.com/user-attachments/assets/28fe9df6-0a1d-4b8d-92eb-7820a0861207)
> 图4.3 障碍同步的执行示例。箭头表示执行活动

在CUDA中，如果存在`__syncthreads()`语句，块中的所有线程都必须执行它。当`__syncthreads()`语句放置在if语句中时，块中的所有线程要么执行包含`__syncthreads()`的路径，要么都不执行。对于if-then-else语句，如果每个路径都有一个`__syncthreads()`语句，块中的所有线程要么执行then路径，要么执行else路径。两个`__syncthreads()`是不同的障碍同步点。例如，在图4.4中，if语句第4行开始有两个`__syncthreads()`。所有线程中偶数`threadIdx.x`值的线程执行then路径，而剩余线程执行else路径。第6行和第10行的`__syncthreads()`调用定义了两个不同的障碍。由于不能保证块中的所有线程执行任一个障碍，同步代码违反了使用`__syncthreads()`的规则，并导致未定义的执行行为。一般来说，不正确使用障碍同步可能导致结果不正确，或者线程永远等待彼此，这被称为死锁。程序员有责任避免这种不恰当的障碍同步使用。
障碍同步对块内的线程施加了执行约束。这些线程应该在时间上接近同步执行，以避免过长的等待时间。更重要的是，系统需要确保所有涉及障碍同步的线程都有必要的资源，最终到达障碍。否则，一个从未到达障碍同步点的线程可能会导致死锁。CUDA运行时系统通过将执行资源分配给块中的所有线程作为一个单元来满足这一约束，如第4.2节中所见。
不仅块中的所有线程必须分配到同一个SM，而且需要同时分配到该SM。即，只有当运行时系统已经为块中的所有线程提供了完成执行所需的所有资源时，块才可以开始执行。这确保了块中所有线程的时间接近性，并防止在障碍同步期间出现过长甚至无限的等待时间。
这导致了CUDA障碍同步设计中的一个重要权衡。

![image](https://github.com/user-attachments/assets/94714bb4-12bd-493b-8493-391bd8264398)
> 图4.5 块之间缺乏同步约束使CUDA程序具有透明的可扩展性。

通过不允许不同块中的线程彼此进行障碍同步，CUDA运行时系统可以相对于彼此以任何顺序执行块，因为它们不需要相互等待。这种灵活性使得扩展实现成为可能，如图4.5所示。图中的时间从上到下进行。在一个低成本系统中，只有少量的执行资源，一个可以同时执行少量的块，如图4.5左侧显示的同时执行两个块。在一个高端实现中，有更多的执行资源，一个可以同时执行许多块，如图4.5右侧显示的同时执行四个块。一个今天的高端GPU可以同时执行数百个块。

```cuda
void incorrect_barrier_example(int i){
    ...
    if (threadIdx.x % 2 == 0){
        ...
        __syncthreads{};
    }
    else {
        ...
        __syncthreads{};
    }
}
```  
> 图4.4 不正确使用`__syncthreads()`的示例

能够以各种速度执行相同的应用代码使得根据不同市场细分的成本、功耗和性能要求生产各种实现成为可能。例如，一个移动处理器可能以极低的功耗执行应用程序，而一个台式处理器可能以更高的速度执行相同的应用程序，同时消耗更多的功耗。两者都无需改变代码执行相同的应用程序。
在不同硬件上以不同数量的执行资源执行相同应用代码的能力被称为透明扩展性，这减少了应用开发者的负担并提高了应用的可用性。

## 4.4 扭曲和SIMD硬件

我们已经看到，相对于彼此来说，块可以以任何顺序执行，这允许在不同设备之间实现透明的可扩展性。然而，我们没有详细讨论每个块内线程的执行时序。从概念上讲，应该假设块中的线程可以以任何顺序相对于彼此执行。在有阶段的算法中，如果我们希望确保所有线程在进入下一阶段之前完成上一阶段的执行，就应该使用障碍同步。内核执行的正确性不应依赖于某些线程在没有使用障碍同步的情况下彼此同步执行的假设。

CUDA GPU中的线程调度是一个硬件实现的概念，因此必须在具体硬件实现的背景下讨论。
在迄今为止的大多数实现中，一旦块被分配到一个SM（流多处理器），它就会进一步分成32线程的单位，称为扭曲（warp）。扭曲的大小是具体实现相关的，并可能在未来的GPU代中有所变化。了解扭曲有助于理解和优化CUDA应用在特定CUDA设备代上的性能。

扭曲是SM中线程调度的单位。图4.6展示了在一个实现中块分成扭曲的过程。在这个例子中，有三个块——块1、块2和块3——都分配到一个SM。每个块进一步分成扭曲以进行调度。每个扭曲由32个连续的`threadIdx`值的线程组成：线程0到31组成第一个扭曲，线程32到63组成第二个扭曲，依此类推。我们可以根据给定块大小和每个SM分配的块数量计算出驻留在SM中的扭曲数量。在这个例子中，如果每个块有256个线程，我们可以确定每个块有256/32或8个扭曲。SM中的三个块共有8*3=24个扭曲。

![image](https://github.com/user-attachments/assets/bab67b5d-823e-4228-aa8e-2273f1f0d061)
> 图4.6 块被划分为扭曲以进行线程调度。

块根据线程索引被划分为扭曲。如果一个块被组织成一维数组，即只使用`threadIdx.x`，划分是直接的。扭曲中的`threadIdx.x`值是连续且递增的。对于32大小的扭曲，扭曲0从线程0开始到线程31结束，扭曲1从线程32开始到线程63结束，依此类推。一般而言，扭曲n从线程32*n开始到线程32*(n+1)-1结束。
对于块大小不是32的倍数的情况，最后一个扭曲将用不活动的线程填充，以填满32个线程位置。例如，如果一个块有48个线程，它将被划分为两个扭曲，第二个扭曲将用16个不活动的线程填充。
对于由多个维度的线程组成的块，在划分为扭曲之前，这些维度将被投影到一个线性化的行优先布局中。线性布局由将具有较大y和z坐标的行放置在较低坐标的行之后确定。即，如果一个块由两个维度的线程组成，将通过将所有`threadIdx.y`值为1的线程放置在`threadIdx.y`值为0的线程之后形成线性布局。`threadIdx.y`值为2的线程将放置在`threadIdx.y`值为1的线程之后，以此类推。具有相同`threadIdx.y`值的线程按递增的`threadIdx.x`顺序放置在连续位置。
图4.7展示了一个将二维块的线程放置到线性布局的例子。上部显示了块的二维视图。读者应注意其与二维数组的行优先布局的相似性。每个线程显示为T<sub>y,x</sub>，`x`为`threadIdx.x`，`y`为`threadIdx.y`。下部显示了块的线性化视图。
前四个线程是`threadIdx.y`值为0的线程；它们按递增的`threadIdx.x`值排序。接下来的四个线程是`threadIdx.y`值为1的线程。它们也按递增的`threadIdx.x`值放置。在这个例子中，所有16个线程组成了一个半扭曲。该扭曲将再填充16个线程以完成一个32线程的扭曲。想象一个8*8线程的二维块。64个线程将组成两个扭曲。第一个扭曲从T<sub>0,0</sub>开始到T<sub>3,7</sub>结束。第二个扭曲从T<sub>4,0</sub>开始到T<sub>7,7</sub>结束。读者可以将其绘制出来作为练习。

![image](https://github.com/user-attachments/assets/a4e3058d-ab5c-4b41-b61a-d42affc7fdab)
> 图4.7 将二维线程放置到线性布局。

对于三维块，我们首先将所有`threadIdx.z`值为0的线程放入线性顺序。这些线程被视为一个二维块，如图4.7所示。所有`threadIdx.z`值为1的线程随后放入线性顺序，依此类推。例如，对于一个三维的2*8*4块（x维4，y维8，z维2），64个线程将被划分为两个扭曲，第一个扭曲从T<sub>0,0,0</sub>到T<sub>0,7,3</sub>，第二个扭曲从T<sub>1,0,0</sub>到T<sub>1,7,3</sub>。
SM设计成按单指令多数据（SIMD）模型执行一个扭曲中的所有线程。即，在任何时刻，为扭曲中的所有线程获取并执行一条指令（参见“扭曲和SIMD硬件”边栏）。
图4.8展示了SM中的核心如何分组成处理块，每8个核心形成一个处理块并共享一个指令获取/调度单元。以实际例子，Ampere A100 SM有64个核心，组织成四个处理块，每块16个核心。相同扭曲中的线程被分配到相同的处理块，该块获取扭曲的指令并同时对扭曲中的所有线程执行它。这些线程对数据的不同部分应用相同的指令。由于SIMD硬件有效地限制一个扭曲中的所有线程在任何时间点执行相同的指令，扭曲的执行行为通常被称为单指令多线程。
SIMD的优势在于控制硬件（如指令获取/调度单元）的成本在许多执行单元之间共享。这种设计选择使得硬件中用于控制的部分较小，更多部分用于增加算术吞吐量。我们预期在可预见的未来，扭曲划分将继续是流行的实现技术。然而，扭曲的大小可能会因具体实现而异。到目前为止，所有CUDA设备都使用了类似的扭曲配置，每个扭曲由32个线程组成。

![image](https://github.com/user-attachments/assets/e556ffb6-8eb7-47f8-a523-8166433e1e31)
> 图4.8 流多处理器组织成处理块以进行SIMD执行。

> 扭曲和SIMD硬件
> 在他1945年的开创性报告中，John von Neumann描述了一种构建电子计算机的模型，基于开创性的EDVAC计算机的设计。这个模型现在通常被称为“冯·诺依曼模型”，已经成为几乎所有现代计算机的基础蓝图。
> ![image](https://github.com/user-attachments/assets/cbd993fa-119c-475c-a477-951c506acadb)
> 冯·诺依曼模型在下图中有所展示。计算机有一个I/O（输入/输出），允许将程序和数据提供给系统并从系统生成。要执行程序，计算机首先将程序及其数据输入到内存中。
> 程序由一组指令组成。控制单元维护一个程序计数器（PC），其中包含要执行的下一条指令的内存地址。在每个“指令周期”中，控制单元使用PC将一条指令获取到指令寄存器（IR）中。然后检查指令位以确定计算机所有组件要采取的行动。这就是为什么该模型也被称为“存储程序”模型，意味着用户可以通过将不同的程序存储到内存中来改变计算机的行为。
> 通过以下反映GPU设计的修改后的冯·诺依曼模型，解释了执行线程作为扭曲的动机。处理器对应于图4.8中的处理块，只有一个控制单元获取并调度指令。\
> ![image](https://github.com/user-attachments/assets/d2cc19d1-75e4-4a2a-b9e3-c4159304d7d3)
> 相同的控制信号（从控制单元到图4.8中处理单元的箭头）传递到多个处理单元，每个处理单元对应于SM中的一个核心，每个核心执行扭曲中的一个线程。
> 由于所有处理单元都由控制单元的指令寄存器（IR）中的相同指令控制，它们的执行差异在于寄存器文件中不同的数据操作数值。这在处理器设计中称为单指令多数据（SIMD）。例如，尽管所有处理单元（核心）都由一条指令（如加法 r1, r2, r3）控制，但r2和r3的内容在不同处理单元中是不同的。
> 现代处理器中的控制单元非常复杂，包括用于获取指令的复杂逻辑和访问指令缓存的端口。让多个处理单元共享一个控制单元可以显著减少硬件制造成本和功耗。
