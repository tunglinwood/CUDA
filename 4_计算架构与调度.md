# 4_计算架构与调度

* 4.1 [现代GPU的架构](https://github.com/tunglinwood/CUDA/blob/main/4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6.md#41-%E7%8E%B0%E4%BB%A3gpu%E7%9A%84%E6%9E%B6%E6%9E%84)
* 4.2 [块调度](https://github.com/tunglinwood/CUDA/blob/main/4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6.md#42-%E5%9D%97%E8%B0%83%E5%BA%A6)
* 4.3 [同步与透明扩展性](https://github.com/tunglinwood/CUDA/blob/main/4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6.md#43-%E5%90%8C%E6%AD%A5%E4%B8%8E%E9%80%8F%E6%98%8E%E6%89%A9%E5%B1%95%E6%80%A7)
* 4.4 [扭曲和SIMD硬件](https://github.com/tunglinwood/CUDA/blob/main/4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6.md#44-%E6%89%AD%E6%9B%B2%E5%92%8Csimd%E7%A1%AC%E4%BB%B6)
* 4.5 [控制分歧](https://github.com/tunglinwood/CUDA/blob/main/4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6.md#45-%E6%8E%A7%E5%88%B6%E6%B5%81%E5%88%86%E6%AD%A7)
* 4.6 [扭曲调度与延迟容忍](https://github.com/tunglinwood/CUDA/blob/main/4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6.md#46-%E6%89%AD%E6%9B%B2%E8%B0%83%E5%BA%A6%E5%92%8C%E5%BB%B6%E8%BF%9F%E5%AE%B9%E5%BF%8D)
* 4.7 [资源分配与占用](https://github.com/tunglinwood/CUDA/blob/main/4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6.md#47-%E8%B5%84%E6%BA%90%E5%88%86%E5%8C%BA%E4%B8%8E%E5%8D%A0%E7%94%A8%E7%8E%87)
* 4.8 [查询设备属性](https://github.com/tunglinwood/CUDA/blob/main/4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6.md#48-%E6%9F%A5%E8%AF%A2%E8%AE%BE%E5%A4%87%E5%B1%9E%E6%80%A7)
* 4.9 [总结](https://github.com/tunglinwood/CUDA/blob/main/4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6.md#49-%E6%80%BB%E7%BB%93)
* [练习](https://github.com/tunglinwood/CUDA/blob/main/4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6.md#%E7%BB%83%E4%B9%A0%E9%A2%98)

在第1章，介绍中，我们看到CPU的设计目标是最小化指令执行的延迟，而GPU的设计目标是最大化指令执行的吞吐量。
在第2章，异构数据并行计算和第3章，多维网格和数据中，我们学习了用于创建和调用内核以启动和执行线程的CUDA编程接口的核心特性。
在接下来的三章中，我们将讨论现代GPU的架构，包括计算架构和内存架构，以及基于对这些架构理解的性能优化技术。
本章介绍了CUDA C程序员理解和推理其内核代码性能行为所需了解的几个GPU计算架构方面。我们将从展示计算架构的高层次、简化视图开始，探讨灵活的资源分配、块调度和占用的概念。
然后，我们将深入到线程调度、延迟容忍、控制分歧和同步。我们将以描述可以用来查询GPU可用资源的API函数和帮助估算GPU在执行内核时占用率的工具来结束本章。
在接下来的两章中，我们将介绍GPU内存架构的核心概念和编程考虑事项。
特别是，第5章，内存架构和数据局部性，重点介绍了片上内存架构，第6章，性能考虑，简要介绍了片外内存架构，并详细讨论了整个GPU架构的各种性能考虑。
掌握这些概念的CUDA C程序员能够编写和理解高性能并行内核。

## 4.1 现代GPU的架构
图4.1展示了典型的支持CUDA的GPU架构的高层次视图，这是CUDA C程序员的视角。它被组织成一组高度线程化的流处理器（SM）数组。每个SM都有几个处理单元，称为流处理器或CUDA核心（以下简称为核心，为了简洁），如图4.1中SM内的小方块所示，它们共享控制逻辑和内存资源。例如，Ampere A100 GPU有108个SM，每个有64个核心，总共6912个核心。
SM还带有不同的片上内存结构，统称为图4.1中的“内存”。这些片上内存结构将在第5章，内存架构和数据局部性中讨论。GPU还带有数GB的片外设备内存，称为图4.1中的“全局内存”。虽然较旧的GPU使用图形双倍数据速率同步DRAM，最近的GPU从NVIDIA的Pascal架构开始可能使用HBM（高带宽内存）或HBM2，这些都由与GPU紧密集成在同一封装中的DRAM（动态随机存取内存）模块组成。为了简洁起见，本书将广泛地将所有这些类型的内存统称为DRAM。我们将在第6章，性能考虑中讨论访问GPU DRAM的最重要概念。

|[top](https://github.com/tunglinwood/CUDA/blob/main/4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6.md#4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6)|
|-|

## 4.2 块调度
当内核被调用时，CUDA运行时系统启动一个线程网格来执行内核代码。这些线程按块分配到SM上。即，一个块中的所有线程同时分配到同一个SM上。

![image](https://github.com/user-attachments/assets/59f7a188-e333-45f3-9d16-bfa361361f25)
> 图4.1 支持CUDA的GPU架构

图4.2说明了块分配到SM上的过程。多个块可能同时分配到同一个SM上。例如，在图4.2中，每个SM上分配了三个块。然而，块需要保留硬件资源来执行，因此只能同时分配有限数量的块到一个SM上。块数量的限制取决于第4.6节讨论的各种因素。
由于SM数量有限，并且每个SM可以同时分配的块数量有限，因此在CUDA设备中可以同时执行的块总数是有限的。大多数网格包含的块数量远超过这个数量。为了确保网格中的所有块都能执行，运行时系统维护一个需要执行的块列表，并在先前分配的块完成执行后，将新块分配给SM。
按块分配线程到SM上保证了同一个块中的线程同时在同一个SM上调度。这一保证使得同一个块中的线程可以相互交互，而不同块中的线程不能相互交互。1 这包括障碍同步，第4.3节中讨论。还包括访问驻留在SM上的低延迟共享内存，第5章，内存架构和数据局部性中讨论。

|[top](https://github.com/tunglinwood/CUDA/blob/main/4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6.md#4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6)|
|-|

## 4.3 同步与透明扩展性
CUDA允许同一个块中的线程使用障碍同步函数`__syncthreads()`来协调它们的活动。注意“__”由两个“_”字符组成。当线程调用`__syncthreads()`时，它将停留在调用的位置，直到同一个块中的每个线程都到达该位置。
这确保了一个块中的所有线程在任何一个线程进入下一阶段之前都已完成其执行阶段。

![image](https://github.com/user-attachments/assets/5216e68d-cf80-4a2f-aa20-7486e3e08d42)
> 图4.2 线程块分配到流处理器（SM）上的示意图

障碍同步是一种简单而流行的并行活动协调方法。在现实生活中，我们经常使用障碍同步来协调多个人的并行活动。例如，假设四个朋友开车去购物中心。他们可以去不同的商店买自己的衣服。这是一种并行活动，比他们作为一个组一起依次访问所有感兴趣的商店效率高得多。然而，在他们离开购物中心之前需要障碍同步。他们必须等到四个朋友都回到车上才能离开。那些比其他人早完成购物的人必须等待那些较晚完成的人。如果没有障碍同步，当车离开时可能会有一个或多个人被留在购物中心，这会严重损害他们的友谊！
图4.3说明了障碍同步的执行。块中有N个线程。时间从左向右移动。一些线程很早到达障碍同步语句，一些线程则晚很多。那些早到达障碍的线程将等待那些晚到达的线程。当最后一个到达障碍时，所有线程都可以继续执行。通过障碍同步，“没有人被落下。”随着时间的推移。垂直曲线标志着每个线程执行`__syncthreads`语句的时间。垂直线右侧的空白区域描绘了每个线程等待所有线程完成的时间。垂直线标志着最后一个线程执行`__syncthreads`语句的时间，之后所有线程都可以继续执行`__syncthreads`语句之后的语句。

![image](https://github.com/user-attachments/assets/28fe9df6-0a1d-4b8d-92eb-7820a0861207)
> 图4.3 障碍同步的执行示例。箭头表示执行活动

在CUDA中，如果存在`__syncthreads()`语句，块中的所有线程都必须执行它。当`__syncthreads()`语句放置在if语句中时，块中的所有线程要么执行包含`__syncthreads()`的路径，要么都不执行。对于if-then-else语句，如果每个路径都有一个`__syncthreads()`语句，块中的所有线程要么执行then路径，要么执行else路径。两个`__syncthreads()`是不同的障碍同步点。例如，在图4.4中，if语句第4行开始有两个`__syncthreads()`。所有线程中偶数`threadIdx.x`值的线程执行then路径，而剩余线程执行else路径。第6行和第10行的`__syncthreads()`调用定义了两个不同的障碍。由于不能保证块中的所有线程执行任一个障碍，同步代码违反了使用`__syncthreads()`的规则，并导致未定义的执行行为。一般来说，不正确使用障碍同步可能导致结果不正确，或者线程永远等待彼此，这被称为死锁。程序员有责任避免这种不恰当的障碍同步使用。
障碍同步对块内的线程施加了执行约束。这些线程应该在时间上接近同步执行，以避免过长的等待时间。更重要的是，系统需要确保所有涉及障碍同步的线程都有必要的资源，最终到达障碍。否则，一个从未到达障碍同步点的线程可能会导致死锁。CUDA运行时系统通过将执行资源分配给块中的所有线程作为一个单元来满足这一约束，如第4.2节中所见。
不仅块中的所有线程必须分配到同一个SM，而且需要同时分配到该SM。即，只有当运行时系统已经为块中的所有线程提供了完成执行所需的所有资源时，块才可以开始执行。这确保了块中所有线程的时间接近性，并防止在障碍同步期间出现过长甚至无限的等待时间。
这导致了CUDA障碍同步设计中的一个重要权衡。

![image](https://github.com/user-attachments/assets/94714bb4-12bd-493b-8493-391bd8264398)
> 图4.5 块之间缺乏同步约束使CUDA程序具有透明的可扩展性。

通过不允许不同块中的线程彼此进行障碍同步，CUDA运行时系统可以相对于彼此以任何顺序执行块，因为它们不需要相互等待。这种灵活性使得扩展实现成为可能，如图4.5所示。图中的时间从上到下进行。在一个低成本系统中，只有少量的执行资源，一个可以同时执行少量的块，如图4.5左侧显示的同时执行两个块。在一个高端实现中，有更多的执行资源，一个可以同时执行许多块，如图4.5右侧显示的同时执行四个块。一个今天的高端GPU可以同时执行数百个块。

```cuda
void incorrect_barrier_example(int i){
    ...
    if (threadIdx.x % 2 == 0){
        ...
        __syncthreads{};
    }
    else {
        ...
        __syncthreads{};
    }
}
```  
> 图4.4 不正确使用`__syncthreads()`的示例

能够以各种速度执行相同的应用代码使得根据不同市场细分的成本、功耗和性能要求生产各种实现成为可能。例如，一个移动处理器可能以极低的功耗执行应用程序，而一个台式处理器可能以更高的速度执行相同的应用程序，同时消耗更多的功耗。两者都无需改变代码执行相同的应用程序。
在不同硬件上以不同数量的执行资源执行相同应用代码的能力被称为透明扩展性，这减少了应用开发者的负担并提高了应用的可用性。

|[top](https://github.com/tunglinwood/CUDA/blob/main/4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6.md#4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6)|
|-|

## 4.4 扭曲和SIMD硬件

我们已经看到，相对于彼此来说，块可以以任何顺序执行，这允许在不同设备之间实现透明的可扩展性。然而，我们没有详细讨论每个块内线程的执行时序。从概念上讲，应该假设块中的线程可以以任何顺序相对于彼此执行。在有阶段的算法中，如果我们希望确保所有线程在进入下一阶段之前完成上一阶段的执行，就应该使用障碍同步。内核执行的正确性不应依赖于某些线程在没有使用障碍同步的情况下彼此同步执行的假设。

CUDA GPU中的线程调度是一个硬件实现的概念，因此必须在具体硬件实现的背景下讨论。
在迄今为止的大多数实现中，一旦块被分配到一个SM（流多处理器），它就会进一步分成32线程的单位，称为扭曲（warp）。扭曲的大小是具体实现相关的，并可能在未来的GPU代中有所变化。了解扭曲有助于理解和优化CUDA应用在特定CUDA设备代上的性能。

扭曲是SM中线程调度的单位。图4.6展示了在一个实现中块分成扭曲的过程。在这个例子中，有三个块——块1、块2和块3——都分配到一个SM。每个块进一步分成扭曲以进行调度。每个扭曲由32个连续的`threadIdx`值的线程组成：线程0到31组成第一个扭曲，线程32到63组成第二个扭曲，依此类推。我们可以根据给定块大小和每个SM分配的块数量计算出驻留在SM中的扭曲数量。在这个例子中，如果每个块有256个线程，我们可以确定每个块有256/32或8个扭曲。SM中的三个块共有8*3=24个扭曲。

![image](https://github.com/user-attachments/assets/bab67b5d-823e-4228-aa8e-2273f1f0d061)
> 图4.6 块被划分为扭曲以进行线程调度。

块根据线程索引被划分为扭曲。如果一个块被组织成一维数组，即只使用`threadIdx.x`，划分是直接的。扭曲中的`threadIdx.x`值是连续且递增的。对于32大小的扭曲，扭曲0从线程0开始到线程31结束，扭曲1从线程32开始到线程63结束，依此类推。一般而言，扭曲n从线程32*n开始到线程32*(n+1)-1结束。
对于块大小不是32的倍数的情况，最后一个扭曲将用不活动的线程填充，以填满32个线程位置。例如，如果一个块有48个线程，它将被划分为两个扭曲，第二个扭曲将用16个不活动的线程填充。
对于由多个维度的线程组成的块，在划分为扭曲之前，这些维度将被投影到一个线性化的行优先布局中。线性布局由将具有较大y和z坐标的行放置在较低坐标的行之后确定。即，如果一个块由两个维度的线程组成，将通过将所有`threadIdx.y`值为1的线程放置在`threadIdx.y`值为0的线程之后形成线性布局。`threadIdx.y`值为2的线程将放置在`threadIdx.y`值为1的线程之后，以此类推。具有相同`threadIdx.y`值的线程按递增的`threadIdx.x`顺序放置在连续位置。
图4.7展示了一个将二维块的线程放置到线性布局的例子。上部显示了块的二维视图。读者应注意其与二维数组的行优先布局的相似性。每个线程显示为T<sub>y,x</sub>，`x`为`threadIdx.x`，`y`为`threadIdx.y`。下部显示了块的线性化视图。
前四个线程是`threadIdx.y`值为0的线程；它们按递增的`threadIdx.x`值排序。接下来的四个线程是`threadIdx.y`值为1的线程。它们也按递增的`threadIdx.x`值放置。在这个例子中，所有16个线程组成了一个半扭曲。该扭曲将再填充16个线程以完成一个32线程的扭曲。想象一个8*8线程的二维块。64个线程将组成两个扭曲。第一个扭曲从T<sub>0,0</sub>开始到T<sub>3,7</sub>结束。第二个扭曲从T<sub>4,0</sub>开始到T<sub>7,7</sub>结束。读者可以将其绘制出来作为练习。

![image](https://github.com/user-attachments/assets/a4e3058d-ab5c-4b41-b61a-d42affc7fdab)
> 图4.7 将二维线程放置到线性布局。

对于三维块，我们首先将所有`threadIdx.z`值为0的线程放入线性顺序。这些线程被视为一个二维块，如图4.7所示。所有`threadIdx.z`值为1的线程随后放入线性顺序，依此类推。例如，对于一个三维的2*8*4块（x维4，y维8，z维2），64个线程将被划分为两个扭曲，第一个扭曲从T<sub>0,0,0</sub>到T<sub>0,7,3</sub>，第二个扭曲从T<sub>1,0,0</sub>到T<sub>1,7,3</sub>。
SM设计成按单指令多数据（SIMD）模型执行一个扭曲中的所有线程。即，在任何时刻，为扭曲中的所有线程获取并执行一条指令（参见“扭曲和SIMD硬件”边栏）。
图4.8展示了SM中的核心如何分组成处理块，每8个核心形成一个处理块并共享一个指令获取/调度单元。以实际例子，Ampere A100 SM有64个核心，组织成四个处理块，每块16个核心。相同扭曲中的线程被分配到相同的处理块，该块获取扭曲的指令并同时对扭曲中的所有线程执行它。这些线程对数据的不同部分应用相同的指令。由于SIMD硬件有效地限制一个扭曲中的所有线程在任何时间点执行相同的指令，扭曲的执行行为通常被称为单指令多线程。
SIMD的优势在于控制硬件（如指令获取/调度单元）的成本在许多执行单元之间共享。这种设计选择使得硬件中用于控制的部分较小，更多部分用于增加算术吞吐量。我们预期在可预见的未来，扭曲划分将继续是流行的实现技术。然而，扭曲的大小可能会因具体实现而异。到目前为止，所有CUDA设备都使用了类似的扭曲配置，每个扭曲由32个线程组成。

![image](https://github.com/user-attachments/assets/e556ffb6-8eb7-47f8-a523-8166433e1e31)
> 图4.8 流多处理器组织成处理块以进行SIMD执行。

> 扭曲和SIMD硬件
> 在他1945年的开创性报告中，John von Neumann描述了一种构建电子计算机的模型，基于开创性的EDVAC计算机的设计。这个模型现在通常被称为“冯·诺依曼模型”，已经成为几乎所有现代计算机的基础蓝图。
> ![image](https://github.com/user-attachments/assets/cbd993fa-119c-475c-a477-951c506acadb)
> 冯·诺依曼模型在下图中有所展示。计算机有一个I/O（输入/输出），允许将程序和数据提供给系统并从系统生成。要执行程序，计算机首先将程序及其数据输入到内存中。
> 程序由一组指令组成。控制单元维护一个程序计数器（PC），其中包含要执行的下一条指令的内存地址。在每个“指令周期”中，控制单元使用PC将一条指令获取到指令寄存器（IR）中。然后检查指令位以确定计算机所有组件要采取的行动。这就是为什么该模型也被称为“存储程序”模型，意味着用户可以通过将不同的程序存储到内存中来改变计算机的行为。
> 通过以下反映GPU设计的修改后的冯·诺依曼模型，解释了执行线程作为扭曲的动机。处理器对应于图4.8中的处理块，只有一个控制单元获取并调度指令。\
> ![image](https://github.com/user-attachments/assets/d2cc19d1-75e4-4a2a-b9e3-c4159304d7d3)
> 相同的控制信号（从控制单元到图4.8中处理单元的箭头）传递到多个处理单元，每个处理单元对应于SM中的一个核心，每个核心执行扭曲中的一个线程。
> 由于所有处理单元都由控制单元的指令寄存器（IR）中的相同指令控制，它们的执行差异在于寄存器文件中不同的数据操作数值。这在处理器设计中称为单指令多数据（SIMD）。例如，尽管所有处理单元（核心）都由一条指令（如加法 r1, r2, r3）控制，但r2和r3的内容在不同处理单元中是不同的。
> 现代处理器中的控制单元非常复杂，包括用于获取指令的复杂逻辑和访问指令缓存的端口。让多个处理单元共享一个控制单元可以显著减少硬件制造成本和功耗。

|[top](https://github.com/tunglinwood/CUDA/blob/main/4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6.md#4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6)|
|-|

## 4.5 控制流分歧

当一个扭曲内的所有线程在处理数据时遵循相同的执行路径（正式称为控制流）时，SIMD执行效果很好。例如，对于一个if-else结构，当一个扭曲内的所有线程执行if路径或所有线程执行else路径时，执行效果良好。然而，当一个扭曲内的线程采取不同的控制流路径时，SIMD硬件需要多次通过这些路径，每次通过执行一个路径。例如，对于一个if-else结构，如果一个扭曲内的一些线程遵循if路径，而其他线程遵循else路径，硬件将需要进行两次通过。一遍执行遵循if路径的线程，另一遍执行遵循else路径的线程。在每次通过中，遵循其他路径的线程不被允许生效。

当一个扭曲内的线程遵循不同的执行路径时，我们称这些线程表现出控制分歧，即它们在执行上发生了分歧。多次通过的方法扩展了SIMD硬件实现CUDA线程完整语义的能力。虽然硬件为一个扭曲内的所有线程执行相同的指令，但它有选择地让这些线程仅在与它们所采取的路径相对应的那一遍生效，使得每个线程看起来像是在执行自己的控制流路径。这在保持线程独立性的同时，利用了SIMD硬件的低成本优势。然而，分歧的代价在于硬件需要额外的通过来允许一个扭曲内的不同线程做出自己的决定，以及在每次通过中不活跃线程所消耗的执行资源。

图4.9展示了一个扭曲在执行一个有分歧的if-else语句时的例子。在这个例子中，当由线程0-31组成的扭曲到达if-else语句时，线程0-23走then路径，而线程24-31走else路径。在这种情况下，扭曲将进行一次通过，线程0-23执行A，而线程24-31处于不活跃状态。扭曲还将进行另一遍，通过代码执行线程24-31执行B，而线程0-23处于不活跃状态。然后，扭曲中的线程重新汇合并执行C。在Pascal架构及之前的架构中，这些通过是顺序执行的，即一个通过完成后再执行另一个通过。

![image](https://github.com/user-attachments/assets/b8257dd4-0514-4855-85e2-ed0791e3fa63)
> 图4.9 if-else语句中的扭曲分歧示例。

从Volta架构开始，这些通过可以并发执行，即一个通过的执行可以与另一个通过的执行交错进行。这个特性被称为独立线程调度。感兴趣的读者可以参考Volta V100架构白皮书（NVIDIA, 2017）了解详情。

分歧也可以出现在其他控制流结构中。图4.10展示了一个扭曲在执行一个有分歧的for循环时的例子。在这个例子中，每个线程执行不同数量的循环迭代，范围从4到8。在前四次迭代中，所有线程都活跃并执行A。在剩余的迭代中，一些线程执行A，而其他线程因为已经完成了迭代而处于不活跃状态。

通过检查决策条件，可以确定一个控制结构是否会导致线程分歧。如果决策条件基于`threadIdx`值，控制语句可能会导致线程分歧。例如，语句`if(threadIdx.x>2) {...}`会导致一个块的第一个扭曲中的线程遵循两个不同的控制流路径。线程0、1和2遵循与线程3、4、5等不同的路径。同样，如果循环条件基于线程索引值，循环也可能导致线程分歧。

使用具有线程控制分歧的控制结构的一个普遍原因是处理将线程映射到数据时的边界条件。这通常是因为线程总数需要是线程块大小的倍数，而数据的大小可以是任意数。回到我们在第2章“异构数据并行计算”中的向量加法内核，我们在`addVecKernel`中有一个`if(i<n)`语句。这是因为并不是所有向量长度都可以表示为块大小的倍数。例如，假设向量长度为1003，而我们选择64作为块大小。需要启动16个线程块来处理所有1003个向量元素。然而，16个线程块将有1024个线程。我们需要禁用第15块中最后21个线程，以避免执行原始程序不期望或不允许的工作。记住，这16个块被分成32个扭曲。只有最后一个扭曲（即最后一个块中的第二个扭曲）会有控制分歧。

![image](https://github.com/user-attachments/assets/fe96b383-a1d8-4780-bc80-6c9ec823f1d9)
> 图4.10 for循环中的扭曲分歧示例。

请注意，控制分歧的性能影响随着处理向量大小的增加而减少。对于长度为100的向量，四个扭曲中的一个会有控制分歧，这会对性能产生显著影响。对于长度为1000的向量，只有32个扭曲中的一个会有控制分歧。也就是说，控制分歧只会影响约3%的执行时间。即使它使扭曲的执行时间翻倍，对总执行时间的净影响也只有约3%。显然，如果向量长度是10000或更多，313个扭曲中只有一个会有控制分歧。控制分歧的影响将远小于1%。

对于二维数据，例如第3章“多维网格和数据”中的颜色到灰度转换示例，if语句也用于处理在数据边缘操作的线程的边界条件。在图3.2中，为处理62*76的图像，我们使用了20个二维块（4*5），每个块由16*16个线程组成。每个块将被划分为8个扭曲；每个扭曲由两个块的行组成。总共有160个扭曲（每个块8个扭曲）参与处理。要分析控制分歧的影响，请参考图3.5。区域1中的12个块中的所有扭曲都不会有控制分歧。区域1中有12*8=96个扭曲。对于区域2，所有24个扭曲都会有控制分歧。对于区域3，所有底部扭曲映射到图像外的数据。因此，它们都不会通过if条件。读者应验证，如果图片在垂直方向上有奇数个像素，这些扭曲将会有控制分歧。在区域4，前7个扭曲将有控制分歧，但最后一个扭曲不会。总的来说，160个扭曲中有31个会有控制分歧。

同样，控制分歧的性能影响随着水平维度上像素数量的增加而减少。例如，如果我们处理一个200*150的图片，使用16*16块，将总共需要130=13*10个线程块或1040个扭曲。区域1到4中的扭曲数量将分别是864（12*9*8），72（9*8），96（12*8）和8（1*8）。只有80个扭曲会有控制分歧。因此，控制分歧的性能影响将小于8%。显然，如果我们处理一个水平维度上有超过1000个像素的现实图片，控制分歧的性能影响将小于2%。

控制分歧的一个重要影响是，不能假设一个扭曲内的所有线程具有相同的执行时间。因此，如果一个扭曲内的所有线程必须完成执行的一个阶段才能继续到下一阶段，必须使用诸如`__syncwarp()`之类的障碍同步机制来确保正确性。

|[top](https://github.com/tunglinwood/CUDA/blob/main/4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6.md#4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6)|
|-|

## 4.6 扭曲调度和延迟容忍

当线程被分配到SM时，通常分配到一个SM的线程数要多于SM中的核心数。也就是说，每个SM在任何时间点上只有足够的执行单元来执行分配给它的所有线程的一个子集。在早期的GPU设计中，每个SM在任何给定时刻只能为一个扭曲执行一条指令。在最近的设计中，每个SM可以在任何时间点为少数几个扭曲执行指令。无论哪种情况，硬件只能为SM中的一部分扭曲执行指令。一个合法的问题是，如果一个SM在任何时间点上只能执行一部分扭曲，为什么需要为一个SM分配这么多扭曲？答案是，这就是GPU如何容忍长延迟操作（如全局内存访问）的方式。

当一个扭曲需要等待先前启动的长延迟操作的结果时，该扭曲不会被选中执行。取而代之的是，另一个不再等待先前指令结果的驻留扭曲将被选中执行。如果有多个扭曲准备好执行，则使用优先级机制选择一个扭曲执行。这种用其他线程的工作填补操作延迟时间的机制通常称为“延迟容忍”或“延迟隐藏”（见“延迟容忍”边栏）。

> ### 延迟容忍
> 延迟容忍在许多日常情况下是需要的。例如，在邮局，每个人在寄包裹之前都应该已经填写好所有的表格和标签。
> 然而，正如我们都有过的经历，有些人等待服务柜台的职员告诉他们需要填写哪张表格以及如何填写表格。
> 当服务柜台前有长队时，最大化服务职员的生产力很重要。让一个人在职员面前填写表格，而所有人都在等待，这不是一个好方法。
> 职员应该在这个人填写表格时帮助排队等待的下一个顾客。这些其他顾客是“准备好”的，不应该被需要更多时间填写表格的顾客阻挡。
> 这就是为什么一个好的职员会礼貌地要求第一个顾客走到一边填写表格，而职员服务其他顾客。在大多数情况下，第一个顾客不需要回到队伍的末尾，而是在他或她填写完表格且职员服务完当前顾客后立即得到服务。
> 我们可以将这些邮局顾客看作是扭曲，将职员看作是硬件执行单元。需要填写表格的顾客对应于一个其继续执行依赖于长延迟操作的扭曲。

请注意，扭曲调度还用于容忍其他类型的操作延迟，例如流水线浮点运算和分支指令。只要有足够多的扭曲，硬件就可能在任何时间点找到一个扭曲执行，从而在一些扭曲的指令等待长延迟操作结果时充分利用执行硬件。选择准备好执行的扭曲不会在执行时间线上引入任何闲置或浪费时间，这称为零开销线程调度（见“线程、上下文切换和零开销调度”边栏）。通过扭曲调度，扭曲指令的长等待时间被通过执行其他扭曲的指令“隐藏”起来。这种容忍长操作延迟的能力是GPU不需要像CPU那样在缓存内存和分支预测机制上投入大量芯片面积的主要原因。因此，GPU可以将更多芯片面积用于浮点执行和内存访问通道资源。

> ### 线程、上下文切换和零开销调度
> 基于冯·诺依曼模型，我们准备更深入地理解线程是如何实现的。现代计算机中的线程是一个程序和在冯·诺依曼处理器上执行该程序的状态。回想一下，线程由程序代码、正在执行的代码指令和变量及数据结构的值组成。
> 在基于冯·诺依曼模型的计算机中，程序代码存储在内存中。PC跟踪正在执行的程序指令的地址。IR保存正在执行的指令。寄存器和内存保存变量和数据结构的值。
> 现代处理器设计允许上下文切换，多个线程可以通过轮流进展来时间共享处理器。通过仔细保存和恢复PC值和寄存器及内存的内容，我们可以暂停线程的执行，并在稍后正确恢复该线程的执行。然而，在这些处理器中，保存和恢复寄存器内容在上下文切换过程中可能会增加显著的执行时间开销。
> 零开销调度指的是GPU能够在不引入任何额外闲置周期的情况下，让需要等待长延迟指令结果的扭曲休眠，并激活准备好的扭曲。传统CPU会产生这样的闲置周期，因为切换执行线程需要将退出线程的执行状态（如寄存器内容）保存到内存，并从内存加载进入线程的执行状态。GPU的SM通过在硬件寄存器中保存所有分配给扭曲的执行状态，从而在切换扭曲时无需保存和恢复状态，实现了零开销调度。

为了使延迟容忍有效，一个SM被分配的线程数量应远多于其执行资源可以同时支持的线程数量，以最大化在任何时间点找到准备好执行的扭曲的机会。例如，在Ampere A100 GPU中，一个SM有64个核心，但可以同时分配多达2048个线程。因此，SM可以分配的线程数是其核心可以在任何给定时钟周期支持的线程数的32倍。这种对SM的线程超量分配对于延迟容忍至关重要。它增加了当当前执行的扭曲遇到长延迟操作时找到另一个扭曲执行的机会。

|[top](https://github.com/tunglinwood/CUDA/blob/main/4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6.md#4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6)|
|-|

## 4.7 资源分区与占用率

我们已经看到，为了容忍长延迟操作，向一个SM分配许多扭曲是可取的。然而，不可能总是将SM支持的最大扭曲数分配给SM。分配给一个SM的扭曲数与其支持的最大扭曲数之比称为占用率。为了理解什么可能阻止SM达到最大占用率，首先要了解SM资源是如何分区的。

SM中的执行资源包括寄存器、共享内存（将在第5章内存架构与数据局部性中讨论）、线程块槽和线程槽。这些资源在执行期间动态分配给线程。例如，一个Ampere A100 GPU每个SM可以支持最多32个块、64个扭曲（2048个线程）和每块1024个线程。如果一个网格启动时块大小为1024线程（允许的最大值），则每个SM中的2048个线程槽被分配并分配给2个块。在这种情况下，每个SM可以容纳最多2个块。同样地，如果一个网格启动时块大小为512、256、128或64线程，则2048个线程槽被分配并分配给4、8、16或32个块，分别对应。

这种在块之间动态分配线程槽的能力使SM变得多功能。它们可以执行多个包含少量线程的块，也可以执行少量包含大量线程的块。这种动态分区可以与固定分区方法对比，后者中每个块无论实际需要多少资源都将收到固定数量的资源。固定分区会导致当一个块需要的线程数少于固定分区支持的线程数时浪费线程槽，并且当一个块需要的线程槽数多于固定分区支持的线程槽数时无法支持该块。

资源的动态分区可能导致资源限制之间的微妙交互，这可能导致资源利用不足。这种交互可能发生在块槽和线程槽之间。在Ampere A100的例子中，我们看到块大小可以从1024到64变化，分别导致每个SM有2到32个块。在所有这些情况下，分配给SM的总线程数为2048，从而最大化占用率。然而，考虑每个块有32个线程的情况。在这种情况下，2048个线程槽需要被分配并分配给64个块。然而，Volta SM一次只能支持32个块槽。这意味着只有1024个线程槽会被使用，即32个块，每个块32个线程。这种情况下的占用率为（1024分配的线程）/（2048最大线程）= 50%。因此，为了充分利用线程槽并达到最大占用率，每个块需要至少64个线程。

另一种可能负面影响占用率的情况是每块的最大线程数不能被块大小整除。在Ampere A100的例子中，我们看到每个SM最多可以支持2048个线程。然而，如果选择768的块大小，SM将只能容纳2个线程块（1536个线程），留下512个线程槽未使用。在这种情况下，既没有达到每个SM的最大线程数，也没有达到每个SM的最大块数。此时的占用率为（1536分配的线程）/（2048最大线程）= 75%。

前面的讨论没有考虑其他资源限制的影响，例如寄存器和共享内存。我们将在第5章内存架构与数据局部性中看到，CUDA内核中声明的自动变量被放置在寄存器中。一些内核可能使用许多自动变量，而另一些可能使用很少的自动变量。因此，应预期一些内核每个线程需要许多寄存器，而另一些需要较少的寄存器。通过在SM中动态分配寄存器，SM可以容纳许多块，如果它们每个线程需要的寄存器较少；而如果每个线程需要的寄存器较多，则可以容纳较少的块。

然而，需要注意的是寄存器资源限制对占用率的潜在影响。例如，Ampere A100 GPU每个SM最多允许65536个寄存器。要达到满占用率，每个SM需要足够的寄存器来支持2048个线程，这意味着每个线程不应使用超过（65536个寄存器）/（2048个线程）= 32个寄存器。例如，如果一个内核每个线程使用64个寄存器，65536个寄存器最多可以支持1024个线程。在这种情况下，无论块大小设置为何，该内核都不能达到满占用率。相反，占用率最多为50%。在某些情况下，编译器可能会执行寄存器溢出以减少每个线程的寄存器需求，从而提高占用率。然而，这通常以增加线程访问溢出寄存器值的内存访问时间为代价，可能导致网格的总执行时间增加。类似的分析将在第5章内存架构与数据局部性中进行。

假设程序员实现了一个每个线程使用31个寄存器的内核，并将其配置为每块512个线程。在这种情况下，每个SM将有（2048个线程）/（512个线程/块）= 4个块同时运行。这些线程将使用总计（2048个线程）*（31个寄存器/线程）= 63488个寄存器，少于65536个寄存器限制。现在假设程序员在内核中声明了另外两个自动变量，将每个线程使用的寄存器数增加到33。2048个线程现在需要67584个寄存器，超过寄存器限制。CUDA运行时系统可能会通过为每个SM分配3个块而不是4个块来应对这种情况，从而将所需寄存器减少到50688个寄存器。然而，这会将每个SM上运行的线程数从2048减少到1536；也就是说，通过使用两个额外的自动变量，程序的占用率从100%下降到75%。这有时被称为“性能悬崖”，即资源使用的轻微增加可能导致并行性和性能的显著下降（Ryoo等，2008）。

显然，所有动态分配资源的约束以复杂的方式相互作用。准确确定每个SM上运行的线程数可能是困难的。读者可以参考CUDA占用率计算器（CUDA Occupancy Calculator, Web），这是一张可下载的电子表格，给定内核的资源使用情况，可以计算特定设备实现中每个SM上实际运行的线程数。

|[top](https://github.com/tunglinwood/CUDA/blob/main/4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6.md#4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6)|
|-|

## 4.8 查询设备属性

关于SM资源分区的讨论引出了一个重要问题：如何找出特定设备上可用的资源数量？当CUDA应用程序在系统上执行时，如何找到设备中的SM数量以及可以分配给每个SM的块和线程的数量？这些问题也适用于我们尚未讨论的其他资源。一般来说，许多现代应用程序设计为可以在各种硬件系统上执行。通常，应用程序需要查询底层硬件的可用资源和能力，以利用更强大的系统，同时弥补较弱系统的不足（见“资源和能力查询”旁注）。

> 资源和能力查询
> 在日常生活中，我们经常查询环境中的资源和能力。例如，当我们预订酒店时，可以查看酒店房间的设施。如果房间里有吹风机，我们就不需要带一个。大多数美国酒店房间配备了吹风机，而许多其他地区的酒店则没有。一些亚洲和欧洲酒店提供牙膏甚至牙刷，而大多数美国酒店则没有。许多美国酒店提供洗发水和护发素，而其他大陆的酒店通常只提供洗发水。如果房间里有微波炉和冰箱，我们可以把晚餐的剩菜带回去，期待第二天继续吃。如果酒店有游泳池，我们可以带上泳衣，会议后去游泳。如果酒店没有游泳池但有健身房，我们可以带跑鞋和运动服。一些高端亚洲酒店甚至提供运动服！这些酒店设施是酒店的属性或资源和能力。经验丰富的旅行者会在酒店网站上检查这些属性，选择最符合需求的酒店，从而更高效、有效地打包。

每个CUDA设备的SM中的资源量作为设备的计算能力的一部分进行说明。一般来说，计算能力水平越高，每个SM中的资源越多。GPU的计算能力通常随着代际的增加而增加。Ampere A100 GPU的计算能力为8.0。

在CUDA C中，主机代码有一个内置机制来查询系统中可用设备的属性。CUDA运行时系统（设备驱动程序）提供了一个API函数 `cudaGetDeviceCount`，用于返回系统中可用CUDA设备的数量。主机代码可以使用以下语句找出可用CUDA设备的数量：

```cuda
int devCount;
cudaGetDeviceCount(&devCount);
```

虽然这可能不明显，但现代PC系统通常有两个或更多的CUDA设备。这是因为许多PC系统配备了一个或多个“集成”GPU。这些GPU是默认的图形单元，提供基本的功能和硬件资源，以执行现代窗口化用户界面的最小图形功能。大多数CUDA应用程序在这些集成设备上的表现通常不佳。这是主机代码需要遍历所有可用设备、查询它们的资源和能力，并选择那些有足够资源来执行应用程序以获得令人满意的性能的原因。

CUDA运行时系统将系统中所有可用的设备编号从0到 `devCount-1`。它提供了一个API函数 `cudaGetDeviceProperties`，返回给定设备号的设备属性。例如，我们可以在主机代码中使用以下语句来遍历可用设备并查询它们的属性：

```cuda
cudaDeviceProp devProp;
for(unsigned int i = 0; i < devCount; i++) {
    cudaGetDeviceProperties(&devProp, i);
    // 决定设备是否具有足够的资源/能力
}
```

内置类型 `cudaDeviceProp` 是一个C结构体类型，其字段表示CUDA设备的属性。有关类型的所有字段，读者可以参考CUDA C编程指南。我们将讨论一些与线程执行资源分配特别相关的字段。我们假设属性以 `devProp` 变量返回，字段由 `cudaGetDeviceProperties` 函数设置。如果读者选择使用不同的变量名，则需要在以下讨论中替换适当的变量名。

如名称所示，字段 `devProp.maxThreadsPerBlock` 给出了查询设备中每个块允许的最大线程数。一些设备允许每块最多1024个线程，而其他设备可能允许更少。未来的设备甚至可能允许每块超过1024个线程。因此，查询可用设备并确定哪些设备在每个块中允许足够的线程是一个好主意。

设备中的SM数量由 `devProp.multiProcessorCount` 给出。如果应用程序需要许多SM以实现令人满意的性能，则应检查该属性。此外，设备的时钟频率在 `devProp.clockRate` 中。时钟频率和SM数量的组合可以很好地指示设备的最大硬件执行吞吐量。

主机代码可以在 `devProp.maxThreadsDim[0]`（x维度）、`devProp.maxThreadsDim[1]`（y维度）和 `devProp.maxThreadsDim[2]`（z维度）字段中找到块在每个维度上允许的最大线程数。这些信息的一个示例用途是自动调优系统，在评估最佳执行块维度时设置块维度的范围。类似地，可以在 `devProp.maxGridSize[0]`（x维度）、`devProp.maxGridSize[1]`（y维度）和 `devProp.maxGridSize[2]`（z维度）字段中找到网格在每个维度上允许的最大块数。使用这些信息的一个典型用途是确定网格是否可以拥有足够的线程来处理整个数据集，或者是否需要某种迭代方法。

字段 `devProp.regsPerBlock` 给出了每个SM中可用的寄存器数量。这个字段在确定内核是否能够在特定设备上实现最大占用率或是否会受到寄存器使用限制时非常有用。请注意，字段的名称有点误导。在大多数计算能力水平下，一个块可以使用的最大寄存器数确实与SM中可用的总寄存器数相同。然而，对于某些计算能力水平，块可以使用的最大寄存器数少于SM中总共可用的寄存器数。

我们还讨论了warp的大小取决于硬件。warp的大小可以从 `devProp.warpSize` 字段中获得。

`cudaDeviceProp` 类型中还有许多其他字段。我们将在书中介绍这些字段，同时引入它们设计反映的概念和功能。

|[top](https://github.com/tunglinwood/CUDA/blob/main/4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6.md#4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6)|
|-|

## 4.9 总结

GPU 被组织为 SM，其中包含多个核心处理块，这些核心共享控制逻辑和内存资源。当启动一个网格时，其块会以任意顺序分配给 SM，从而实现 CUDA 应用程序的透明可扩展性。然而，这种透明的可扩展性有一个限制：不同块中的线程不能相互同步。

线程按块的基础分配到 SM 上执行。一旦一个块被分配给 SM，它会进一步划分为 warps。warp 中的线程按照 SIMD 模型执行。如果同一个 warp 中的线程由于执行不同的路径而发生分歧，处理块会在每个线程仅在其执行路径对应的 pass 中活跃的情况下执行这些路径。

一个 SM 可能分配了比它能够同时执行的线程数量更多的线程。在任何时刻，SM 只执行其驻留 warps 的一个小子集的指令。这使得其他 warps 可以等待长延迟操作，而不会减慢大量处理单元的整体执行吞吐量。分配给 SM 的线程数量与 SM 能够支持的最大线程数量的比率称为占用率。SM 的占用率越高，它就越能隐藏长延迟操作。

每个 CUDA 设备对每个 SM 中可用资源的数量可能施加不同的限制。例如，每个 CUDA 设备对每个 SM 可以容纳的块数量、线程数量、寄存器数量和其他资源的数量都有一定的限制。对于每个内核，这些资源限制中的一个或多个可能成为占用率的限制因素。CUDA C 提供了查询 GPU 中可用资源的能力，使程序员能够在运行时获取这些信息。

## 练习题

1. 考虑以下 CUDA 内核及其对应的主机函数：

```cuda
__global__ void foo_kernel(int* a, int* b){
    unsigned int i = blockIdx.x*blockDim.x + threadIdx.x;
    if(threadIdx.x < 40 || threadIdx.x >= 104){
        b[i] = a[i] + 1;
    }
    if(unsigned int j = 0; j < 5 - (i%3); ++j){
      b[i] += j;
    }
}
void foo(int* a_d, int* b_d){
    unsigned int N = 1024;
    foo_kernel<<<(N+128-1)/128, 128>>>(a_d,b_d);
}
```

a. 每个块中的 warp 数量是多少？

b. 网格中的 warp 数量是多少？

c. 对于第 04 行的语句：
   * 网格中有多少 warp 是活跃的？
   * 网格中有多少 warp 是发散的？
   * 块 0 的 warp 0 的 SIMD 效率是多少（以 % 表示）？
   * 块 0 的 warp 1 的 SIMD 效率是多少（以 % 表示）？
   * 块 0 的 warp 3 的 SIMD 效率是多少（以 % 表示）？

d. 对于第 07 行的语句：
   * 网格中有多少 warp 是活跃的？
   * 网格中有多少 warp 是发散的？
   * 块 0 的 warp 0 的 SIMD 效率是多少（以 % 表示）？

e. 对于第 09 行的循环：
   * 有多少次迭代没有发散？
   * 有多少次迭代发生了发散？

2. 对于向量加法，假设向量长度为 2000，每个线程计算一个输出元素，线程块的大小为 512 线程。网格中会有多少个线程？

3. 对于上一个问题，预计有多少个 warp 会由于向量长度的边界检查而发生发散？

4. 考虑一个假设的块，有 8 个线程在到达屏障之前执行一段代码。线程执行这些代码段所需的时间（以微秒为单位）为：2.0、2.3、3.0、2.8、2.4、1.9、2.6 和 2.9；其余时间都在等待屏障。线程总执行时间中有多少百分比是花费在等待屏障上的？

5. 一个 CUDA 程序员说，如果他们每个块中只有 32 个线程，他们可以省略 `__syncthreads()` 指令，无需在需要屏障同步的地方使用。你认为这是一个好主意吗？请解释。

6. 如果一个 CUDA 设备的 SM 可以容纳最多 1536 个线程和最多 4 个线程块，那么以下哪种块配置会导致 SM 中线程的数量最多？
   a. 每块 128 个线程
   b. 每块 256 个线程
   c. 每块 512 个线程
   d. 每块 1024 个线程

7. 假设一个设备允许每个 SM 最多 64 个块和 2048 个线程。指明以下哪些分配是可能的。如果可能，请指明占用率水平。
   a. 每块 128 线程，共 8 块
   b. 每块 64 线程，共 16 块
   c. 每块 32 线程，共 32 块
   d. 每块 32 线程，共 64 块
   e. 每块 64 线程，共 32 块

8. 考虑一个 GPU，具有以下硬件限制：每个 SM 2048 个线程，32 个块，以及每个 SM 64K（65,536）寄存器。对于以下每种内核特性，指定该内核是否可以达到全占用率。如果不能，指定限制因素。
   a. 内核使用每块 128 线程，每线程 30 个寄存器。
   b. 内核使用每块 32 线程，每线程 29 个寄存器。
   c. 内核使用每块 256 线程，每线程 34 个寄存器。

9. 一位学生提到，他们使用一个 32x32 的线程块的矩阵乘法内核来乘以两个 1024x1024 的矩阵。学生使用的 CUDA 设备允许每块最多 512 个线程，每个 SM 最多 8 个块。学生进一步提到，每个线程块中的每个线程计算结果矩阵的一个元素。你会有什么反应，为什么？

|[top](https://github.com/tunglinwood/CUDA/blob/main/4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6.md#4_%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%B0%83%E5%BA%A6)|
|-|
